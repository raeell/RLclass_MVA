{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030eda65-9fd3-4657-9e96-2e577b1098ff",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817d749-b572-4f25-b203-0c7e1a2ae3e8",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 3: Learning value functions</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6f01d-08c1-4df1-b0ee-6413e211e383",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**  \n",
    "By the end of this chapter, you should be able to:\n",
    "- explain approximate value iteration and recall its (non-)convergence properties,\n",
    "- explain why and how approximate dynamic programming can be cast as a series of risk minimization problems,\n",
    "- write a TD(0) algorithm for value function estimation and explain its properties,\n",
    "- write a Q-learning algorithm for value function estimation and explain its properties,\n",
    "- identify and explain the three intrinsic challenges of RL which are value function approximation, the improvement problem and the exploration vs. exploitation tradeoff.\n",
    "\n",
    "Additionally, after doing the homework, you should be able to:\n",
    "- explain the difference between on-policy and off-policy algorithms,\n",
    "- implement $n$-step temporal difference estimators, from Monte Carlo ones to TD($\\lambda$), and discuss their properties, \n",
    "- discuss the impact of the behavior distribution on risk minimization, \n",
    "- explain what an actor-critic architecture is, \n",
    "- define a greedy in the limit of infinite exploration (GLIE) actor, \n",
    "- implement SARSA.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85133920-4d7b-4586-b217-aa5831fd4bd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Approximate dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb7319-d217-404d-bc38-7cf812d89e19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's take a step back on the contents of the previous chapter.\n",
    "\n",
    "With the Bellman equation, we have a way to **characterize** $Q^*$. This characterization directly translates to the **Value Iteration** algorithm. In turn, once we know $Q^*$, we can deduce $\\pi^*$.\n",
    "\n",
    "That's all very nice, but is it applicable in practice, on real world examples? In particular, how does the computation of $Q^*$ scale with large state and action spaces?\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Do you recall the time complexity of a single iteration of value iteration in terms of $|S|$ and $|A|$?  \n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$O(S^2 A)$\n",
    "</details>\n",
    "\n",
    "The curse of dimensionality makes the number of states and actions scale exponentially with the dimension of the state and action spaces. So exact computation of $Q^*$ quickly becomes intractable, as building $Q_{n+1} = T^*Q_n$ from $Q_n$ requires $|S|$ operations in every state-action pair (hence the complexity in the exercise above).\n",
    "\n",
    "Instead, one can try to *approximate* the resolution of $T^* Q_n$ at each step of Value Iteration. This yields the Approximate Value Iteration algorithm.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Approximate Value Iteration** is the algorithm that computes the sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$, where $\\mathcal{A}$ is an approximation procedure.\n",
    "</div>\n",
    "\n",
    "Note in particular that when dealing with parametric functions $Q_\\theta$, finding a minimizer of the loss\n",
    "$L_n(\\theta) = \\| Q_\\theta - T^* Q_n \\|$\n",
    "is such an approximation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b248c-b12d-4f79-9485-2bba1e441522",
   "metadata": {},
   "source": [
    "Let us suppose that $\\mathcal{A}$ is not a bad approximation procedure and that its approximation error is uniformly bounded, that is, \n",
    "$$\\forall f \\in \\mathbb{R}^{SA}, \\ \\| f-\\mathcal{A}f \\|_\\infty \\leq \\epsilon.$$\n",
    "\n",
    "The first important result is that Approximate Value Iteration **does not converge**. However, one can prove that $Q_n$ reaches a neighborhood of $Q^*$. Specifically, there exists $N$ such that for all $n\\geq N$,\n",
    "$$\\| Q^* - Q_n \\|_\\infty \\leq \\frac{\\epsilon}{1-\\gamma}.$$\n",
    "\n",
    "More importantly, let $\\pi_n$ be the greedy policy with respect to $Q_n$, then:\n",
    "$$\\|Q^*-Q^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma}{1-\\gamma} \\|Q^*-Q_n\\|_\\infty.$$\n",
    "\n",
    "And consequently, for such $n\\geq N$,\n",
    "$$\\|Q^*-Q^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}.$$\n",
    "\n",
    "So,\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Approximate Value Iteration does not necessarily converge but reaches policies whose values are close to optimal.\n",
    "</div>\n",
    "\n",
    "These results are proven in the **[Neuro-dynamic programming](http://athenasc.com/ndpbook.html)** book by D. P. Bertsekas and J. Tsitsiklis (1996).\n",
    "\n",
    "Most supervised learning algorithms minimize a loss that is expressed as a weighted $L_2$ norm. Thus, they don't explicitly provide guarantees in $L_\\infty$ norm. R. Munos provided **[error bounds for approximate value iteration](https://www.aaai.org/Papers/AAAI/2005/AAAI05-159.pdf)** in the general case of weighted $L_p$ norms. Those bounds are similar to the one in $L_\\infty$ norm, thus justifying the use of supervised learning techniques (such as neural networks or random forests for instance) in Approximate Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f6211-6628-487c-a834-15bc6d612bd8",
   "metadata": {},
   "source": [
    "**Approximate value iteration generalizes value iteration**\n",
    "\n",
    "Let us stress this out: approximate value iteration fully generalizes value iteration.  \n",
    "The Bellman optimality equation defines $Q^*$ as the solution to $Q=T^* Q$. This solution belongs to the space $\\mathbb{R}^{SA}$ of functions from $S\\times A$ to $\\mathbb{R}$. This space of functions is a vector space whose dimension is the cardinality of $S\\times A$. Hence, beyond the case of finite state and action spaces, no finite basis of functions can span $\\mathbb{R}^{SA}$ and searching for a solution to $Q=T^* Q$ is searching inside an infinite dimensional space.  \n",
    "\n",
    "When $\\mathcal{A}$ turns out to be exact (that is, its approximation error $\\epsilon$ is null), then AVI boils down to VI.  \n",
    "This can happen when $S$ and $A$ are finite and the functions $\\mathbb{R}^{SA}$ are represented exactly as vectors in $\\mathbb{R}^{|S||A|}$. But this can happen also in continuous state or action spaces with very specific hypotheses, where the optimality equation admits a closed-form solution. An example of this where the value function is a polynomial function of a continuous state variable is developped in these [two](https://papers.nips.cc/paper_files/paper/2000/hash/09b15d48a1514d8209b192a8b8f34e48-Abstract.html) [papers](https://ieeexplore.ieee.org/abstract/document/5364653).\n",
    "\n",
    "On the other hand, when the state-action space is continuous, or just too large to be enumerated, AVI trades exact construction of the $Q_{n+1} = T^*Q_n$ sequence for scalability, by constructing an approximate sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$ that still provides good policies if $\\mathcal{A}$ is a good approximation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa61021-05ab-4a96-8949-20e1cf501f0f",
   "metadata": {},
   "source": [
    "**And approximate (modified) policy iteration?**\n",
    "\n",
    "Approximate Policy Iteration consists in solving the evaluation equation up to a certain precision $\\epsilon$ and then taking a greedy improvement step.\n",
    "\n",
    "We write $V_n$ the approximation of $V^{\\pi_n}$ at the end of an evaluation phase in Policy Iteration and suppose that the approximation error is uniformly bounded:\n",
    "$$\\| V^{\\pi_n} - V_n \\|_\\infty \\leq \\epsilon.$$\n",
    "\n",
    "Then it is known that, even though the sequence of greedy policies $\\pi_n$ does not converge, it oscillates among a set of policies such that:\n",
    "$$\\|V^*-V^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}.$$\n",
    "\n",
    "This result is also proven in the **[Neuro-dynamic programming](http://athenasc.com/ndpbook.html)** book by D. P. Bertsekas and J. Tsitsiklis (1996).\n",
    "\n",
    "So the neighborhood reached has the same size as that of Approximate Value Iteration.\n",
    "\n",
    "Similar **[error bounds for approximate policy iteration](https://www.aaai.org/Papers/ICML/2003/ICML03-074.pdf)** in weighted $L_p$ norms were provided by R. Munos (2003).  \n",
    "These bounds were later generalized by the study of **[approximate modified policy iteration](https://icml.cc/2012/papers/608.pdf)** by B. Scherrer et al. (2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d92f4a-c53b-4936-87a9-c9437d8efdca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "From now on, we will cast our algorithms as approximate dynamic programming ones, operating on a given function space for Q-functions, part of $\\mathbb{R}^{SA}$, regardless of the nature of $S$ and $A$.\n",
    "</div>\n",
    "\n",
    "The key idea we develop in this chapter is that one can actually *learn* the sequence of AVI functions using interaction samples rather than *calculate* it using a model.\n",
    "\n",
    "<center><img src=\"img/brain.png\" width=\"400px\"></img></center>\n",
    "\n",
    "Although we have introduced a fair amount of abstract concepts, it is important to keep in mind that these maths simply formalize an intuitive cognitive process. By experiencing rewards and punishments, we (humans) incrementally learn to evaluate the outcomes of our actions and then decide to act accordingly. This cognitive process is thus in line with the formalism we have introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c98508-18dc-44ad-af3e-dced0a6eedce",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Learning for policy evaluation\n",
    "\n",
    "## Sampling the return to learn a value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53369ce-5f63-401b-aa1e-700989362855",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's step aside from dynamic programming for a second and return to the definition of $Q^\\pi$. \n",
    "Recall that evaluating $Q^\\pi(s,a)$ is estimating the mathematical expectation of the *return* random variable $G^\\pi(s,a)$.\n",
    "\n",
    "**[Stochastic approximation](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full)** theory tells us that, for a given $s,a$ pair, given a series $g^\\pi_t$ of independent realizations of $G^\\pi(s,a)$, the sequence\n",
    "$q_{t+1} = q_t + \\alpha_t \\left(g^\\pi_t - q_t\\right)$\n",
    "converges to $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$, if the sequence of $\\alpha_t$ respects the Robbins-Monro conditions ($\\sum_t \\alpha_t = \\infty$ and $\\sum_t \\alpha_t^2 < \\infty$).\n",
    "\n",
    "**An intuitive reminder on Stochastic Approximation.**  \n",
    "\n",
    "    \n",
    "For those unfamiliar with stochastic approximation procedures, we can understand the previous update as: $g^\\pi_t$ are sample estimates of $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$. If I already have an estimate $q_t$ of $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$ and I receive a new sample $g^\\pi_t$, I should \"pull\" my previous estimate towards $g^\\pi_t$. But $g^\\pi_t$ carries a part of noise, so I should be cautious and only take a small step $\\alpha$ in the direction of $g^\\pi_t$.\n",
    "    \n",
    "In turn, the convergence conditions simply state that any value $Q^\\pi(s,a)$ should be reachable given any initial guess $Q(s,a)$, no matter how far is this first guess from $Q^\\pi(s,a)$; hence the $\\sum\\limits_{t=0}^\\infty \\alpha_t = \\infty$. However, we still need the step-size to be decreasing so that we don't start oscillating around $Q^\\pi(s,a)$ when we get closer; so to insure convergence we impose $\\sum\\limits_{t=0}^\\infty \\alpha_t^2 < \\infty$.\n",
    "\n",
    "So this provides us with a way to estimate $Q^\\pi(s,a)$ from experience samples rather than from a known probabilistic transition and reward model.  \n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Policy evaluation as stochastic approximation**  \n",
    "If we can obtain independent realizations $g^\\pi(s,a)$ of $G^\\pi(s,a)$ in all $s,a$, we can perform stochastic approximation updates of $Q$ under the form:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left(g^\\pi(s,a) - Q(s,a)\\right).$$\n",
    "Then $Q$ converges to $Q^\\pi$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ed3d6-f18f-4cd5-97fd-8672ef303720",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The stochastic gradient descent formulation.**\n",
    "\n",
    "A more modern formulation of Stochastic Approximation is Stochastic Gradient Descent. So we will slightly generalize the formulation above.  \n",
    "\n",
    "In each $(s,a)$ pair, $Q^\\pi(s,a)$ is the value that minimizes $\\ell(q) = \\frac{1}{2} \\mathbb{E}\\left[\\left(q - G^\\pi(s,a)\\right)\\right]^2$.\n",
    "\n",
    "Then this $\\ell(q)$ quantity can be approached with a Monte Carlo estimator. Given a set $\\left\\{g^\\pi_i(s,a)\\right\\}_{i\\in [1,N]}$ of $N$ independently drawn realizations of $G^\\pi(s,a)$,\n",
    "$$\\hat{\\ell}(q) = \\frac{1}{2} \\frac{1}{N} \\sum_{i=1}^N \\left[q - g^\\pi_i(s,a)\\right]^2.$$\n",
    "\n",
    "And its gradient is:\n",
    "$$\\nabla_q \\hat{\\ell}(q) = \\frac{1}{N} \\sum_{i=1}^N \\left[q - g^\\pi_i(s,a)\\right].$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Policy evaluation as stochastic gradient descent (1/2)**  \n",
    "If we can access a set $\\left\\{g^\\pi_i(s,a)\\right\\}_{i\\in [1,N]}$ of $N$ independently drawn realizations of $G^\\pi(s,a)$ in all $s,a$, we can perform stochastic gradient descent updates of $Q$ in each $s,a$ independently, under the form:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\frac{1}{N} \\sum_{i=1}^N \\left[g^\\pi_i(s,a) - q\\right].$$\n",
    "Then $Q$ converges to $Q^\\pi$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcbfb4-fb42-49f7-9cb5-f57b85ab61c7",
   "metadata": {},
   "source": [
    "One weakness of the formulation above is that we need to have a set $\\left\\{g^\\pi_i(s,a)\\right\\}_{i\\in [1,N]}$ in each $(s,a)$ pair separately, and perform the updates separately also for each $(s,a)$ pair.\n",
    "\n",
    "As a matter of fact, what we have in practice is not a separate set $\\left\\{g^\\pi_i(s,a)\\right\\}_{i\\in [1,N]}$ for each $(s,a)$ pair but, more generally, a set of pairs $\\left\\{(s,a)_i,g^\\pi_i(s,a)\\right\\}_{i\\in [1,N]}$ where the $(s,a)$ are drawn according to some marginal distribution of density $\\rho(s,a)$ and $g^\\pi_i(s,a)$ are (conditional) realizations of $G^\\pi(s,a)$.\n",
    "\n",
    "This permits casting the search for $Q^\\pi$ as one of **risk minimization**. We can define the risk on $Q$ as:\n",
    "$$L(Q) = \\mathbb{E}_{\\substack{(s,a)\\sim \\rho\\\\ g^\\pi(s,a)\\sim G^\\pi(s,a)}} \\left[ \\ell\\left( Q(s,a),g^\\pi(s,a) \\right)\\right].$$\n",
    "\n",
    "For the sake of readability, we shall write:\n",
    "$$L(Q) = \\mathbb{E}_{(s,a)\\sim \\rho} \\left[ \\ell\\left( Q(s,a), G^\\pi(s,a) \\right)\\right].$$\n",
    "\n",
    "\n",
    "That is, with $p(g|s,a,\\pi)$ the probability density of $G^\\pi(s,a)$:\n",
    "$$L(Q) = \\frac{1}{2} \\int_{S\\times A} \\int_\\mathbb{R} \\left[Q(s,a) - g \\right]^2 \\rho(s,a) p(g|s,a,\\pi) dsdadg$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ee712-a46c-47f9-b95f-18f47dfedc04",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Note that finding $Q^\\pi$ is not exactly the same thing as solving $\\min_Q L(Q)$! Why?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "If $\\rho(s,a)$ has zero mass on some $(s,a)$ pairs, then there might exist a minimizer of $L(Q)$ that is not $Q^\\pi$. So we already have a feeling that this distribution $\\rho$ will be important. This is the topic of a homework exercise.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d92fef-2eb8-452b-8f50-fb8e75751814",
   "metadata": {},
   "source": [
    "Nonetheless, if we want to eventually *learn* $Q$ from data, this data needs to be sampled somehow, and $\\rho$ is the marginal on $S\\times A$ of our sampling distribution $\\rho(s,a) p(g|s,a,\\pi)$ on $((s,a),g^\\pi(s,a))$ items.\n",
    "\n",
    "As previously, provided we have a set $\\{(s_i,a_i,g^\\pi_i\\}_{i\\in [1,N]}$ where the $(s_i,a_i)$ are independent realizations of $(s,a)$ drawn according to $\\rho$ and $g^\\pi_i$ are independent realizations of $G^\\pi(s,a)$, then a Monte Carlo estimator of $L(Q)$ is the **empirical risk**:\n",
    "$$\\hat{L}(Q) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\left[ Q(s_i,a_i) - g^\\pi_i \\right]^2.$$\n",
    "\n",
    "Now $Q$ lives in $\\mathbb{R}^{SA}$, which has infinite dimension in the general case. Taking the gradient of $L(Q)$ with respect to $Q$ is hence not feasible in terms of computation. Let us suppose that $Q$ is parameterized by a finite dimensional vector $\\theta \\in \\mathbb{R}^d$. This means $Q$ actually lives in a subset of $\\mathbb{R}^{SA}$ which is a manifold of dimension $d$. We will write $Q_\\theta$ to indicate this parameterization (sometimes, if it permits better readability, we will use $Q(s,a;\\theta)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8243c-67a0-4252-b272-674c6aa020c9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "What would be an obvious parameterization of $Q$ for a finite state and action space?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "If $S\\times A$ is discrete, then we can number its elements. If $(s,a)$ is the $i$th element in $S\\times A$, we write $Q(s,a) = \\theta_i$.  \n",
    "Consequently, there are $d=|S||A|$ elements in $\\theta$. This is a direct consequence of the isomorphism between $\\mathbb{R}^{SA}$ and $\\mathbb{R}^{|S||A|}$.\n",
    "\n",
    "This parameterization is **not** an approximation: all functions in $\\mathbb{R}^{SA}$ can be represented with such vectors (which won't be the case for infinite (eg. countable or continuous) state and action spaces.\n",
    "\n",
    "Note that although this parameterization is straightforward, it is not unique. For example any full rank linear transformation of $\\theta$ would also be a valid parameterization.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f319621-9672-492d-9813-659f58def483",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "For the parameterization of the previous exercise, what is $\\nabla_\\theta Q(s,a)$ in a given $(s,a)$?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "If $(s,a)$ is the $i$th element in $S\\times A$, then\n",
    "$\\nabla_\\theta Q(s,a) = \\left[ \\begin{array}{c} 0\\\\ \\vdots\\\\ 0\\\\ 1 \\\\ 0\\\\ \\vdots\\\\ 0 \\end{array} \\right]$, where the \"1\" is at position $i$ in the vector, that is the position corresponding to the $(s,a)$ pair.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935efd6-40d7-496f-89e3-b707c0e0835a",
   "metadata": {},
   "source": [
    "So we can now write the risk $L(Q_\\theta)$ as a function $L(\\theta)$. The previous Monte Carlo estimator of $L(\\theta)$ (the empirical risk) still holds:\n",
    "$$\\hat{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\left[ Q_\\theta(s_i,a_i) - g^\\pi_i \\right]^2.$$\n",
    "\n",
    "And we can now obtain a Monte Carlo estimator of $L(\\theta)$'s gradient (ie. the empirical risk's gradient):\n",
    "$$\\nabla_\\theta \\hat{L}(Q) = \\frac{1}{N} \\sum_{i=1}^N \\left[ Q_\\theta(s_i,a_i) - g^\\pi_i \\right] \\nabla_\\theta Q_\\theta(s_i,a_i).$$\n",
    "\n",
    "And finally:\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Policy evaluation as stochastic gradient descent (2/2)**  \n",
    "Provided we have a set $\\{(s_i,a_i,g^\\pi_i\\}_{i\\in [1,N]}$ where the $(s_i,a_i)$ are independent realizations of $(s,a)$ drawn according to $\\rho$ and $g^\\pi_i$ are independent realizations of $G^\\pi(s,a)$, then we can perform stochastic gradient descent updates of the parameters $\\theta$ of a parametric function $Q_\\theta$, under the form:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\frac{1}{N} \\sum_{i=1}^N \\left[g^\\pi_i - Q_\\theta(s_i,a_i) \\right] \\nabla_\\theta Q_\\theta(s_i,a_i).$$\n",
    "Then $Q_\\theta$ converges to some approximation of $Q^\\pi$ on the support of $\\rho$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7a087-505c-4022-b79d-9a7ab8d95763",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Consider a finite state-action space and separate sets $\\{g^\\pi_i(s,a)\\}_{i\\in [1,N]}$ for each $(s,a)$ pair. Use the expression of $\\nabla_\\theta Q_\\theta(s,a)$ derived previously to write an update on $Q(s,a)$ instead of $\\theta$\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Using the previous parameterization, $\\nabla_\\theta Q_\\theta(s,a)$ is a vector of zeros, with a single \"1\" at position $i$ corresponding to $(s,a)$. So the stochastic gradient descent update will only update parameter $\\theta_i$, which is precisely $Q(s,a)$. So the update boils down to:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\frac{1}{N} \\sum_{i=1}^N \\left[g^\\pi_i(s,a) - Q(s,a) \\right].$$\n",
    "\n",
    "We fall back to the previous $(s,a)$-wise update of $Q(s,a)$.  \n",
    "And if we take $N=1$ we fall back to the stochastic approximation update of $Q(s,a)$.\n",
    "</details>\n",
    "\n",
    "So, overall, if we manage to draw independent samples $g^\\pi(s_i,a_i)$ of $G^\\pi(s,a)$ in all $s,a\\in S\\times A$, we can **learn** the value $Q^\\pi$ (or $V^\\pi$) of policy $\\pi$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8413e9b-9808-4e3e-85c0-1dd13369cc01",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sampling the bootstrapped return: dynamic programming as a sequence of supervised learning problems\n",
    "\n",
    "Now, obtaining a sample $g^\\pi(s,a)$ of $G^\\pi(s,a)$ might be a bit costly, as it requires running a full trajectory and summing the observed rewards, as we have done in the previous chapters' exercises. It's doable (and is actually the topic of a series of homework exercises) but instead, we might want to sample from the bootstrapped return $G^\\pi_1(s,a,Q)$. This is a lot cheaper since it only requires sampling the next state.\n",
    "\n",
    "Recall that $$G^\\pi_1(s,a,Q) = R_0 + \\gamma Q(S_1, A_1) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_1 \\sim \\pi(S_1),\\\\ S_{1}\\sim p(\\cdot|S_0,A_0),\\\\ R_0 = r(S_0,A_0,S_{1}).\\end{array}$$\n",
    "\n",
    "So sampling from $G^\\pi_1(s,a,Q)$ is done by sampling a next state $s'$ according to $p(s'|s,a)$ and a next action $a'$ according to $\\pi(a'|s')$. The sample is then:\n",
    "$$g^\\pi_1(s,a,Q) = r(s,a,s') + \\gamma Q(s',a').$$\n",
    "\n",
    "Recall also that $$(T^\\pi Q)(s,a) = \\mathbb{E} \\left[ G^\\pi_1(s,a,Q) \\right].$$\n",
    "\n",
    "So sampling from $G^\\pi_1(s,a,Q)$ actually provides samples to learn $(T^\\pi Q)(s,a)$ (while sampling from $G^\\pi(s,a)$ provided samples to learn $Q^\\pi(s,a)$).  \n",
    "\n",
    "**If we repeatedly replace $Q$ in $G^\\pi_1(s,a,Q)$ by the value function we have previously learned, we are learning the sequence of approximate dynamic programming functions $Q_{n+1} = \\mathcal{A}T^\\pi Q_n$, where the approximation operator $\\mathcal{A}$ is the operation \"learn from samples of $G^\\pi_1(s,a,Q_n)$\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad8872-1634-4a4e-8f74-1a681654ecba",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Suppose Q-functions belong to some parametrized set of functions $Q(s,a;\\theta)$. The parameter of $Q_n$ is noted $\\theta_n$.  \n",
    "We want to approximate $(T^\\pi Q_n)(s,a)$ with $Q(s,a;\\theta_{n+1})$.  \n",
    "Use the quadratic loss function to define learning $T^\\pi Q_n$ as a risk minimization problem $\\min_\\theta L_n(\\theta)$. Then write the gradient of this risk with respect to the Q-function's parameters. Use this to introduce a stochastic gradient descent method to find $\\theta_{n+1}$.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa5d52-e6c7-4054-b940-41cd7669aef0",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Learning $Q_{n+1}$ is a regression problem. This problem takes inputs in $S\\times A$ and outputs in $\\mathbb{R}$.  \n",
    "To define the risk, we need to introduce the distribution over inputs and outputs. Let us write a marginal distribution $\\rho$ over inputs in $S\\times A$. For a given $s,a$, the probability law of $G^\\pi_1(s,a,Q_n)$ is fully determined by the transition model $p(s'|s,a)$ and the policy $\\pi(a'|s')$. \n",
    "\n",
    "The risk to minimize is then:\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{\\substack{(s,a) \\sim \\rho\\\\ g \\sim G^\\pi_1(s,a,Q_n)}}\\left[ \\left( Q(s,a;\\theta) - g \\right)^2 \\right] = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( Q(s,a;\\theta) - G^\\pi_1(s,a,Q_n) \\right)^2 \\right].$$\n",
    "\n",
    "In the expression above, $\\rho$ is a distribution over the state-action space. Intuitively, it should cover the important parts of the state-action space.\n",
    "\n",
    "The gradient of this risk is:\n",
    "$$\\nabla_\\theta L_n(\\theta) = \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( Q(s,a;\\theta) - G^\\pi_1(s,a,Q_n) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right].$$\n",
    "\n",
    "Recall that we can sample from $G^\\pi_1(s,a,Q_n)$ simply by sampling a next state $s'$ according to $p(s'|s,a)$ and a next action $a'$ according to $\\pi(s')$. The sample is then:\n",
    "$$g = r(s,a,s') + \\gamma Q(s',a';\\theta_{n}).$$\n",
    "\n",
    "So when we wrap all this together, the opposite of the gradient is:\n",
    "\n",
    "$$-\\nabla_\\theta L_n(\\theta) = \\mathbb{E}_{\\substack{(s,a) \\sim \\rho\\\\ s' \\sim p(\\cdot|s,a)\\\\ a'\\sim \\pi(s')}}\\left[ \\left( r(s,a,s') + \\gamma Q(s',a';\\theta_{n}) - Q(s,a;\\theta) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right]$$\n",
    "\n",
    "We can build a descent direction as a Monte Carlo estimate of $-\\nabla_\\theta L_n(\\theta)$, given a mini-batch of independently and identically drawn samples $\\left\\{\\left(s_i,a_i,r_i,s'_i\\right)\\right\\}_{i\\in [1,B]}$, with $(s,a) \\sim \\rho$ and $s',a' \\sim p(s' | s,a)\\pi(a'|s')$:\n",
    "$$d_n(\\theta) = \\frac{1}{B} \\sum_{i=1}^B \\left[ \\left( r_i + \\gamma Q(s_i',a';\\theta_{n}) - Q(s_i,a_i;\\theta) \\right) \\nabla_\\theta Q(s_i,a_i;\\theta) \\right].$$\n",
    "\n",
    "The stochastic gradient descent procedure builds a sequence of parameter values $\\theta_k$ such that:\n",
    "$$\\theta_{k+1} = \\theta_{k} + \\alpha_k d_n(\\theta_{k})$$\n",
    "\n",
    "By repeating such gradient steps, one progressively minimizes $L_n(\\theta)$ and finds $\\theta_{n+1}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad1701-f0bb-486a-a3f7-a4e8a23fdfe6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Approximate dynamic programming as a sequence of risk minimization problems.**  \n",
    "Approximate dynamic programming can be cast as finding the sequence of functions $Q(s,a;\\theta_n)$ defined by $\\theta_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta)$, with\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( Q(s,a;\\theta) - G^\\pi_1(s,a,Q_n) \\right)^2 \\right].$$\n",
    "\n",
    "If this risk is differentiable, provided one can draw a mini-batch of independently and identically drawn samples $\\left\\{\\left(s_i,a_i,r_i,s'_i\\right)\\right\\}_{i\\in [1,B]}$ (either by sampling from a larger training set, or directly from the system to control), with $(s,a) \\sim \\rho(\\cdot)$ and $s',a' \\sim p(s' | s,a)\\pi(a'|s')$, then one can derive a stochastic gradient descent learning procedure and iteratively learn $\\theta_{n+1}$ as the limit of the sequence $\\theta_{k+1} \\leftarrow \\theta_{k} + \\alpha_k d_n(\\theta_{k})$ with \n",
    "$$d_n(\\theta) = \\frac{1}{B} \\sum_{i=1}^B \\left[ \\left( r_i + \\gamma Q(s_i',a';\\theta_{n}) - Q(s_i,a_i;\\theta) \\right) \\nabla_\\theta Q(s_i,a_i;\\theta) \\right].$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5c6de-5bc6-4a36-95ed-4bca14dd3c8a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Sources of error in the approximation procedure**  \n",
    "Note that three independent factors might prevent us from actually learning $Q^\\pi$:\n",
    "1. $Q^\\pi$ might not live in the set of parametric functions $Q_\\theta$.\n",
    "2. The SGD updates might converge to a non-zero empirical risk (and to a non-zero risk).\n",
    "3. $\\rho$ might not cover appropriately the whole span of $S\\times A$.\n",
    "\n",
    "Factor 1 begs for good approximation methods, with little bias: universal function approximators will play a big role in value function learning.  \n",
    "Factor 2 begs for good SGD optimizers and maybe for regularization of $L_n(\\theta)$.  \n",
    "Factor 3 warns us about the interplay between collected samples and minimization of $L_n$ (which has consequences both for online and offline RL). \n",
    "\n",
    "Note that minimizing the empirical risk does not require it to be differentiable with respect to the parameters of $Q$. For instance, one could use decision trees or **[random forests](https://link.springer.com/article/10.1023/A:1010933404324)** for this purpose.  \n",
    "Note also that other objective functions can be used instead of the empirical risk, like regularized risk measures (as in **[support vector regression](https://link.springer.com/article/10.1023/B:STCO.0000035301.49549.88)** for instance).\n",
    "\n",
    "The goal of this section was to state an important idea: \n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Approximate dynamic programming can be tackled as a sequence of supervised learning problems.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441aa08-f863-4c1f-8cbc-4fb4ad49ae0d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Temporal differences: stochastic approximation and dynamic programming on the evaluation equation\n",
    "\n",
    "Let's apply what we have just written using stochastic approximation as a learning procedure, that is with a mini-batch size $B=1$ and samples drawn on-the-fly from interaction with the system to control, at each time step. To fix ideas, we will also write everything for finite state and action spaces MDPs. Thus, in this section, we will repeat (although maybe in a simpler way) many things we have stated in the previous one for the more general case of stochastic gradient descent.\n",
    "\n",
    "Consider the sample $(s_t,a_t,r_t,s_{t+1})$ obtained at time $t$.\n",
    "\n",
    "Once this transition is over we can update our knowledge of $Q(s_t, a_t)$ by using $r_t+\\gamma Q(s_{t+1},\\pi(s_{t+1}))$. This estimate uses $Q(s_{t+1},\\pi(s_{t+1}))$ to *bootstrap* the estimator of $Q(s_t, a_t)$.\n",
    "\n",
    "This idea was first introduced in R. Sutton's **[Learning to predict by the methods of temporal differences](https://link.springer.com/article/10.1007/BF00115009)** article.\n",
    "\n",
    "The bootstrapped sample $g^\\pi_t$ of $Q^\\pi(s_t,a_t)$ is obtained by summing $r_t$ and $\\gamma Q_t(s_{t+1}, \\pi(s_{t+1}) )$:\n",
    "$$g_t = r_t + \\gamma Q_t(s_{t+1}, \\pi(s_{t+1})).$$\n",
    "\n",
    "Note that in the expression above, we have used $Q_t$ to emphasize that we use the function $Q$ as it was at time step $t$, to define the target $g^\\pi_t$ used in the update that will provide $Q_{t+1}$.\n",
    "\n",
    "Formally, this comes directly from the evaluation operator. Let's rewrite $T^\\pi$ in terms of random variables.\n",
    "$$(T^\\pi Q)(s,a) = \\mathbb{E}_{R,S'}\\left[ R + \\gamma Q(S', \\pi(S')) \\right]$$\n",
    "\n",
    "Since $Q^\\pi$ is the fixed point of $T^\\pi$, by taking $g_t = r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1}))$ we are taking one stochastic approximation step in the direction of $T^\\pi Q_t$. \n",
    "\n",
    "**Bootstrapping** (in this particular context) is the operation of using the value of $Q_t(s_{t+1},\\pi(s_{t+1}))$ in the update of $Q$.\n",
    "\n",
    "Then the stochastic approximation update becomes what is called the **TD(0) update**:\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**TD(0) update:**  \n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left(r_t + \\gamma Q(s_{t+1}, \\pi(s_{t+1})) - Q(s_t,a_t)\\right).$$\n",
    "    \n",
    "This update consists in taking one stochastic approximation step in the direction of $T^\\pi Q$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a14db5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's insist on this point:  \n",
    "TD(0) does not directly solve $Q=\\mathbb{E}\\left[\\sum_t\\gamma^t R_t \\right]$ (this is what other methods, called *Monte Carlo*, do --- see the homework for details on Monte Carlo methods). Instead, it implements stochastic approximation on top of the repeated application of the $T^\\pi$ operator. So it \"solves\" approximately $Q_{n+1} = T^\\pi Q_n$. At each step $t$, it takes the current value function $Q_t$, draws one or several samples from $T^\\pi Q_t$ and approximates $T^\\pi Q_t$ by taking one step of gradient descent from $Q_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e82559",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\delta_t=r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1})) - Q_t(s_t,a_t)$ is called the prediction **temporal difference** (hence the name of the algorithm - the \"0\" won't be explained here). It is the difference between our estimate $Q_t(s_t,a_t)$ *before* obtaining the information of $r_t$, and the bootstrapped value $r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1}))$.\n",
    "<div class=\"alert alert-success\"><b>Temporal difference:</b>\n",
    "\n",
    "$$\\delta=r + \\gamma Q(s',\\pi(s')) - Q(s,a)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0a84a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now it seems obvious that if some state-action pair $s,a$ is never visited, then no update of its $Q(s,a)$ can ever take place. Therefore, for the TD(0) update to converge, we need to guarantee that all state-action pairs will be visited frequently enough for $Q$ to converge to $Q^\\pi$.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>TD(0) temporal difference update on $Q$-functions:</b><br>\n",
    "\n",
    "\n",
    "For a sample $(s,a,r,s')$, the temporal difference is:\n",
    "$$\\delta = r + \\gamma Q(s',\\pi(s')) - Q(s,a)$$\n",
    "And the TD update is:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s',\\pi(s')) - Q(s,a) \\right]$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, and under the Robbins-Monro conditions, this procedure converges to $Q^\\pi$.\n",
    "</div>\n",
    "\n",
    "Interestingly, this algorithm puts restrictions on the policy we apply when interacting with the environment. We will call such a policy a **behavior policy**. The behavior policy and the policy being learned might be different (in the case of TD(0), this even is an obligation since we need to enforce visitation of all state-action pairs).\n",
    "\n",
    "Vocabulary: **Off-policy** evaluation algorithms can use a behavior policy that is different than the policy being evaluated.\n",
    "\n",
    "Do you recall the distribution $\\rho$ on states and actions in the previous section? It is the distribution of states and actions under the behavior policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58916695",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together):**  \n",
    "Let's implement TD(0) on $Q$-functions.  \n",
    "To insure that all states and actions are sampled infinitely often, we take a behavior policy that acts randomly in each state.  \n",
    "The policy we evaluate is the policy that always moves to the right.  \n",
    "We take $\\gamma=0.9$ and run the algorithm for $10^6$ time steps.   \n",
    "To keep things simple, we take a constant $\\alpha=0.01$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b77e6-acfb-47dc-be07-56ffff2c5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first recall the model-based Q-function, so we can compare.\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "V_pi0, residuals = policy_eval_iter_mat2(env,pi0,1e-4,10000)\n",
    "Q_pi0 = Q_from_V(env,V_pi0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc911c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now let's implement TD(0)\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "max_steps=int(1e6)\n",
    "Qtd = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "error = np.zeros((max_steps))\n",
    "x,_ = env.reset()\n",
    "for t in tqdm(range(max_steps)):\n",
    "    a = np.random.randint(4)\n",
    "    y,r,d,_,_ = env.step(a)\n",
    "    Qtd[x,a] = Qtd[x,a] + alpha * (r+gamma*Qtd[y,fl.RIGHT]-Qtd[x,a])\n",
    "    error[t] = np.max(np.abs(Qtd-Q_pi0))\n",
    "    if d==True:\n",
    "        x,_ = env.reset()\n",
    "    else:\n",
    "        x=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f2231",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the difference between Qtd and Q_pi0\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Max error:\", np.max(np.abs(Qtd-Q_pi0)))\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de28fc-b2a2-4058-9e2c-43189e83af71",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Learning optimal value functions\n",
    "\n",
    "## Approximate Value Iteration as a sequence of supervised learning problems\n",
    "\n",
    "Extending the ideas developped in the previous section is quite straightforward when we remember that value iteration is actually the alternance of applying a *greediness operator* on a Q-function to define $\\pi$, then applying $T^\\pi$ to $Q$.\n",
    "\n",
    "Let's recall a few definitions of the previous chapter:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Greediness operator**  \n",
    "For deterministic policies:\n",
    "$$\\pi \\in \\mathcal{G} Q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{a\\in A} Q(s,a)$$\n",
    "\n",
    "This can be extended to stochastic policies:\n",
    "$$\\pi \\in \\mathcal{G} Q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{\\pi \\in \\Delta_A} \\mathbb{E}_{a\\sim\\pi} \\left[Q(s,a)\\right]$$\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Value iteration**\n",
    "$$\\pi_n \\in \\mathcal{G} Q_n, \\quad Q_{n+1} = T^{\\pi_n} Q_n.$$\n",
    "</div>\n",
    "\n",
    "Then, $Q^*$ is the limit reached by this sequence of $Q_n$ functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe473d-3e6e-4eda-bb5d-af81b3ad74cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, if we use an approximation procedure $\\mathcal{A}$ which minimizes the empirical risk when learning $T^{\\pi_n} Q_n$, we are learning $Q_{n+1}$. Then we can define $\\pi_{n+1} \\in \\mathcal{G} Q_{n+1}$ and repeat the learning procedure with $T^{\\pi_{n+1}} Q_{n+1}$.\n",
    "\n",
    "The key difference with what we wrote for the evaluation equation is that the policy being evaluated at each step of the sequence now changes, as it is defined as greedy with respect to the last learned value function.\n",
    "\n",
    "Let us sketch an algorithm out of this:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Approximate value iteration as a sequence of risk minimization problems**  \n",
    "$$\\pi_n \\in \\mathcal{G} Q_n,$$\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( Q(s,a;\\theta) - G^{\\pi_n}_1(s,a,Q_n) \\right)^2 \\right],$$\n",
    "$$\\theta_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta),$$\n",
    "$$Q_{n+1}(s,a) = Q(s,a;\\theta_{n+1}).$$\n",
    "</div>\n",
    "\n",
    "We won't go any further in this section and in particular we won't directly implement the algorithm above. The homework will guide you to go a bit further and future classes will implement it with random forests (yielding the **[Tree-based Fitted Q-Iteration](https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf)** algorithm) and with neural networks (yielding the famous **[Deep Q-networks](https://www.nature.com/articles/nature14236)** algorithm). The point of this section is really to cast the resolution of the Bellman equation as the very generic problem of a sequence of supervised learning problems. So we repeat what was already written in the previous section:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Approximate dynamic programming can be tackled as a sequence of supervised learning problems.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a73dc-edcb-40d7-b5c8-a23a2c0be8dc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Q-learning: approximate Value Iteration as stochastic approximation\n",
    "\n",
    "In this section we will adapt the reasoning we had earlier with TD learning: we will implement the approximate value iteration algorithm above using stochastic approximation as an approximation procedure, in discrete state and action space MDPs, and with deterministic policies. \n",
    "\n",
    "Note that if we use deterministic policies, then we don't even need to write $\\pi_n$ anymore: we can directly replace any mention of $\\pi_n(s)$ with an $\\arg\\max_a Q_n(s,a)$, and any mention of $Q_n(s,\\pi_n(s))$ by $\\max_a Q_n(s,a)$. \n",
    "\n",
    "So we directly adapt the idea of temporal difference learning to the approximate value iteration case.  \n",
    "\n",
    "In this case, we want to learn $T^* Q_t$ (instead of $T^\\pi Q_t$) so our samples are:\n",
    "$$g_t = r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a').$$\n",
    "\n",
    "And the learning algorithm becomes the famous **Q-learning** algorithm, introduced by C. J. Watkins in his [PhD thesis](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf) in 1989:\n",
    "<div class=\"alert alert-success\"><b>Q-learning</b><br>\n",
    "\n",
    "For a sample $(s,a,r,s')$, the temporal difference is:\n",
    "$$\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$$\n",
    "And the TD update is:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, and under the Robbins-Monro conditions, this procedure converges to $Q^*$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8228281",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To implement a Q-learning algorithm, one needs to decide on a behavior policy. As for TD(0), Q-learning will converge to $Q^*$, provided that all states and actions are visited infinitely often. It is actually notable that $Q$ converges to $Q^*$ even if the behavior policy does not. But it also looks like a waste of computational resources to keep exploring uniformly around the starting state.\n",
    "\n",
    "This tradeoff between exploring new actions and exploiting what has already been inferred in $Q$ is called the **exploration versus exploitation tradeoff**. It is a crucial problem that strongly affects the ability of the algorithm to discover new, interesting rewards.\n",
    "\n",
    "Here we will implement a rather naive tradeoff strategy called an **$\\epsilon$-greedy** behavior. It consists in picking the $Q$-greedy action with probability $1-\\epsilon$ and a random action with probability $\\epsilon$.\n",
    "\n",
    "$\\epsilon$ will start at 1 and then will periodically be divided by 2.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**\n",
    "\n",
    "Write a function that picks an epsilon-greedy action.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed71e74-a9f5-4820-8fc6-e7873ba88b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_epsilon_greedy.py\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(env, Q, s, epsilon):\n",
    "    a = np.argmax(Q[s,:])\n",
    "    if(np.random.rand()<=epsilon): # random action\n",
    "        aa = np.random.randint(env.action_space.n-1)\n",
    "        if aa==a:\n",
    "            a=env.action_space.n-1\n",
    "        else:\n",
    "            a=aa\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215aa9d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**\n",
    "\n",
    "Write a Q-learning algorithm on FrozenLake. Keep track of the error w.r.t. $Q^*$ and the number of times each state-action pair is visited.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2efaa7-fad8-4d78-905d-c972b69784f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recall the optimal value function from the previous chapter\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_value_iteration import value_iteration\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(env,Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(env,Vstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2628b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Implement Q-learning. \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "Qql = np.zeros((env.observation_space.n,env.action_space.n)) \n",
    "count = np.zeros((env.observation_space.n,env.action_space.n)) # to track update frequencies\n",
    "max_steps = int(2e6)\n",
    "epsilon = 1\n",
    "epsilon_update_period = int(1e6)\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "error = np.zeros((max_steps))\n",
    "x,_ = env.reset()\n",
    "for t in tqdm(range(max_steps)):\n",
    "    if((t+1)%epsilon_update_period==0):\n",
    "        epsilon = epsilon/2\n",
    "    a = epsilon_greedy(env,Qql,x,epsilon)\n",
    "    y,r,d,_,_ = env.step(a)\n",
    "    Qql[x][a] = Qql[x][a] + alpha * (r+gamma*np.max(Qql[y][:])-Qql[x][a])\n",
    "    count[x][a] += 1\n",
    "    error[t] = np.max(np.abs(Qql-Qstar))\n",
    "    if d==True:\n",
    "        x,_ = env.reset()\n",
    "    else:\n",
    "        x=y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e362e69-083e-49e2-bfec-97d289cc9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the difference between Qql and Qstar\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Max error:\", np.max(np.abs(Qql-Qstar)))\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137060b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**\n",
    "\n",
    "Display the visitation frequency</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d290a7e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from solutions.fl_actions import actions\n",
    "from solutions.fl_to_row_col import to_row_col\n",
    "\n",
    "count_map = np.zeros((env.unwrapped.nrow, env.unwrapped.ncol, env.action_space.n))\n",
    "for a in range(env.action_space.n):\n",
    "    for x in range(env.observation_space.n):\n",
    "        row,col = to_row_col(env,x)\n",
    "        count_map[row, col, a] = count[x,a]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4)\n",
    "for a in range(env.action_space.n):\n",
    "    name = \"a = \" + actions[a]\n",
    "    axs[a].set_title(name)\n",
    "    axs[a].imshow(np.log(count_map[:,:,a]+1), interpolation='nearest')\n",
    "    #print(\"a=\", a, \":\", sep='')\n",
    "    #print(count_map[:,:,a])\n",
    "plt.show()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71af86-0086-4f94-a3ae-970eb1167069",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**\n",
    "\n",
    "Display the final policy and its state occupancy.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f5065-2476-4fe2-869f-947ea4fdf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_greedyQpolicy import greedyQpolicy\n",
    "from solutions.fl_print_policy import print_policy\n",
    "from solutions.fl_P_and_r import fl_P_and_r\n",
    "from solutions.fl_state_occupancy_measure import state_occupancy_measure\n",
    "from solutions.fl_display_function_of_state import display_function_of_state\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "# Print greedy policy\n",
    "pi_ql = greedyQpolicy(env,Qql)\n",
    "print_policy(env,pi_ql)\n",
    "\n",
    "# Compute and display state occupancy measure.\n",
    "rho0 = np.zeros((env.observation_space.n))\n",
    "rho0[0] = 1\n",
    "P_pi,r_pi = fl_P_and_r(env,pi_ql)\n",
    "gamma = 0.9\n",
    "horizon = 200\n",
    "rho_pi = state_occupancy_measure(P_pi,rho0,gamma,horizon)\n",
    "display_function_of_state(rho_pi)\n",
    "\n",
    "# Making this graphical\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "axs[0].imshow(rho_pi.reshape((4,4)), interpolation='nearest')\n",
    "axs[1].imshow(np.log(rho_pi.reshape((4,4))), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0771a77",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**  \n",
    "Did we find the same policy as when solving the Bellman equation with the model?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bad45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from solutions.fl_greedyQpolicy import greedyQpolicy\n",
    "from solutions.fl_print_policy import print_policy\n",
    "\n",
    "pi_ql = greedyQpolicy(env,Qql)\n",
    "print(\"Greedy Q-learning policy:\")\n",
    "print_policy(env,pi_ql)\n",
    "pi_star = greedyQpolicy(env,Qstar)\n",
    "print(\"Optimal policy:\")\n",
    "print_policy(env,pi_star)\n",
    "print(\"Are the policies the same?\", pi_ql-pi_star)\n",
    "index = np.argmax(np.abs(pi_ql-pi_star))\n",
    "print(\"Q-learning in the state of discrepancy:\", Qql[index])\n",
    "print(\"Q* in the state of discrepancy:        \", Qstar[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fdefc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155e387",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Chapter wrap-up\n",
    "\n",
    "Previous chapters have shown how to characterize and find optimal policies given the MDP model. We have built on the results of Approximate Dynamic Programming to **learn** optimal value functions from interaction samples.\n",
    "\n",
    "So we have built the third stage of our three-stage rocket defined in the introduction chapter.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**How do we learn an optimal strategy?**  \n",
    "To learn an optimal strategy, we rely on a risk minimization procedure to learn of $Q^*$, given samples drawn from the MDP. This risk minimization learning of $Q^*$ can be a stochastic approximation of the $Q_n$ sequence of approximate value iteration. In the end, we need to find good approximation architectures and to control the exploration versus exploitation tradeoff.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff413c8-ffb3-4529-99e9-6b429096e2f7",
   "metadata": {},
   "source": [
    "## Author's commentary on this class\n",
    "\n",
    "If you have taken classes on RL before, or read books, or blog posts, you might have noticed the present chapter has a different feel than most introductions to TD-learning or Q-learning.   \n",
    "\n",
    "Most classical RL classes won't go through all the details of SGD, and function approximation, and empirical risk minimization, to finally reach TD-learning and Q-learning. Actually, the subsections on TD(0) and Q-learning above can probably be read without going through their respective predecessors.  \n",
    "\n",
    "But this is the XXIst century: most algorithms don't work in tabular form, data is abundant, neural networks (and other function approximators) are everywhere. So I made the deliberate choice **not** to write a class about TD-learning and Q-learning, but rather to discuss how one can implement approximate dynamic programming as a sequence of risk minimization problems, in its full generality. TD-learning and Q-learning are important landmarks in RL history. They help grasping intuitions and concepts. But they are also special cases of risk minimization applied to the dynamic programming sequence of functions arising from Bellman equations. So my belief is that a **modern** class on RL should not present them as independent algorithms but should rather fully set the stage they belong to, and only then do them historical justice by presenting them as the important milestones they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0113d2-e37b-465a-9467-4d6a6ffcf89f",
   "metadata": {},
   "source": [
    "## Wrapping-up on the general definition of RL\n",
    "\n",
    "Let's take a step back and provide a more general summary of the whole class this far.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**What is Reinforcement Learning?**  \n",
    "RL is the discipline that studies the *learning* process of optimal control policies in the MDP framework.  \n",
    "Its roots overlap Cognitive Psychology, Control Theory, Artificial Intelligence, Machine Learning.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**What are the building blocks of RL?**  \n",
    "RL is built upon the framework of Markov Decision Processes.  \n",
    "It draws from the characterization of optimal policies that maximize a given criterion, notably through Bellman's equations.  \n",
    "It learns (notably through stochastic approximation or SGD) solutions to these equations using interaction samples.\n",
    "</div>\n",
    "\n",
    "Of course we have barely touched the surface of RL for now. We haven't explored the weaker notion of optimality and the direct policy search methods sketched out in chapter 1 for instance. The overall goal of these first chapters was to acquire a common vocabulary and set of concepts, so that you become comfortable with the objects often manipulated in RL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e2bfe-7bf5-4e25-afc3-707d315e2d5c",
   "metadata": {},
   "source": [
    "## Three intrinsic challenges in Reinforcement Learning\n",
    "\n",
    "From here, we can identify three challenges which make the RL problem intrinsically difficult:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Intrinsic challenges in RL:**  \n",
    "- function approximation,\n",
    "- the improvement problem,\n",
    "- the exploration versus exploitation trade-off.\n",
    "</div>\n",
    "\n",
    "These three challenges are quite independent and we could study them in any order.\n",
    "\n",
    "As we have seen, **function approximation** is key in finding good policies. Although function approximation does not intrinsically require stochastic gradient descent, the interplay between Deep Learning and Reinforcement Learning has triggered major advances in RL. \n",
    "Chapter 4 is dedicated to this topic and will lead us to manipulate function approximators in AVI, including deep neural networks. \n",
    "\n",
    "The ideas we developped in this class relied on estimating value functions to deduce greedy policies. Finding such greedy policies was made easy because actions were discrete. But **finding improving policies** is actually a challenge in itself. This problem is present both when one searches for a greedy action with respect to $Q$, and when one directly aims at solving the $\\max_\\pi J(\\pi)$ problem without going through the proxy of the optimality equation. Chapter 5 will take us towards the realm of continuous actions in AVI. Chapter 6 will tackle direct policy search and resolution of the $\\max_\\pi J(\\pi)$ problem, notably through policy gradient algorithms.\n",
    "\n",
    "Behavior policies are a cornerstone of RL: which action should one take to obtain informative samples? Should one explore uniformly the environment or rather follow a policy that takes the system towards promising states before exploring more agressively? This is called the **tradeoff between exploration and exploitation**. Chapters 7 and 8 will be dedicated to properly studying this tradeoff through the theory of stochastic bandits, which leads to the UCT and Monte Carlo Tree Search algorithms that are at the root of [alphaGo](https://www.youtube.com/watch?v=WXuK6gekU1Y) and the subsequent alphaZero algorithms.  \n",
    "\n",
    "## Subtopics in RL\n",
    "\n",
    "Beyond these three intrinsic challenges, there are countless, context-dependent, open questions in RL, that form a span of specific questions:\n",
    "- Hierarchical RL\n",
    "- RL from human feedback\n",
    "- World (surrogate) models\n",
    "- Multi-agent RL\n",
    "- Partially observable MDPs\n",
    "- Robust RL\n",
    "- Offline RL\n",
    "- Consolidation and Transfer in RL\n",
    "- Causal RL\n",
    "- and many more (not counting all the application fields)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920e970-db85-44be-961d-a59de7e4f3cb",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "The exercises below are here to help you play with the concepts introduced above, to better grasp them. They also introduce additional important notions. They are not optional to reach the class goals. Often, the provided answer reaches out further than the plain question asked and provides comments, additional insights, or external references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1817a155",
   "metadata": {},
   "source": [
    "## Variations on TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08af4f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Write a function `TD_Qeval` that runs TD(0) on tabular $Q$ functions for a discrete state-action environment `env` and a given deterministic policy `pi` (it's almost the same code than in class). The stochastic behavior policy is provided through a `beta` array of size $|S|\\times |A|$ indicating the probability of picking each action in each state. To keep things simple, use a constant learning rate $\\alpha$. Add an option for providing the true $Q$ function and monitoring the error along training. Use the code below to compute the $Q$ function of the policy that always goes right in FrozenLake and plot the evolution of the error between `Qtrue` (recall the model-based computations we used in class) and $Q$.  \n",
    "The signature of your function should be `TD_Qeval(env, pi, beta, max_steps, alpha, gamma, Qinit=None, Qtrue=None)` and it should return the learned Q-function and the sequence of stepwise $\\|\\|_\\infty$ errors between $Q$ and $Q^\\pi$ if the latter was provided through `Qtrue`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "\n",
    "def TD_Qeval(env, pi, beta=None, max_steps=int(1e6), alpha=0.001, gamma=0.9, Qinit=None, Qtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (beta is None):\n",
    "        beta = (1./env.action_space.n) * np.ones((env.observation_space.n,env.action_space.n))\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q = np.copy(Qinit)\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps, disable=disable_tqdm)):\n",
    "        a = np.random.choice(env.action_space.n, p=beta[x,:])\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][pi[y]]-Q[x][a])\n",
    "        if (Qtrue is not None):\n",
    "            error[t] = np.max(np.abs(Q-Qtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return Q, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e73c06-f2a5-42be-b479-cf39778f9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_TD_Qeval.py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def TD_Qeval(env, pi, beta=None, max_steps=int(1e6), alpha=0.001, gamma=0.9, Qinit=None, Qtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (beta is None):\n",
    "        beta = (1./env.action_space.n) * np.ones((env.observation_space.n,env.action_space.n))\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q = np.copy(Qinit)\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        a = np.random.choice(env.action_space.n, p=beta[x,:])\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][pi[y]]-Q[x][a])\n",
    "        if(Qtrue is not None):\n",
    "            error[t] = np.max(np.abs(Q-Qtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return Q, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67ac099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2668/3000000 [00:00<07:13, 6918.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000000/3000000 [09:00<00:00, 5547.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0.04975919896883785\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGvCAYAAACJsNWPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7xUlEQVR4nO3deXhU5f3+8XsmKxASlkDCEgiLbIIJBogBEZBgEKpFreXbXxWLSxXRYqPWoBVE1FBXqtCitIJiK4iK1oIsBkHFKDUBZAn7viQBgQQCJGTm/P6gPTgkwUxI5szyfl3XXD7nOc+ZfHIcJ7dneY7NMAxDAAAAFrFbXQAAAAhshBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKWCrS6gOpxOpw4ePKiGDRvKZrNZXQ4AAKgGwzB04sQJtWzZUnZ71cc/fCKMHDx4UHFxcVaXAQAAamDfvn1q3bp1let9Iow0bNhQ0rlfJjIy0uJqAABAdRQXFysuLs78O14Vnwgj/zs1ExkZSRgBAMDH/NQlFlzACgAALEUYAQAAliKMAAAAS9UojEyfPl3x8fEKDw9XcnKyVq9efdHxx48f19ixY9WiRQuFhYWpU6dOWrRoUY0KBgAA/sXtC1jnzZun9PR0zZgxQ8nJyZo6darS0tK0ZcsWNW/evML4srIyDRkyRM2bN9f777+vVq1aac+ePWrUqFFt1A8AAHyczTAMw50NkpOT1bt3b02bNk3SuQnJ4uLi9OCDDyojI6PC+BkzZuiFF17Q5s2bFRISUqMii4uLFRUVpaKiIu6mAQDAR1T377dbp2nKysqUk5Oj1NTU829gtys1NVXZ2dmVbvOvf/1LKSkpGjt2rGJiYtS9e3c999xzcjgcVf6c0tJSFRcXu7wAAIB/ciuMHDlyRA6HQzExMS79MTExys/Pr3SbnTt36v3335fD4dCiRYv05JNP6qWXXtIzzzxT5c/JzMxUVFSU+WL2VQAA/Fed303jdDrVvHlzvfHGG0pKStLIkSP1xBNPaMaMGVVuM378eBUVFZmvffv21XWZAADAIm5dwBodHa2goCAVFBS49BcUFCg2NrbSbVq0aKGQkBAFBQWZfV27dlV+fr7KysoUGhpaYZuwsDCFhYW5UxoAAPBRbh0ZCQ0NVVJSkrKyssw+p9OprKwspaSkVLpNv379tH37djmdTrNv69atatGiRaVBBAAABBa3T9Okp6dr5syZeuutt5SXl6cxY8aopKREo0ePliSNGjVK48ePN8ePGTNGR48e1bhx47R161YtXLhQzz33nMaOHVt7vwUAAPBZbs8zMnLkSB0+fFgTJkxQfn6+EhMTtXjxYvOi1r1798puP59x4uLitGTJEv3+97/XFVdcoVatWmncuHF67LHHau+3qKGn/rVRh0+W6vlbrlCDMJ94ZiAAAH7H7XlGrFBX84z0euYzHTlZqmu7NNebv+lda+8LAADqaJ4Rf3PkZKkkafnmQosrAQAgcAV0GPlobD+zvXjDIQsrAQAgcAV0GEmMa2S2F6w5YF0hAAAEsIAOI5I0ste52V2XbCxQWbnzJ0YDAIDaFvBh5MHBHc32w/PXWVgJAACBKeDDSOvG9RUZfu623k/WHVRJabnFFQEAEFgCPoxI0opHB5ntN77YaWElAAAEHsKIpCYNzk9L/92eoxZWAgBA4CGM/NddV7eTJK3a/oPKHVzICgCApxBG/uvGhJZme9GGfAsrAQAgsBBG/ivhR3OO/O7dNdYVAgBAgCGM/Mh9AzqY7f9NFQ8AAOoWYeRH0od0Mttb8k9YWAkAAIGDMPIjocHnd8esVbutKwQAgABCGLlAn/gmkqRdR05aXAkAAIGBMHKB6y6PkSTtOFxicSUAAAQGwsgFurWINNsf8SRfAADqHGHkAikdmprth+at1akynlUDAEBdIoxcwGazae5vrzKXv93J9PAAANQlwkglrmp//ujInG/2WFgJAAD+jzBShX4dzwWS5ZsLLa4EAAD/Rhipwo8nQDt+qszCSgAA8G+EkSokxjU224++/72FlQAA4N8II1UIstvUtml9SdKyTQUqdzgtrggAAP9EGLmI52+5wmzf+nq2hZUAAOC/CCMX0addE7O9Zu9xvbt6r4XVAADgnwgjF2Gz2fTpuP7m8vOLN1tYDQAA/okw8hO6tojU1JGJkqRjp87KMAxrCwIAwM8QRqphYOdmZvsvK3ZYWAkAAP6HMFINjeqHmu0XlmyR08nREQAAagthpJpeujXBbN84/SsLKwEAwL8QRqppRM9WZnvDgWKVljssrAYAAP9BGKmmILtNGyalmct5h05YWA0AAP6DMOKGiLBgs71o/SELKwEAwH8QRtxks53756aDxdYWAgCAnyCMuGlAp3O3+a7accTiSgAA8A+EETcN6txcksTcZwAA1A7CiJuS2jY223mHOFUDAMClIoy4qXurKLM9kif5AgBwyQgjNXDTf+ccKT5Trn1HT1lcDQAAvo0wUgPPjOhuth/4Z64cTA8PAECNEUZqoEFYsC5rHiFJWre/SI/MX2dxRQAA+C7CSA1N/tHRkQVrDsjg9hoAAGqEMFJDV7VvqsUP9TeXb/rL1zpWUmZhRQAA+CbCyCXoEhtpttfuO66ek5eprNxpYUUAAPgewsgl+uLRQS7LC9bst6gSAAB8E2HkErVpWl87nxtmLj/2wXoLqwEAwPcQRmqB3W7Tr/q0MZcPHD9tYTUAAPgWwkgteXxYF7P96mfbLKwEAADfQhipJQ3DQ8z2vO/2WVgJAAC+hTBSi+7s187qEgAA8Dk1CiPTp09XfHy8wsPDlZycrNWrV1c5dvbs2bLZbC6v8PDwGhfszW6+spXZ/mLrYQsrAQDAd7gdRubNm6f09HRNnDhRubm5SkhIUFpamgoLC6vcJjIyUocOHTJfe/bsuaSivVW3FufnHRn15mqeWQMAQDW4HUZefvll3XPPPRo9erS6deumGTNmqH79+nrzzTer3MZmsyk2NtZ8xcTEXFLR3sput+n+gR3M5Q6PL9Lnm6sOaQAAwM0wUlZWppycHKWmpp5/A7tdqampys7OrnK7kydPqm3btoqLi9PPf/5zbdy48aI/p7S0VMXFxS4vX/GHoV1clkfP/o9FlQAA4BvcCiNHjhyRw+GocGQjJiZG+fn5lW7TuXNnvfnmm/r444/1zjvvyOl0qm/fvtq/v+qZSjMzMxUVFWW+4uLi3CnTchsmpalNk/rmclZegYXVAADg3er8bpqUlBSNGjVKiYmJGjBggD788EM1a9ZMr7/+epXbjB8/XkVFReZr3z7fulU2IixYSx66xly+663vLKwGAADv5lYYiY6OVlBQkAoKXP9Pv6CgQLGxsdV6j5CQEPXs2VPbt2+vckxYWJgiIyNdXr6mXmiQHvvRKRsnF7MCAFApt8JIaGiokpKSlJWVZfY5nU5lZWUpJSWlWu/hcDi0fv16tWjRwr1KfdDofvFmu/BEqXWFAADgxdw+TZOenq6ZM2fqrbfeUl5ensaMGaOSkhKNHj1akjRq1CiNHz/eHP/0009r6dKl2rlzp3Jzc3Xbbbdpz549uvvuu2vvt/BS4SFBZvuqzCwZBkdHAAC4ULC7G4wcOVKHDx/WhAkTlJ+fr8TERC1evNi8qHXv3r2y289nnGPHjumee+5Rfn6+GjdurKSkJH399dfq1q1b7f0WXqx+aJBOlTkkSY998L2e/0WCxRUBAOBdbIYP/O96cXGxoqKiVFRU5HPXjxw5Wapez3xmLs+/L0W945tYWBEAAJ5R3b/fPJumjkVHhGneb68yl2+dUfV8LAAABCLCiAckt2+qGxNamss+cDAKAACPIYx4yOSfdzfbfacst7ASAAC8C2HEQ6Lqh5jtQ0VndNbhtLAaAAC8B2HEg9ZNvM5sbys4aWElAAB4D8KIB0XVO3905Ol/X/xhgQAABArCiIc1CD03Edo3O49q2SYeoAcAAGHEw96+K9ls3/M2D9ADAIAw4mFJbRvrzn7tzOV/rTtoYTUAAFiPMGKB+wd1MNtLNuZbWAkAANYjjFggOiJMnWIiJEkLvz+klVsPW1wRAADWIYxY5I6+8efbb67W1oIT1hUDAICFCCMW+X992uiGH00RP/7D9RZWAwCAdQgjFrHZbHrtVz01uEtzSVLOnmPadaTE4qoAAPA8wojFnhje1WzPyd5jYSUAAFiDMGKx9s0i1LH5uYtZ387ebW0xAABYgDDiBVLaN5UklTsNiysBAMDzCCNe4KYrW5ntnD1HLawEAADPI4x4gcTWjcz2LX/Ntq4QAAAsQBjxAna7TVd3jDaXj5WUWVgNAACeRRjxErNH9zbbPScv05mzDgurAQDAcwgjXiI4yC6b7fzyc4vyrCsGAAAPIox4kfVPpZntt7P36Pv9x60rBgAADyGMeJGIsGB9NLafuXzjtFXaXnjSwooAAKh7hBEvkxjXSOlDOpnLqS+v1MHjpy2sCACAukUY8UK/G3yZxl/fxVz+64odFlYDAEDdIox4qXsHdFDTBqGSpDnf7NHfv9plcUUAANQNwogXS+sea7Yn/3uTHpm/zsJqAACoG4QRL/bY0C5KatvYXH4/Z7/OOpwWVgQAQO0jjHixqHoh+mBMXz0wqKPZ9+7qvRZWBABA7SOM+IBH0jqb7Qkfb9SavccsrAYAgNpFGPERE2/oZrZv+svXmvPNHgurAQCg9hBGfMQdKfH6Td94c/nJjzbI4TSsKwgAgFpCGPERdrtNT914uWbcdqXZt2xTgYUVAQBQOwgjPmZo9xZm+753cmQYHB0BAPg2wogPGpXS1mxnfLDewkoAALh0hBEf9Mfh5y9mnffdPp0uc1hYDQAAl4Yw4oNCg+1acH9fc5m5RwAAvoww4qN6tjk/M+vT/94kJ3fWAAB8FGHEhz08pJPZbv/4Ii5mBQD4JMKID3tw8GVqGB5sLl+VmWVhNQAA1AxhxMf954lUs11QXKpNB4strAYAAPcRRnxceEiQvn18sLn80tItFlYDAID7CCN+ICYy3GxnbS7Uo/PXWVgNAADuIYz4iTl39THb83P26/ipMgurAQCg+ggjfqL/Zc1cnluT+PQylTucFlYEAED1EEb8yNDuLRRVL8Rc7vjEp8o7xAWtAADvRhjxM5+O6++yfP2fv1RZOUdIAADeizDiZ1o2qqedzw1zOUIybfk2CysCAODiCCN+yG63ae2EIebyq8u3W1gNAAAXRxjxUzabzWW6+DNnebIvAMA7EUb82H0DO5jtLk8u1tTPtlpYDQAAlatRGJk+fbri4+MVHh6u5ORkrV69ulrbzZ07VzabTSNGjKjJj4WbQoJc//VO/WwbM7QCALyO22Fk3rx5Sk9P18SJE5Wbm6uEhASlpaWpsLDwotvt3r1bjzzyiPr373/Rcahd6yZc57L8GtePAAC8jNth5OWXX9Y999yj0aNHq1u3bpoxY4bq16+vN998s8ptHA6Hfv3rX2vSpElq3779JRUM90TVD9GuzGG6qWcrs6+g+IyFFQEA4MqtMFJWVqacnBylpp5/Uqzdbldqaqqys7Or3O7pp59W8+bNddddd1Xr55SWlqq4uNjlhZqz2WzKvLmHubx0U4GF1QAA4MqtMHLkyBE5HA7FxMS49MfExCg/P7/Sbb766iv9/e9/18yZM6v9czIzMxUVFWW+4uLi3CkTlQgPCVKQ3SZJCvnvPwEA8AZ1ejfNiRMndPvtt2vmzJmKjo6u9nbjx49XUVGR+dq3b18dVhk4hvVoIUnK+HC9DMOwuBoAAM4JdmdwdHS0goKCVFDgepi/oKBAsbGxFcbv2LFDu3fv1g033GD2OZ3npiYPDg7Wli1b1KFDhwrbhYWFKSwszJ3SUA2FP7pWJCuvUKndYi4yGgAAz3DryEhoaKiSkpKUlZVl9jmdTmVlZSklJaXC+C5dumj9+vVau3at+brxxhs1aNAgrV27ltMvHvb2XX3M9t1vf2dhJQAAnOfWkRFJSk9P1x133KFevXqpT58+mjp1qkpKSjR69GhJ0qhRo9SqVStlZmYqPDxc3bt3d9m+UaNGklShH3UvLDhIgzo30+dbDkuS/rJiu+4f2NHiqgAAgc7tMDJy5EgdPnxYEyZMUH5+vhITE7V48WLzota9e/fKbmdiV2/1tzt6q8PjiyRJzy/eomsua6ZuLSJl56JWAIBFbIYPXMlYXFysqKgoFRUVKTIy0upyfN6GA0X62WtfufRtejpN9UPdzqYAAFSpun+/OYQRgLq3iqrQN2PFDgsqAQCAMBKwtj17vWb9pre5/Ory7co7xORyAADPI4wEqJAguwZ1aa5H0zqbfdf/+UuVlTstrAoAEIgIIwFu7KCO6tmmkbn8+kpO1wAAPIswAs2/9/wcMTzVFwDgaYQRKDjIrhdvTZAklTmcOllabnFFAIBAQhiBJOmmnq3M9rc7f7CwEgBAoCGMQJIUZLepUf0QSdJfuc0XAOBBhBGYQoPOfRy+23PM4koAAIGEMALTlFt6mO34jIXaeLDIwmoAAIGCMALTtV1iXJaHv/pVFSMBAKg9hBG4yHp4gMvyyq2HLaoEABAoCCNw0aFZhDY9nWYuP/jPXAurAQAEAsIIKqgfGqw+8U0kScVnylVa7rC4IgCAPyOMoFJP/qyb2e78x8U66+CZNQCAukEYQaV6tI5yWX552VaLKgEA+DvCCKq0K3OY2V6797h1hQAA/BphBFWy2Wy65crWkqTsnT+orJxTNQCA2kcYwUVd26W52U6avMzCSgAA/oowgosa1iPWbJ8oLVd8xkLtO3rKwooAAP6GMIKLstlsmn9fiktf/+c/V6cnPtXTn2zi1A0A4JIRRvCTerVtrDv7tXPpK3M49eaqXer0x0918PhpiyoDAPgDwgh+ks1m04Qbumn3lOF6eEinCuv7TlmuM2eZGA0AUDOEEbjlwcGXaesz1+ub8YNd+v/x7V6LKgIA+DrCCNwWGmxXbFS4dk8ZrsjwYEnSNzt/sLgqAICvIozgktz833lIlm0qsLgSAICvIozgktyQ0MJsl5SWW1gJAMBXEUZwSa5s09hs/2JGtoWVAAB8FWEEl8Rms5ntvEPFKufpvgAANxFGcMnuuvr8HCT3zsmxsBIAgC8ijOCS/XF4V7OdtblQP5/2FfOOAACqjTCCS2az2TThZ93M5XX7i9TlycUWVgQA8CWEEdSK0f3i1bVFpEtf+ntrrSkGAOBTCCOoFTabTZ+O668nf3SE5MPcAzIMw8KqAAC+gDCCWnXX1e30dca15vLc/+yzsBoAgC8gjKDWtWxUz2yP/3C9vth6WEdLyiysCADgzQgjqBN/GNrZbI96c7WunLxMizccsrAiAIC3IoygTowZ0KFC333v5OrzzYUWVAMA8GaEEdQJm82m3VOGa+5vr1J0RKjZP3r2fxSfsVAHj5+2sDoAgDchjKBOXdW+qb59PLVCf98py+VwcqcNAIAwAg8Istu0YVKaZv2mt0v/I/PXWVQRAMCbEEbgERFhwRrUpbm2P3u92bdgzQELKwIAeAvCCDwqOMiu3w2+zFx+8qMNFlYDAPAGhBF43P0Dz99pM+ebPZr5xU4LqwEAWI0wAo8LDwnS8B4tzOVnF+VZWA0AwGqEEVhi+q+vVEr7puZy4YkzFlYDALASYQSW+ec9yWa7z7NZFlYCALASYQSWsdlsim9a31ye/x0P1QOAQEQYgaU+GNPXbD/6/vcWVgIAsAphBJZqGhGmN3/Ty1yOz1ioqZ9ttbAiAICnEUZguUGdm7ssT/1sm+IzFsowDBkGU8YDgL+rURiZPn264uPjFR4eruTkZK1evbrKsR9++KF69eqlRo0aqUGDBkpMTNScOXNqXDD8j81m05onh1Tobzd+kdqNX6Rxc9fozFmHBZUBADzB7TAyb948paena+LEicrNzVVCQoLS0tJUWFj5o+GbNGmiJ554QtnZ2fr+++81evRojR49WkuWLLnk4uE/GjcI1e4pw/XPu5MrrPt47UF1eXIxgQQA/JTNcPM4eHJysnr37q1p06ZJkpxOp+Li4vTggw8qIyOjWu9x5ZVXavjw4Zo8eXK1xhcXFysqKkpFRUWKjIx0p1z4oMMnSjX4pRUqPlPu0n9DQku99queFlUFAHBXdf9+u3VkpKysTDk5OUpNPf9IeLvdrtTUVGVnZ//k9oZhKCsrS1u2bNE111xT5bjS0lIVFxe7vBA4mjUM0/dPpWn3lOF69qbuZv8n6w5q1qpdFlYGAKgLboWRI0eOyOFwKCYmxqU/JiZG+fn5VW5XVFSkiIgIhYaGavjw4Xrttdc0ZEjFawT+JzMzU1FRUeYrLi7OnTLhR36d3FZv3dnHXJ70ySbFZyzUL/76tYVVAQBqk0fupmnYsKHWrl2r//znP3r22WeVnp6uFStWVDl+/PjxKioqMl/79jEZViAb0KmZ/jC0s0vfd3uOaVvBCYsqAgDUpmB3BkdHRysoKEgFBQUu/QUFBYqNja1yO7vdro4dO0qSEhMTlZeXp8zMTA0cOLDS8WFhYQoLC3OnNPi5+wd2VERYsCZ8vNHsG/LKF9qVOUw2m83CygAAl8qtIyOhoaFKSkpSVtb554g4nU5lZWUpJSWl2u/jdDpVWlrqzo8GNColXrunDNdv+sabfe3GL1J8xkJl5RVUvSEAwKu5fZomPT1dM2fO1FtvvaW8vDyNGTNGJSUlGj16tCRp1KhRGj9+vDk+MzNTy5Yt086dO5WXl6eXXnpJc+bM0W233VZ7vwUCylM3Xl6h7663vtPML3ZaUA0A4FK5dZpGkkaOHKnDhw9rwoQJys/PV2JiohYvXmxe1Lp3717Z7eczTklJie6//37t379f9erVU5cuXfTOO+9o5MiRtfdbIOAs+/01GvLKFy59zy7KU3x0A13TKVphwUEWVQYAcJfb84xYgXlGcDFzV+9VxofrXfpaNaqnVRnXWlQRAECqo3lGAG/0f33aVOg7cPy0fj9vreeLAQC4jTACv7BxUpqu6dRMtya1NvsWrDmg+IyF2nSQSfMAwJsRRuAXGoQF6+07++iFWxM05eYeLuuGvfqlthackGEYPN8GALwQ14zAL139p+Xaf+x0pevuH9hBfxjaxcMVAUDg4ZoRBLSvHrtWO58bpt8NvqzCur+s2KG/rNiucofTgsoAABcijMBv2e02pQ/ppF/2al1h3fOLt2ja59stqAoAcCFO0yCg/PPbvXp8wbnbgFtGhevr8YMtrggA/BenaYBK/L/kNhr331M3B4vOKO9QsX44yaMJAMBKhBEEnGs6RZvt6//8pZKe+Uxl5Vw/AgBWIYwg4CS1bVKhr9MfP5UPnLEEAL9EGEFA2jx5qBLjGrn03fSXr60pBgACHGEEASk8JEgfje2nFY8MNPvW7juutAsevgcAqHuEEQS0+OgGemVkgrm8peCE/t/Mb5ipFQA8iDCCgHdTz9b6x93J5vLXO35QlycXa8OBIgurAoDAQRgBJPXrGK2nbujm0vez177iCAkAeABhBPiv3/Rrp+UPD3Dpe3ZhnkXVAEDgIIwAP9K+WYR2PjfMXJ7zzR7t+aHEwooAwP8RRoAL2O023XtNe3N5wAsrdKjotJxO5iEBgLpAGAEqMX5YV5fllMzlav/4Iu07esqiigDAfxFGgCpsf/b6Cn3/+HavBZUAgH8jjABVCA6ya1fmMJe+L7cdtqgaAPBfhBHgImw2m3ZPGa7ru8dKkjYeLLa4IgDwP4QRoBoGd40x2+nvreViVgCoRYQRoBpSuzY32x/mHlD7xxeppLTcwooAwH8QRoBqaFQ/VOMGX+bS91b2bmuKAQA/QxgBqun3Qzpp7KAO5vLzi7fIMDhdAwCXijACuOHRtC4a2LmZuZyVV2hhNQDgHwgjgJv+8usrzfZLy7ZaWAkA+AfCCOCm+qHBGn5FC0lS3qFixWcsVHzGQuXsOaqjJWUWVwcAvifY6gIAXzRmQAct/P6QS98tf82WJPXr2FQje7fRjQktrSgNAHwOR0aAGri8ZaRaNapX6bpV23/Q795do/iMhVqyMd/DlQGA77EZPnA7QHFxsaKiolRUVKTIyEirywFMp8scKikrV/aOH/Tgu2sqHbPlmaEKCw7ycGUAYL3q/v3myAhwCeqFBik6Ikw3JLRUzh9T1TIqvMKYzn9crILiMxZUBwC+gSMjQB2Jz1josjx1ZKJG9GxlUTUA4HkcGQEstnFSmsvyQ/PW6sxZh0XVAID3IowAdaRBWLC+fXywoiPCzL4uTy7WkJdXWlgVAHgfwghQh2Iiw5X18ACXvm2FJzV61mqLKgIA70MYAepYVL0QrXx0oG740bwjn285rPiMhXI4vf6SLQCoc4QRwAPaNm2g137VU33aNXHp7/D4IosqAgDvQRgBPOi9e1N00wV31Mz5Zo9F1QCAdyCMAB72yshEbXv2enP5yY82qOjUWQsrAgBrEUYAC4QE2TV2UAdzOeHppSord1pYEQBYhzACWOTRtC6KiTx/2++HufstrAYArEMYASy0/OGBZjvjw/WKz1io2at2WVcQAFiAMAJYqEFYsJ78WTeXvqc+2aT4jIU89RdAwCCMABa76+p2Va67d06OBysBAGsQRgAvsO3Z6zXvt1dp3ODLKqyLz1ior3ccUWm5Qz7wXEsAcBtP7QW81IVP/ZWkpg1ClfPkEAuqAQD38dRewMd9PLZfhb4fSsoUn7FQC78/ZEFFAFA3CCOAl0qIa6RPx/WvdN3Yf+bqjS92eLgiAKgbhBHAi3VtEandU4Zr53PDdO817V3WPbdos1ZuPWxRZQBQe2oURqZPn674+HiFh4crOTlZq1dX/Tj0mTNnqn///mrcuLEaN26s1NTUi44HUJHdbtP4YV21+onB6tg8wux/7z/7LKwKAGqH22Fk3rx5Sk9P18SJE5Wbm6uEhASlpaWpsLCw0vErVqzQr371K33++efKzs5WXFycrrvuOh04cOCSiwcCTfOG4fosfYBaN64nSVq4/pD6P79c//dGtsbNXaOP1/LfFQDf4/bdNMnJyerdu7emTZsmSXI6nYqLi9ODDz6ojIyMn9ze4XCocePGmjZtmkaNGlWtn8ndNICrvEPFuv7PX1a5fsZtSRraPdaDFQFARdX9+x3szpuWlZUpJydH48ePN/vsdrtSU1OVnZ1drfc4deqUzp49qyZNmlQ5prS0VKWlpeZycXGxO2UCfq9ri0h9/9R1WrwhX7l7jik4yKZ3vtlrrr/vnXOTpe3KHCabzWZVmQBQLW6dpjly5IgcDodiYmJc+mNiYpSfX71pqx977DG1bNlSqampVY7JzMxUVFSU+YqLi3OnTCAgRIaH6Je94jTlliv0zIgeynt6aIXZXNuNX6SNB4ssqhAAqsejd9NMmTJFc+fO1YIFCxQeHl7luPHjx6uoqMh87dvHRXrAT6kXGqQnf9ZNuzKHufQPf/UrOZxeP7chgADmVhiJjo5WUFCQCgoKXPoLCgoUG3vx89MvvviipkyZoqVLl+qKK6646NiwsDBFRka6vABUj81m067MYWrVqJ7Z1+HxRZr3n71yEkoAeCG3wkhoaKiSkpKUlZVl9jmdTmVlZSklJaXK7Z5//nlNnjxZixcvVq9evWpeLYBqsdlsWpVxrRrXDzH7Hvtgvdo/vkgfrz1AKAHgVdw+TZOenq6ZM2fqrbfeUl5ensaMGaOSkhKNHj1akjRq1CiXC1z/9Kc/6cknn9Sbb76p+Ph45efnKz8/XydPnqy93wJApVY/karfXtNeDcPOX6s+bu5atX98kfYdPWVhZQBwXo0elDdt2jS98MILys/PV2Jiol599VUlJydLkgYOHKj4+HjNnj1bkhQfH689e/ZUeI+JEyfqqaeeqtbP49Ze4NI9Mn+d3s/Z79LXOaahPn6gn8JDgiyqCoA/q+7fb57aCwSYP7y/Tu995xpKFtzfVz3bNLaoIgD+ijACoEpnzjo0dOoX2v2D66ma3VOGW1QRAH9U3b/fPCgPCEDhIUFa8eggPX+L651tF57GAQBPIIwAAeyXveO07dnrzeVH5q/Tun3HrSsIQEAijAABLiTIrg/v72su/3z6KvXNzNLC7w9ZWBWAQEIYAaAr2zRW+pBO5vLBojMa+89c9ZuyXHt/4BZgAHWLC1gBmE6cOat/rTuoJxZsqLDuj8O76u7+7S2oCoCv4gJWAG5rGB6iXye31dcZ16pt0/ou655ZmKe/f7XLosoA+DOOjACoksNp6FdvfKPVu4+afcF2m/79u6vVJZb/FgFcHPOMAKg1WwtO6LpXvqjQ/9QN3dS7XRM1rh+qiPBgRYaHVLI1gEBFGAFQq7YXntBvZv1H+4+drnJMsN2mySO669ouzRUTGe7B6gB4I8IIgDpxusyhQS+uUFS9EJ11OLXzSEmVY++6up1Su8YopUNTD1YIwFsQRgB4xFmHU2XlTv12zndatf2HKsftyhwmm83mwcoAWI0wAsASx0+VKfHpZRcd8/iwLuoU01DdWkSqOadzAL9FGAHgFeIzFlZr3GfpA9SxeUQdVwPAkwgjALzGqu1HtCX/hD75/qAOHDutdtEN9O2uoxXGdWjWQG+M6qXTZQ6VljvVpEGo2kU3sKBiALWBMALAqzmchpZtKtAHufu1bFPBRccmtI7Sgvv7yW7nmhPAlxBGAPiM5ZsLdOfs78zlhuHBOnGmvMK4fh2bquj0WU26sbsSWkcpOIhJpAFvRhgB4HMcTkNFp8+qcf0Q2Ww27TpSokEvrqhy/JVtGumDMX25SwfwUoQRAH7j0/WHdPz0Wf39q13aXniy0jFDL4/VdZfH6LrLYxURFuzhCgFUhjACwG8VnzmrK55aWuX6hNZR+uPPuql3fBMPVgXgQoQRAH5v9a6j+mjtAX2+uVCHis5UWH/vNe312NAuXPgKWIQwAiDg7DpSoteWb9OHuQdc+q/uGK3WjetpZO849WzT2KLqgMBDGAEQsD5ac0APzVtb5fotzwxVWHCQ5woCAhRhBEDAy917TK+v3KHvdh/TDyVlLuvWTbhOUfVDLKoMCAyEEQD4kR9OlmrgCyt0ovT8/CUrHx2otk2Z4RWoK9X9+82MQQACQtOIMK2flKa+HZqafQNeWKFfvp6tnYcrv10YgGdwZARAwJn0yUbNWrW7Qv/DQzppW+FJ3dO/vXq0jvJ8YYCf4TQNAFzEV9uO6KlPNlY5idrjw7rot9d08HBVgH8hjABANZwuc6jrhMXq2aaRnIa0bt9xc91zN/VQ+2YN1Du+iYKYqwRwG2EEAGpg0fpDuv8fuRX6p/+/K9U7vrGiI8KYRA2oJsIIANTQnz/bpr99udPlzpsLpXZtrr/d0duDVQG+hzACALUg89M8vb5yZ6XrusQ21Adj+qoBD+YDKkUYAYBa8r+vyaMlZfpy25EKs7vecmVrTfhZNyZRAy7APCMAUEtsNptsNpuaRoRpRM9W+vbxwYqOCDPXf5C7XwlPL9VX245YWCXguzgyAgA1VFh8RmP+kaucPcdc+ls1qqclv79GEZy+QYDjyAgA1LHmkeH6YExf3TugvUv/geOn1X3iEnV9crG+2fmDRdUBvoMjIwBQC4pOndXD89fqs7zCStdnXN9FIxJbKTYq3MOVAdbhAlYAsMj+Y6f06PzvlV3JUZE5d/VR/8uaWVAV4HmEEQDwAvfNydHGQ0Xad/S02Xd991jdN6CD3l29V51iGqp3fBN1bxUpm43J1OBfCCMA4EX+te6gfvfumouOiYkM05y7ktUppqGHqgLqFhewAoAXuTGhpSbe0O2iYwqKS3XdK18oYdJSLd5wyEOVAdbjyAgAeFi5wym7zWY+4+bZhZu0teCkVm497DLu47H9lBDXyIIKgdrBaRoA8DGFJ85o4feHNOmTTWZf4/ohWv1EqkKCOJAN38NpGgDwMc0bhmt0v3Z6NK2z2Xfs1Fld9sSnzO4Kv0YYAQAvc//ADlpwf1+Xvtv+/q3iMxYq71CxRVUBdYfTNADgpQzD0PNLtujvX+1SWbnT7L/76nZqHhkmu82mkCC7ru8eq+aRTKYG78M1IwDgR0bPWq3Ptxyucv2LtyboF0mtPVgR8NMIIwDgZ86cdei2v32r7y54MN+P9WnXRLNH91a9kCAmUYPlCCMAEADW7D2mm/7ydZXr37g9SdddHuvBioDzCCMAECDKHU7tO3Zag15cUeWYl3+ZoJuv5DQOPKu6f7+DPVgTAKAOBAfZ1S66gXY+N0zbCk/qrMOph99bpy0FJ8wx6e+tU2R4iFK7xVhYKVC5Gt3aO336dMXHxys8PFzJyclavXp1lWM3btyoW265RfHx8bLZbJo6dWpNawUAXITdblPn2Ibq3ipKS35/jXY+N0yj+8Wb6+9++zs9/N46HTlZal2RQCXcDiPz5s1Tenq6Jk6cqNzcXCUkJCgtLU2FhYWVjj916pTat2+vKVOmKDaW85YA4Cl2u00Tb7hcf/6/RLPvg9z96vXMZ4rPWKjfvbtGSzbma/+xU9YVCagG14wkJyerd+/emjZtmiTJ6XQqLi5ODz74oDIyMi66bXx8vB566CE99NBDbhXJNSMAcGlWbT+i+/+Rq6LTZy86buWjA9W2aQMPVQV/VyfTwZeVlSknJ0epqann38BuV2pqqrKzs2te7QVKS0tVXFzs8gIA1Fy/jtFaN/E6bXlmqFLaN5Uk9WzTqMK4AS+s0LMLN+nd1Xu1dGO+HE6vv8cBfsCtC1iPHDkih8OhmBjXC6BiYmK0efPmWisqMzNTkyZNqrX3AwCcExYcpHd/e5W57HQa2nfslCb+a6NW/HdStZlf7qqw3f0DO+gPQ7t4rE4EFq98Ns348eNVVFRkvvbt22d1SQDgl+x2m9o2baDZo/vosf+GjeiIsArj/rJih+IzFmrk69nKLzqjwuIz8oGZIeAj3DoyEh0draCgIBUUFLj0FxQU1OrFqWFhYQoLq/gfAwCg7owZ2EFjBnYwl/f+cEqZn+bp0w35Zt+3u47qqswsl+1S2jfVczf3UFzjegoO8sr/x4WXcyuMhIaGKikpSVlZWRoxYoSkcxewZmVl6YEHHqiL+gAAFmnTtL7+eluSyh1Ofbn9iEbP+k+l47J3/lBhwrUv/zBIcU3qSzp3KsiQFGRnenpUzu1Jz9LT03XHHXeoV69e6tOnj6ZOnaqSkhKNHj1akjRq1Ci1atVKmZmZks5d9Lpp0yazfeDAAa1du1YRERHq2LFjLf4qAIC6EBxk16DOzbV7ynAZhqHCE6U6WlKmpz/ZpA0HinSitLzCNv2f/7xCX3K7Jvp211FJUuP6IRo/rKuu7dJcxafPqlXjegoLDqrz3wXeqUbTwU+bNk0vvPCC8vPzlZiYqFdffVXJycmSpIEDByo+Pl6zZ8+WJO3evVvt2rWr8B4DBgzQihUrqvXzuLUXALzbsZIybT98Ur+e+a3KHM4av8+Sh65R59iGtVgZrMSzaQAAlih3OPXi0q2y26TwkCCVOw2t23dcjeuH6KvtRxQaZNfBojNuv2/mzT3UKSZCV7RupBNnytW4fghPJvZyhBEAgFcrPnNWpWedMgxDf125Q7NW7Xb7PV68NUG/SOIBgN6KMAIA8CkFxWf0+sqd2nH4pApPlOqaTtH6atsRbTz40xNfJrVtrOdu6qEOzRrIaUihwdzV4w0IIwAAv3KytFybDxUrNNiuXUdKNG7u2mpt91DqZRo3+DJO6ViAMAIA8HufrDuoxxes14kzFe/oqUz76AaacEM3Xdm2sSLDQ+q4OhBGAAAB4+Dx03rji51K7Rqjf607oMMnShXXpL7ezt5z0e36xDfR7wZfpl7xjRUewq3FtY0wAgAIeE6noazNhdp0sFjf7PxBBcVntPNISZXj104Yokb1Qz1YoX8jjAAAUImP1x7QR2sO6PP/PhjwQtd3j9UjaZ01/7v9OlZSpn+tO6iM67volqTWighze67QgEYYAQDgJ5wuc+j0WYeunLysRttnj79WLaLq1XJV/oMwAgCAGz5Zd1APvrumxttPubmHbrqyFdPa/whhBACAGnA4DdltcrkVuNzh1Dc7j2rWql3qGBOh11fuvOh7fP7IQLWLblDXpXo9wggAAHXIMAx9uiFf9/8j96LjAvmiWMIIAAAeYhiG1uw7rpv/8nWVY27u2UrP3NRd9UMD5yJYwggAABYoK3fqve/26d3Veyudyv7WpNb60y1XyG73/xlhCSMAAFhs+eYCfbo+X/Nz9ldY171VpDYdLFanmIaaPbqPYqPCLaiwbhFGAADwIkOnfqHN+Sd+ctyC+/uqZ5vGFx1z5qxDZx1ONfTyKe0JIwAAeKF9R0/pqX9t1J6jp7S98GSV4778wyDFNamv/KIzatYwTEF2mzYeLNLwV79yGffG7Um67vLYui67RggjAAD4gH1HT+n389bq6Kky7Txc9VT1M0f10j1vf1fpusybe6iktFzPLMzToM7NNGt0n7oq1y2EEQAAfMyOwyc1+KWVtfJeO58bZvlFsoQRAAB8lGEYajd+UaXrrusWozdG9VLR6bNKmLT0ou/TMCxYnz08QDGR1lwcSxgBAMAPlJSWa/rn22WzSQ9ee5nCQ1ynm9/zQ4km/ztPa/cdU2R4SKVPJd49ZbinynVBGAEAIACVlTvV6Y+fVrn+o7H9lBjXyCO1VPfvt90j1QAAAI8IDbZr95Th2jx5aKXrR0xfVeWFsFYJnDlpAQAIIOEhQfp1chv949u9FdYt21Sg+IyF5vLQy2M14/YkT5bngtM0AAD4uXX7jmv3DyVavetopeFEkt6/L0W94pvU6s/lNA0AAJAkJcQ10s8TW+nZm3po8+ShCg+p+OffZuFdwJymAQAggISHBGnz5OslSUWnzupISak2HChSu+gIy2oijAAAEKCi6ocoqn6IOjSzLohInKYBAAAWI4wAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCmfeGqvYRiSpOLiYosrAQAA1fW/v9v/+zteFZ8IIydOnJAkxcXFWVwJAABw14kTJxQVFVXlepvxU3HFCzidTh08eFANGzaUzWartfctLi5WXFyc9u3bp8jIyFp7X3/EvnIP+6v62FfVx76qPvZV9dXlvjIMQydOnFDLli1lt1d9ZYhPHBmx2+1q3bp1nb1/ZGQkH9ZqYl+5h/1Vfeyr6mNfVR/7qvrqal9d7IjI/3ABKwAAsBRhBAAAWCqgw0hYWJgmTpyosLAwq0vxeuwr97C/qo99VX3sq+pjX1WfN+wrn7iAFQAA+K+APjICAACsRxgBAACWIowAAABLEUYAAICl/D6MTJ8+XfHx8QoPD1dycrJWr1590fHz589Xly5dFB4erh49emjRokUeqtR67uyr2bNny2azubzCw8M9WK11vvjiC91www1q2bKlbDabPvroo5/cZsWKFbryyisVFhamjh07avbs2XVepzdwd1+tWLGiwufKZrMpPz/fMwVbKDMzU71791bDhg3VvHlzjRgxQlu2bPnJ7QLxO6sm+ypQv7P++te/6oorrjAnNEtJSdGnn3560W2s+Ez5dRiZN2+e0tPTNXHiROXm5iohIUFpaWkqLCysdPzXX3+tX/3qV7rrrru0Zs0ajRgxQiNGjNCGDRs8XLnnubuvpHOz9R06dMh87dmzx4MVW6ekpEQJCQmaPn16tcbv2rVLw4cP16BBg7R27Vo99NBDuvvuu7VkyZI6rtR67u6r/9myZYvLZ6t58+Z1VKH3WLlypcaOHatvvvlGy5Yt09mzZ3XdddeppKSkym0C9TurJvtKCszvrNatW2vKlCnKycnRd999p2uvvVY///nPtXHjxkrHW/aZMvxYnz59jLFjx5rLDofDaNmypZGZmVnp+F/+8pfG8OHDXfqSk5ONe++9t07r9Abu7qtZs2YZUVFRHqrOe0kyFixYcNExf/jDH4zLL7/cpW/kyJFGWlpaHVbmfaqzrz7//HNDknHs2DGP1OTNCgsLDUnGypUrqxwTyN9ZP1adfcV31nmNGzc2/va3v1W6zqrPlN8eGSkrK1NOTo5SU1PNPrvdrtTUVGVnZ1e6TXZ2tst4SUpLS6tyvL+oyb6SpJMnT6pt27aKi4u7aNIOdIH6uboUiYmJatGihYYMGaJVq1ZZXY4lioqKJElNmjSpcgyfrXOqs68kvrMcDofmzp2rkpISpaSkVDrGqs+U34aRI0eOyOFwKCYmxqU/JiamyvPP+fn5bo33FzXZV507d9abb76pjz/+WO+8846cTqf69u2r/fv3e6Jkn1LV56q4uFinT5+2qCrv1KJFC82YMUMffPCBPvjgA8XFxWngwIHKzc21ujSPcjqdeuihh9SvXz917969ynGB+p31Y9XdV4H8nbV+/XpFREQoLCxM9913nxYsWKBu3bpVOtaqz5RPPLUX3iclJcUlWfft21ddu3bV66+/rsmTJ1tYGXxZ586d1blzZ3O5b9++2rFjh1555RXNmTPHwso8a+zYsdqwYYO++uorq0vxetXdV4H8ndW5c2etXbtWRUVFev/993XHHXdo5cqVVQYSK/jtkZHo6GgFBQWpoKDApb+goECxsbGVbhMbG+vWeH9Rk311oZCQEPXs2VPbt2+vixJ9WlWfq8jISNWrV8+iqnxHnz59Aupz9cADD+jf//63Pv/8c7Vu3fqiYwP1O+t/3NlXFwqk76zQ0FB17NhRSUlJyszMVEJCgv785z9XOtaqz5TfhpHQ0FAlJSUpKyvL7HM6ncrKyqryXFlKSorLeElatmxZleP9RU321YUcDofWr1+vFi1a1FWZPitQP1e1Ze3atQHxuTIMQw888IAWLFig5cuXq127dj+5TaB+tmqyry4UyN9ZTqdTpaWlla6z7DNVp5fHWmzu3LlGWFiYMXv2bGPTpk3Gb3/7W6NRo0ZGfn6+YRiGcfvttxsZGRnm+FWrVhnBwcHGiy++aOTl5RkTJ040QkJCjPXr11v1K3iMu/tq0qRJxpIlS4wdO3YYOTk5xv/93/8Z4eHhxsaNG636FTzmxIkTxpo1a4w1a9YYkoyXX37ZWLNmjbFnzx7DMAwjIyPDuP32283xO3fuNOrXr288+uijRl5enjF9+nQjKCjIWLx4sVW/gse4u69eeeUV46OPPjK2bdtmrF+/3hg3bpxht9uNzz77zKpfwWPGjBljREVFGStWrDAOHTpkvk6dOmWO4TvrnJrsq0D9zsrIyDBWrlxp7Nq1y/j++++NjIwMw2azGUuXLjUMw3s+U34dRgzDMF577TWjTZs2RmhoqNGnTx/jm2++MdcNGDDAuOOOO1zGv/fee0anTp2M0NBQ4/LLLzcWLlzo4Yqt486+euihh8yxMTExxrBhw4zc3FwLqva8/91+euHrf/vnjjvuMAYMGFBhm8TERCM0NNRo3769MWvWLI/XbQV399Wf/vQno0OHDkZ4eLjRpEkTY+DAgcby5cutKd7DKttPklw+K3xnnVOTfRWo31l33nmn0bZtWyM0NNRo1qyZMXjwYDOIGIb3fKZshmEYdXvsBQAAoGp+e80IAADwDYQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAAhQX3zxhW644Qa1bNlSNptNH330kdvvYRiGXnzxRXXq1ElhYWFq1aqVnn32Wbfeg6f2AgAQoEpKSpSQkKA777xTN998c43eY9y4cVq6dKlefPFF9ejRQ0ePHtXRo0fdeg9mYAUAALLZbFqwYIFGjBhh9pWWluqJJ57Qu+++q+PHj6t79+7605/+pIEDB0qS8vLydMUVV2jDhg3q3LlzjX82p2kAAEClHnjgAWVnZ2vu3Ln6/vvvdeutt2ro0KHatm2bJOmTTz5R+/bt9e9//1vt2rVTfHy87r77brePjBBGAABABXv37tWsWbM0f/589e/fXx06dNAjjzyiq6++WrNmzZIk7dy5U3v27NH8+fP19ttva/bs2crJydEvfvELt34W14wAAIAK1q9fL4fDoU6dOrn0l5aWqmnTppIkp9Op0tJSvf322+a4v//970pKStKWLVuqfeqGMAIAACo4efKkgoKClJOTo6CgIJd1ERERkqQWLVooODjYJbB07dpV0rkjK4QRAABQYz179pTD4VBhYaH69+9f6Zh+/fqpvLxcO3bsUIcOHSRJW7dulSS1bdu22j+Lu2kAAAhQJ0+e1Pbt2yWdCx8vv/yyBg0apCZNmqhNmza67bbbtGrVKr300kvq2bOnDh8+rKysLF1xxRUaPny4nE6nevfurYiICE2dOlVOp1Njx45VZGSkli5dWu06CCMAAASoFStWaNCgQRX677jjDs2ePVtnz57VM888o7ffflsHDhxQdHS0rrrqKk2aNEk9evSQJB08eFAPPvigli5dqgYNGuj666/XSy+9pCZNmlS7DsIIAACwFLf2AgAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGCp/w/hgHxWVz7+pAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGvCAYAAAB8Tl4/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0XklEQVR4nO3dd3RUdeL+8WfSKUkglAAmEGoglFAMCChNMJAsK2vXXZdd2+qCK8uqG0AFVAQVLKvZVVeFRX+CDbCEKggo0iEQeksA6c0USkgy9/cHXy+OSSCBZO7cmffrHM6Zz+feSR4uc2YebhuHYRiGAAAAbMDP6gAAAABlRXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2EWB1gIrmdDp18OBBhYaGyuFwWB0HAACUgWEYys3NVYMGDeTnV/p+Fa8rLgcPHlR0dLTVMQAAwBXYv3+/oqKiSl3udcUlNDRU0oW/eFhYmMVpAABAWeTk5Cg6Otr8HC+N1xWXnw8PhYWFUVwAALCZy53mwcm5AADANiguAADANiguAADANiguAADANiguAADANiguAADANiguAADANiguAADANiguAADANiguAADANiguAADANrymuKSmpiouLk4JCQlWRwEAAJXEYRiGYXWIipSTk6Pw8HBlZ2dX6Jcsvv99pmZnHNK9XRvp5vbXVNjPBQAAZf/89rpvh64sKzNPaM3eU1qz95QKiwzd2inK6kgAAPgcrzlUVNmS2tY3H//j0w0a8+VmC9MAAOCbKC5ldHP7azTzr93M8ZQfsnT3OyssTAQAgO+huJRDh4Y1NW9YD3O8fM8Jvfd9poWJAADwLRSXcoqtF6rVo/qa4+e+3qKVe05YmAgAAN9BcbkCdUKDtegfPc3xnRwyAgDALSguV6hJneoa97s25jgmJU3nCoosTAQAgPejuFyF33dp5DJu+fRci5IAAOAbKC5XKXN8ksv48U83WJQEAADvR3G5Sg6HQzueH2COP1v7o2JS0rTraJ6FqQAA8E4UlwoQFOCn9Gf6ucz1fWWJTucXWpQIAADvRHGpIDWqBilzfJK6Na1lzrUePU+z1h+wMBUAAN6F4lKBHA6HPnrwOvWKrWPODfs4Xev2nbIwFQAA3oPiUgmm/Lmznv5NnDm+5d8/WJgGAADvQXGpJPdf31j94iLN8c4juRamAQDAO1BcKtHrd7U3H/d7danWc8gIAICrQnGpRFWDAjTp9nhz/Lt//6AbXlokp9OwMBUAAPZFcalkt3aK0ht3dzDH+0+eVZORsxWTkqaz5/mKAAAAyoPi4gYD4xto89hEVQn0d5lv9cxcFRY5LUoFAID9UFzcpFpwgLY+11+fP9LNZb7ZqDkWJQIAwH4oLm7WqVHNYt9v9P9W7rUoDQAA9kJxscCvv99o1MxNiklJ0/RV+yxMBQCA56O4WCQowE8v3dbOZS5lRoZacOgIAIBSeU1xSU1NVVxcnBISEqyOUmZ3XButrc/21xOJsebc+SKnth/mZnUAAJTEYRiGV91UJCcnR+Hh4crOzlZYWJjVccos91yB2o6Zb44HtKmn1+/qoKAAr+mWAACUqqyf33wqeojQkECXPS9zNh1Wi6c4bAQAwC9RXDzIkN7N9OXQ7i5zMSlp3GkXAID/Q3HxMO2iaihrQrLL3F3/XWFRGgAAPAvFxUP98l4vqzJPKiYlTUXseQEA+DiKi4dyOBxa+I+eLnPtn50vLzuXGgCAcqG4eLCmdaprzwsX97zknitU4xGztXDrEQtTAQBgHYqLh/Pzc2jT2ESXufv/t0YLtlBeAAC+h+JiA9WDA5Q5Pkmv3hlvzj04dY2FiQAAsAbFxSYcDod+1yFKE25pa871nrjYukAAAFiA4mIzd3VuaD7OPH5afV9ZonMFRRYmAgDAfSguNrR6VF/z8a6jeWr59Fyt3XvSwkQAALgHxcWG6oQGF7tU+tb/LFeLp+boUPZZi1IBAFD5KC421bROdWVNSNY9XS4eOjpf6FTX8Yv05qKd3KwOAOCV+HZoL1DkNNR05Oxi81ueTVTVoAALEgEAUD58O7QP8fdzaPcLSWpV3/UfOu6Zefoi/QB32wUAeA32uHgZwzDUeETxvS87xw1QoD89FQDgmdjj4qMcDof2vJCk65vVdplvPmqOCoqcFqUCAKBiUFy8kJ+fQx8+0EVZE5IVGRZszjcfNUen8wstTAYAwNWhuHi5lSP7uoxbj56n/EJuWAcAsCeKiw/IHJ/kMo59aq6Gf5LOSbsAANuhuPgAh8OhzPFJqh8eYs7NWHdAQ6ettzAVAADlR3HxEQ6HQ8tH3Kj/3dfZnEvbeEgxKWk6c57zXgAA9kBx8TE9W9TR7L/d4DIX98w8/eHdlVx1BADweBQXHxTXIEw7nh/gMvf9ruNqPmqOMn7MtigVAACXR3HxUUEBfsqakKwn+8e6zA9883uLEgEAcHkUFx/3117NlDk+SX/s2sici0lJ0wP/W6OccwUWJgMAoDiKC+RwOPTszW1c5r7ZekTtxsy3KBEAACWjuMC054UkhYa4fpv0uLQtFqUBAKA4rykuqampiouLU0JCgtVRbMvPz6GMMYnKmpBszv33u0xuVAcA8BheU1yGDBmiLVu2aPXq1VZH8QrD+jY3Hyf9ixN2AQCewWuKCyrW/dc3Nh9vPZSj7hMWWZgGAIALKC4oUWhIoFaNvNEcH/jprGJS0hSTkqajOecsTAYA8GUUF5SqbliIdo0bUGy+8wsLtffEaQsSAQB8HcUFlxTg76cNz9yk5Hb1XeZ7vrxYi7YdsSgVAMBXUVxwWeFVA5V6T0dte66/YiNDzfn7pqxRkZMrjgAA7kNxQZmFBPpr3t97aGjvZubc5GWZFiYCAPgaigvK7fHEi99v9HzaVguTAAB8DcUFV2Ti7fHm45iUNI2YsdHCNAAAX0FxwRW5rVOUy3jaqv36aOU+i9IAAHwFxQVXbOe4AXr8phbmeOTMDJ0rKLIwEQDA21FccMUC/f00tE9zl8NGLZ+eqxV7TliYCgDgzSguuGq3dYrSNTWqmOO73llhYRoAgDejuKBCLEvpo5viIs3xwx+stTANAMBbUVxQYV65s735eO7mw4pJSdMdby3XrqN5yi/k3BcAwNVzGIbhVbc+zcnJUXh4uLKzsxUWFmZ1HJ+z7XCO+r/2XanL5/+9h1r84u67AABIZf/8Zo8LKlTLemH67OGu6tsqssTlN726VLMzDrk5FQDAW7DHBZVqz7E8ZZ04rfumrHGZz5qQbFEiAIAnYo8LPEKTOtXVp2WksiYk6+Xb2pnzx/PyLUwFALArigvcZlCHa8zH1z7/jYVJAAB2RXGB2wT6+6lRrarm+Nb//GBhGgCAHVFc4FbzhvUwH6/de0oxKWl67/tMCxMBAOyE4gK3Cgn016J/9HSZe+7rLRr+Sbo1gQAAtkJxgds1qVNd3wzv4TI3Y90Brco8aVEiAIBdUFxgiWZ1Q5U1IVkL/n6xwNzx9nJNW7VPmcdPy+n0qqv0AQAVhOICSzWPDNXfbmxujkfMyFDviYv5okYAQIkoLrDc8H4tFBkW7DK3KuukYlLSFJOSpl1Hcy1KBgDwNNw5Fx7jXEGR1u/7SXf/t/jelq+GXq+2UeEWpAIAuAN3zoXthAT6q2vTWlo18kbd8oub1UnSwDe/57wXAADFBZ6nbliIXrmzvbImJOvhnk3N+SYjZ+unM+ctTAYAsBrFBR4tZUBLl3H7ZxfIy45uAgDKgeICj7d6VF+X8dTley1KAgCwGsUFHq9OaLCyJiSb49FfbtbKPScsTAQAsArFBbbxwf2dzcd3vrNCWcdPc9gIAHwMxQW2cUPzOnrx1rbmuNfExWo8Yra+SD9gYSoAgDtxHxfYTt9XlmjX0bwSl93S4RqNv7WtggP83ZwKAHA1uI8LvNY3w3vquyd7l7hsxvoDin1qruZuOuzmVAAAd6C4wJaiI6oqa0KyVo26UQ5H8eUPf7hW6ft/cnsuAEDl8ppDRampqUpNTVVRUZF27NjBoSIflHuuQG3HzC827+/n0K5xA+QoqeEAADxCWQ8VeU1x+RnnuODRaev11YaDLnMNI6pqaSmHlwAA1uMcF/isN+7uoLfv7eQyt+/kGcWkpKnfK0u07XCORckAAFeLPS7wakdzz6nzuIXF5rs0jtDHf+lqQSIAQEnY4wJIqhsaos8f6VZsfmXmScWkpOnFudu0KvOkBckAAFeCPS7wKRt//Em/fXNZsfkXb22rOxMaWpAIACCxxwUoUbuoGhr729bF5v/5eYZiUtK0YMsRC1IBAMqKPS7wWYZh6IMVe/XMF5td5gfGN1CXxhHafDBHowfGKSSQu/ACQGXjcmiKC8roqVkZ+nDFvlKXLx/RR/XDq7gxEQD4HooLxQXldDwvX9c+/02Jyx7q0UQjk1q5OREA+A7OcQHKqXb1YG19tr/e/9O1WvtUX7WsF2oue2fpHguTAQB+RnEBfqFKkL/6tIxUrerBmjush16+rZ25LCYlTY9OW29hOgAAxQW4hEEdrnEZf7XhoGJS0uRlR1gBwDYoLsAlBPr7KfWejsXmm46cbUEaAADFBbiM5Hb1tXlsosv3HzmNC4eOipzseQEAd6K4AGVQLThAia3radaQ7i7zTUfO1ox1P1qUCgB8D8UFKIf20TW0fEQfl7nhn2xQTEqaRYkAwLdQXIByqh9eRRvH3FRs/t3vuGQaACobxQW4AmEhgcqakKzv/9nbnHs+bSvfNA0AlYziAlyFqJpV9fpd7c3xHW8v10tzt+lo7jnrQgGAF6O4AFfp5vau93r59+Ld6jxuoSbO225RIgDwXhQXoAJkTUjWfd0bu8z9b3mWNWEAwItRXIAK8szAOP379xdvVpd7rtDCNADgnSguQAVKaltfaX+73hy/932mhWkAwPtQXIAKFlf/4texP/f1FsWkpHGyLgBUEIoLUMEcDofuv971fJeUzzMsSgMA3oXiAlSCp38TpycSY83xom1HdSw338JEAOAdKC5AJRnSu5m+HHrxu43+/nG6dWEAwEtQXIBK1C6qhvn4+13Htf/kGWWfLbAuEADYnMMwDMPqEBUpJydH4eHhys7OVlhY2OWfAFSynUdy1e/VpcXm772ukUYmtVKh06nQkEALkgGA5yjr53eAGzMBPql5ZGiJ8x+s2KsPVuyVJA3r21x/7dVMQQHsBAWAS+FdEnCDdU/3U72wkFKXv/bNTrV4ag6HkQDgMjhUBFhg5Z4TuvOdFSUuyxyfJIfD4eZEAGCtsn5+U1wAi2WfLVD82Pkucy/8rq3u6dLQokQA4H5l/fzmUBFgsfAqgdo0NtFlbuTMDJ0vdFqUCAA8F8UF8ADVgwP07eO9VD344vnyLZ6ao5iUNAtTAYDnobgAHqJx7WrF9rxIorwAwC9QXAAP892TvdU5JsJlLiYlTcfz+MoAAKC4AB4mOqKqPnm4q575TZzL/LXPf6Occ1wuDcC3UVwAD3Xf9Y018fZ4l7lOzy2wKA0AeAaKC+DBbusUpawJyea4oMjQ9zuPW5gIAKxFcQFs4Kuh15uP//DeSu0+lmdhGgCwDsUFsIG2UeF6pFdTczxiRoaFaQDAOhQXwCaeTIw17/OyKvOkYlLS1OOlb1Xk9KqbXwPAJVFcAJtwOBz6/JFuLnP7Tp5R05GzFZOSpsHvr7IoGQC4D8UFsJHYeqG6rVNUicuW7Dimk6fPuzkRALgXX7II2FDW8dPaeTRP/168S+v3/eSybMSAlvpT9xg55FBQAP83AWAPfDs0xQU+pLSvBdgw+iaFVwl0cxoAKD++HRrwIRljbipxPn7sfH254aC87P8nAHwYxQXwAqEhgVo58kbVrFp878rfpq3XHW8vtyAVAFQ8igvgJSLDQrT+mZu0+4UkfXB/Z5dlq7NOadqqfRYlA4CKQ3EBvIy/n0M3NK+jVaNuVHx0DXN+7FebrQsFABWE4gJ4qbqhIfpiSHdd36y2JOlcgVMjZmzU9FX7tOlANpdOA7AlrioCvNyPp87o+he/LXHZgzc01hOJLblsGoDluKoIgCQpqmZVff5IVyW1raeaVQMVVbOKuey/32WqxVNzNPqLTRYmBICyC7A6QEVJTU1VamqqioqKrI4CeJxOjSLUqVGEOd51NFe/S/1BufmFkqT/Ld+rIsPQ84PaWhURAMqEQ0WAD9t/8oxueMn1MFLWhGSL0gDwZRwqAnBZ0RFVi31x48eruWwagOeiuAA+rlOjmtrzQpI5/ufnGbrhpUVauPWIhakAoGQUFwDy83Pof/ddvGnd/pNndf//1mjEjAxlnymwMBkAuKK4AJAk9WxRR58+3FXJ7eqbc9NW7VP8s/O1+1iehckA4CKKCwBTQkyEUu/pqDfv6eAyf+OkJRoxI8OiVABwEcUFQDG/addAWROS9ZeeTcy5aav2KSYlTWfOF1qYDICvo7gAKNWIAa20ccxNLnNxz8zThyv2WpQIgK+juAC4pLCQQGVNSFav2Drm3FOzNmlV5kkLUwHwVRQXAGUy5c+dNbxfC3N8x9vLLUwDwFdRXACU2d9ubK77r29sjp/9aouFaQD4IooLgHL5Z/+W5uP3l2UqJiVNbyzcaWEiAL6E4gKgXIIC/LTg7z1c5iYt2KG3l+y2KBEAX0JxAVBuzSNDte25/noquZU5N37ONsWkpOmL9AMWJgPg7SguAK5ISKC/HrihiT57uKvL/GPT0xWTkqajOecsSgbAm1FcAFyVa2MilDk+SQ/1aOIy3/mFhRYlAuDNKC4ArprD4dDIpFbKHJ+k65pEmPMxKWn6euNBC5MB8DYUFwAVxuFwaNqD17nMDf1ovbpPWKRth3P4ugAAV81hGIZhdYiKlJOTo/DwcGVnZyssLMzqOIBPyj5ToE7PL1Chs+S3l+uaROj+65uoX1ykm5MB8FRl/fymuACoNGuyTuq2ty59h907r43WMwPjVC04wE2pAHgiigvFBfAYefmFyj1XoMnLsnS+0KkpP2QVW6ddVLhaRIZqxICWqlU92P0hAViK4kJxATzW+UKnxny1WR+t3Ffi8s1jE9kDA/gYigvFBbCF8bO36r/f7dGvT4e549ooNa1TXX1a1lXzyFBrwgFwG4oLxQWwnfbPztdPZwpKXf5EYqz+2LWRQkMC3ZgKgDtQXCgugO0YhqEHp67R2r2ndOoSBaZ7s1r6fw9cV+pyAPZDcaG4ALb281vT1kO5envpbn2R7noju4YRVbX0yd46V1Ckgz+dVfWQANUNDbEiKoAKQHGhuABeZ8+xPPWZtOSS67x+V3vd3P4aNyUCUFHK+vnNnXMB2EaTOtX1zfAel1znsenpSv12l86cL5SzlBvgAbAv9rgAsB3DMLTlUI52Hc1Ts7rV1bJemN79bo/Gz9lWbN0+LetqxICWXJkEeDgOFVFcAJ+z98Rp9Xx5cYnLGoSHaFlKHzkcDveGAlAmFBeKC+CTipyG0vf/pG2HczRq5qZiy4f1ba5HejVVcIC/BekAlIbiQnEBIKmwyKlmo+aUuKxHizr67x87UWIAD8DJuQAgKcDfTzvHDZC/X/FDREt3HFPsU3N173srLUgG4EpQXAB4vUB/P+1+IUlZE5K1aWyi4qNruCz/budxvblopzXhAJQLh4oA+KS8/EK9912mXv1mhzl3a8covXxbO/mVsHcGQOXiHBeKC4AymL5qn1JmZBSb/0e/FvpT9xi+FwlwE85xAYAyuKtzQz12Y/Ni85MW7FC7sfPlZf+3A2yPPS4AoAs3tZudcVhzNh3S1xsPuSzLmpBsUSrAd7DHBQDKweFwKLldfb15T0dteTbRZdng91dZlArAr1FcAOBXqgYFuOxlWbLjmN5ZutvCRAB+RnEBgFLMHXaD+fiF2duUl19oYRoAEsUFAErVsl6Yxt/S1hy3GT1Pd7+zQkdyzlmYCvBtFBcAuIS7EqJdxsv3nFCXFxZqTsYhvfbNDm06kG1RMsA3cVURAJTB1OVZeuaLzSUumzesh2Lrhbo5EeBduKoIACrQH7vGKHN8kjmuXT3IfJz42lIrIgE+KcDqAABgFw6Hw+Vqoz9NXqXF249Jkr7beUw3NK9jVTTAZ3CoCACukNNpqMnI2S5zVQL9tfW5/hYlAuyLQ0UAUMn8/Bx6KrmVy9zZgiJ1n7BIy3efUJHTq/5fCHgE9rgAwFXadjhHX204qNRvS75J3a0dozTpjng3pwLshT0uAOAmLeuF6YnElvr60etLXP75uh+VX1jk5lSAd2KPCwBUMKfT0I+nzmrq8iy9+32mOb9pbKKqB3NNBFAS9rgAgEX8/BxqWKuqnvpNnLo2qWXOtxk9T5+u2W9hMsD+KC4AUImmPXSdrmsSYY6f+Gyj3li408JEgL1RXACgkk1/qKv+8/uO5njSgh36Iv2AhYkA+6K4AIAbDGhbXwv+3sMcPzY9XSv2nLAwEWBPFBcAcJPmkaH6c/cYc3zXOysUk5KmLQdzrAsF2AzFBQDcaPTA1i7lRZKS/vWd/jZtvbzsIk+gUnA5NABYIC+/UL0nLtax3HyX+Xf/eK36xkValAqwDpdDA4AHqx4coNWj+mp4vxYu8w9MXaOVe06w9wUoBXtcAMBihUVO3fjKEu09ccZl/oeUPmpQo4pFqQD3Yo8LANhEgL+fljzRW/HRNVzmH5u+3ppAgAejuACAh/hiSHdljLlJdUKDJUmrs05xyAj4FQ4VAYCH2XMsT30mLTHHSW3raXbGYUlSg/AQTbwjXt2a1rYqHlApOFQEADbVpE51l/HPpUWSDmaf0z3/XamvNx5kbwx8EsUFADzQl0O7X3L50I/Wq/GI2ZQX+BwOFQGABzMMQ0VOQwH+F/6fGZOSVmyd8be01d2dG7o7GlChOFQEAF7A4XCYpUWSsiYkK3N8kss6I2ZkaM+xPHdHAyxBcQEAm3E4HNr2XH89d3Nrc+7vn2ywMBHgPhQXALChkEB/3ds1xhxv2P+T1u49aV0gwE0oLgBgY88NamM+vvU/y7Uqk/IC70ZxAQAbu61jlBJiaprjO95ebmEaoPJRXADAxqoE+evTh7upXVS4OReTkqbD2ecsTAVUHooLAHiBzx7u5jK+bvxC7vECr0RxAQAvEBTgpw3P3OQyF/vUXMoLvA7FBQC8RHjVQGVNSDbH54ucGpS6zMJEQMXjzrkA4GXW7TulW/79Q4nLoiOqaPHjveXv53BzKuDSuHMuAPiojg1rKv2ZfiUu23/yrEbNzHBzIqDiUFwAwAvVqBqkpLb1JEl1Q4PVpXGEuWz66v2au+mQzhc6rYoHXDEOFQGAj1i795Ru/Y/rIaRfnhMDWIlDRQAAF50a1Sw2N3lZphZtO2JBGuDKsMcFAHxMXn6h2oyeV2z+28d7qXHtahYkAtjjAgAoRfXgANUPDyk233viYv105rwFiYCyY48LAPggwzC0du8pVQsO0IDXv3NZ9tygNrr3ukYWJYOvYo8LAKBUDodD18ZEqFX9MGVNSFbzutXNZU/P2sQVR/BYFBcAgBYM7+kyfn3hDouSAJdGcQEASHK9NDr1290WJgFKR3EBAJj+3reF+XjD/p+sCwKUguICADAN6d3UfHxz6jKNmLGRb5iGR/HI4vK73/1ONWvW1G233WZ1FADwKQH+fhoY38AcT1u1X41HzNaeY3kWpgIu8sji8thjj2nq1KlWxwAAn/TG3R006fZ4l7k+k5YoJiVNt7/1A3tgYCmPLC69evVSaGio1TEAwGfd2ilKm8cmFptfnXVKjUfM1guzt1qQCriC4rJ06VINHDhQDRo0kMPh0KxZs4qtk5qaqpiYGIWEhKhLly5atWpVRWQFALhRteAAZU1I1ku3tlNQgOvHxTtL9+j+KastSgZfFlDeJ5w+fVrx8fG67777dMsttxRb/vHHH2v48OF666231KVLF7322mtKTEzU9u3bVbduXUlS+/btVVhYWOy58+fPV4MGDYrNX0p+fr7y8/PNcU5OTjn/RgCAS7kjIVp3JEQrv7BId769Qun/d7XRwm1HdeZ8oaoGlfujBLhiV3XLf4fDoZkzZ2rQoEHmXJcuXZSQkKA333xTkuR0OhUdHa1HH31UKSkpZf7Zixcv1ptvvqnPPvvskuuNGTNGY8eOLTbPLf8BoHJsPZRjfk3AnddG6/HEWNWsGqgAf488+wA2Yckt/8+fP6+1a9eqb9++F3+Bn5/69u2r5cuXV+SvMo0YMULZ2dnmn/3791fK7wEAXNCq/sUPlY/X7FfCuG/UbNQcxY+dL6eTE3dRuSq0uBw/flxFRUWKjIx0mY+MjNThw4fL/HP69u2r22+/XbNnz1ZUVNQlS09wcLDCwsJc/gAAKte/7u5QbC77bIGajJzNjetQqTzywOQ333xjdQQAwCX8Nr6BOjWqqSXbj+nUmfN6ed52c9nNqcv05dDuahdVw7qA8FoVuseldu3a8vf315EjR1zmjxw5onr16lXkrwIAWOyaGlV0T5eGGtK7mbImJKtTo5rmst++uUxjvtxsYTp4qwotLkFBQerUqZMWLlxozjmdTi1cuFBdu3atyF8FAPAwnz/SzWU85Ycs/f7dFdp1NFcFRU6LUsHblPtQUV5ennbt2mWOMzMzlZ6eroiICDVs2FDDhw/X4MGDde2116pz58567bXXdPr0af35z3+u0OAAAM+TOT5JH63ap1EzN0mSlu06ob6vLJUk1agaqO//2UfVgz3yLAXYRLkvh168eLF69+5dbH7w4MGaMmWKJOnNN9/Uyy+/rMOHD6t9+/b617/+pS5dulRI4Msp6+VUAIDKs3j7Uf1pcsk3qPtyaHfF1gtVcIC/m1PBk5X18/uq7uPiiSguAOAZCouc+mbrEX28er++3X6sxHWeSIzVkN7N3JwMnojiQnEBAI9y73sr9d3O48Xm1z3dTxHVgixIBE9iyQ3oAAAozQf3d9HKkTeqzTWuH0odn1ug7YdzJUm7juZq4dYjKuRkXpSCPS4AAEt0eHa+Tp0pKHHZLR2uUY2qQXp/WaYkafvz/TknxstxqIjiAgAer83oecrLL/6luyUZ+9vWyj5boMTW9RRbL7SSk8HdKC4UFwDweIZh6KEP1mrBlgs3Lq0eHFCmItOoVlUteaL4Fa6wL4oLxQUAbOmnM+cVEuivrBOnZRgyv4n61/7ULUZjftvazelQWXyuuKSmpio1NVVFRUXasWMHxQUAvMi6faf0ZfpBNa1bXU/P2mTOb322vwqcTp06fV5VAv1VNyzEwpS4Gj5XXH7GHhcA8G5LdxzTH99fdcl1siYkuykNKkpZP7+57zIAwFZ6tKhz2XViUtIkXbhL78GfzqpHizqqGsRHnjdgjwsAwHbOFRRpyg9ZmjBnmzlXJdBfZwuKSn3OprGJfE+SB+NQEcUFAHxO5vHT6j1xcenLxyfJ4XC4LxDKjDvnAgB8TuPa1bRr3AB9/kg39Y4tfkip8YjZKnJ61f/XfQ57XAAAXq3IaajpyNkuc5y863nY4wIAgCR/P4eWj+jjMheTkian05CTvS+2wx4XAIBPWLbruH7/7spLrsMJvNZhjwsAAL/QvVltzRrS/ZLrtBk9T5sOZLspEa4ExQUA4DPaR9dQ31aRl1xn2qp9OpJzzk2JUF4cKgIA+JyjuedUq1qwjuae05aDOWobFa4eL32rcwVOl/W4fNp9OFQEAEAp6oaGyN/PofrhVXRjq0jVDQ3Rk4kti63XeMRsTuD1MBQXAAAk/bl7jLo3q1VsvsnI2co5V2BBIpSEQ0UAAPzK0Zxz6vzCQpe5B29orFHJcRYl8n4cKgIA4ArVDQvRnheSXOb++12mYlLS9Mnq/RalguRFxSU1NVVxcXFKSEiwOgoAwAv4+Tn0r7s7FJt/8vONuvnN7y1IBIlDRQAAXNK5giL1nrhYh7JdL5H+S48mGpHUyqJU3odDRQAAVICQQH/9kNJH//3jterZ4uIXN769dI+FqXwXxQUAgMtwOBzqFxep/93XWQkxNc35k6fPW5jKN1FcAAAoh5duizcfd3xugc4XOrV+3yk9OHWN+r+2VHuO5VmYzvvxTVIAAJRDTK2qLuMWT81xGfeZtESSNOOv3dSxYU2hYnFyLgAA5ZR7rkBtx8wv8/rN61bXguE9KzGR/XFyLgAAlSQ0JFBPJMa6zI0ZGKdralQpcf2dR/P05Gcb3BHN67HHBQCAClRQ5NTIGRn6dO2PxZal3tNRye3qW5DK85X185viAgBAJcg+U6AvNhxQfoFT42ZvNefH/a6Nft+lkYXJPBOHigAAsFB41UD9sWuMHuzRxGV+1MxNmrZqn0Wp7I/iAgBAJdvzQpL6t65njkfMyFBMSpqFieyL4gIAQCXz83PorXs7Kaqm68m7TUZQXsqL4gIAgJt8/88++sN1Dc2x05C+SD9gYSL7obgAAOBGzw9q6/Kt049NT9es9ZSXsqK4AADgZr+Nb6C6ocHmeNjH6fpoJSfslgXFBQAAC8wb1sNlPHJmht5YuNOiNPZBcQEAwAI1qwVpzwtJLifsTlqww8JE9uA1xSU1NVVxcXFKSEiwOgoAAGXi5+fQ7MduUIeGNcy5mJQ07TtxxrpQHo475wIAYDHDMNR4xGyXuV3jBijA32v2L1wWd84FAMAmHA5HsXu8dHxugUVpPBvFBQAAD/D9P/so/Zl+5jjnXKHmZByyMJFnorgAAOAhalQN0uQ/XzxX82xBkYVpPBPFBQAAD9I7tq75ePgnG7iz7q9QXAAA8GCPTU/XnmN5VsfwGBQXAAA8zEu3tXMZ95m0RIu2HdG8zYeVX+jbh4+4HBoAAA8V98xcnTnvWlSS29ZX6u87WpSo8nA5NAAANrfl2f7F5tIyDsnL9jmUC8UFAAAP9sod8cXmGo+Yrb98sEbHcvMtSGQtDhUBAGATMSlpxeayJiRbkKTicagIAAAvM+Ov3cr9nN3H8rzqkuoAqwMAAICy6diwpj5/pKu2HMzR019sliQN/zhdr9zZvti65wqK1PPlb3Uk58LhpO2Hc/Vk/5bujFsp2OMCAICNdGoUoVs7RZnjGesPqMhZ/KyPjT9mm6VFkv69eLdX3A+G4gIAgM1UDQpQctv65rjpyNnF1tl1tHhJ6TNpic4XOis1W2WjuAAAYEMP9mjiMh7z5WbFpKQpJiVNZ84XaseR3BKft3zPCXfEqzQUFwAAbCg+Klx/6hZjjqf8kGU+fmvxbq3Ze7LE5+07eUaSNGLGRsWkpKmwyF57YCguAADYkMPh0JjfttYfuzYqtuz46fPadCBHkjT2t61dlq3OPKms46c1bdV+SdLjn26o/LAViOICAICN/bqYSNJHK/eZj69rUksL/t7DHH+54aB6TVxsjmtWC6rUfBWN4gIAgI05HA599nDXUpdHR1RR88hQ3RQXWeLyiKoUF0ukpqYqLi5OCQkJVkcBAMCtro2J0KcPd1WT2tVc5pPb1VfVoAu3bPtT95gSnztpwY4Sr0DyVF5TXIYMGaItW7Zo9erVVkcBAMDtEmIi9Fjf5i5zqfdc/BbpLo1rKT4qvMTnfrJmf6Vmq0heU1wAAPB1vVrUNR9/eH8Xl2X+fg59MfT6Ep/3ztI9lZqrInHLfwAAvER41UBljk9SodNQoH/J+yYWP95L/V5dooIi17vtZp8pUHjVQHfEvCrscQEAwIs4HI5SS4skxdSupp3jkjT5z67nhMY/O7+yo1UIigsAAD6od2xdZU1Idpl7dNp6i9KUHcUFAABIkr7acNDqCJdFcQEAwIdNf+g6l/GZ84UWJSkbigsAAD7suia1NKR3U3P865N2PQ3FBQAAHzesb4uLA8/uLRQXAAB8nZ/DYT7OzS+wMMnlUVwAAPBx/n4Xi8tDU9fqaO45C9NcGsUFAAAoOqKKJGnLoRx1HrdQMSlpempWhsWpiqO4AAAA7T95ttjchyv2WZDk0iguAACgVIbhWWfrUlwAAIA+erBLifMnTp93c5JLo7gAAAB1a1pby1L6FJsfOSPDo/a6UFwAAIAk6ZoaVfTWHzqpdvUgc27+liOase6AhalcUVwAAICpf5t6WvNUP0WGBZtzH6zYa2EiVxQXAABQzA8pN5qP0/f/ZF2QX6G4AACAYvz9HGpzTZg5fmPhTn24Yq/GfrXZwlRSgKW/HQAAeKxHejbTkI/WSZImLdhhzvdvXU9dmtSyJBN7XAAAQImS2tYrcX7OpsNuTnIRxQUAAJTI4XBo/dP9is03q1vdgjQXUFwAAECpalYL0obRN7nMrd17yqI0XlRcUlNTFRcXp4SEBKujAADgVcKrBGrEgJbmuHWDsEusXbkchifdDq8C5OTkKDw8XNnZ2QoLs27DAgDgbXLOFei7HccV1yBMjWtXq9ifXcbPb64qAgAAZRIWEqjkdvUtzeA1h4oAAID3o7gAAADboLgAAADboLgAAADboLgAAADboLgAAADboLgAAADboLgAAADboLgAAADboLgAAADboLgAAADboLgAAADboLgAAADb8LpvhzYMQ9KFr8cGAAD28PPn9s+f46XxuuKSm5srSYqOjrY4CQAAKK/c3FyFh4eXutxhXK7a2IzT6dTBgwcVGhoqh8NRYT83JydH0dHR2r9/v8LCwirs53ortlfZsa3Kjm1VdmyrsmNblV1lbivDMJSbm6sGDRrIz6/0M1m8bo+Ln5+foqKiKu3nh4WF8cIuB7ZX2bGtyo5tVXZsq7JjW5VdZW2rS+1p+Rkn5wIAANuguAAAANuguJRRcHCwRo8ereDgYKuj2ALbq+zYVmXHtio7tlXZsa3KzhO2ldednAsAALwXe1wAAIBtUFwAAIBtUFwAAIBtUFwAAIBtUFx+ITU1VTExMQoJCVGXLl20atWqS67/6aefqmXLlgoJCVHbtm01e/ZsNyX1DOXZXlOmTJHD4XD5ExIS4sa01li6dKkGDhyoBg0ayOFwaNasWZd9zuLFi9WxY0cFBwerWbNmmjJlSqXn9ATl3VaLFy8u9ppyOBw6fPiwewJbaPz48UpISFBoaKjq1q2rQYMGafv27Zd9ni++Z13JtvLV96v//Oc/ateunXlzua5du2rOnDmXfI4VrymKy//5+OOPNXz4cI0ePVrr1q1TfHy8EhMTdfTo0RLX/+GHH3T33Xfr/vvv1/r16zVo0CANGjRImzZtcnNya5R3e0kX7rR46NAh88/evXvdmNgap0+fVnx8vFJTU8u0fmZmppKTk9W7d2+lp6dr2LBheuCBBzRv3rxKTmq98m6rn23fvt3ldVW3bt1KSug5lixZoiFDhmjFihVasGCBCgoKdNNNN+n06dOlPsdX37OuZFtJvvl+FRUVpQkTJmjt2rVas2aN+vTpo5tvvlmbN28ucX3LXlMGDMMwjM6dOxtDhgwxx0VFRUaDBg2M8ePHl7j+HXfcYSQnJ7vMdenSxfjLX/5SqTk9RXm31+TJk43w8HA3pfNMkoyZM2decp0nn3zSaN26tcvcnXfeaSQmJlZiMs9Tlm317bffGpKMU6dOuSWTJzt69KghyViyZEmp6/j6e9bPyrKteL+6qGbNmsa7775b4jKrXlPscZF0/vx5rV27Vn379jXn/Pz81LdvXy1fvrzE5yxfvtxlfUlKTEwsdX1vciXbS5Ly8vLUqFEjRUdHX7LF+zJffl1dqfbt26t+/frq16+fli1bZnUcS2RnZ0uSIiIiSl2H19YFZdlWEu9XRUVFmj59uk6fPq2uXbuWuI5VrymKi6Tjx4+rqKhIkZGRLvORkZGlHi8/fPhwudb3JleyvWJjY/X+++/riy++0Icffiin06lu3brpxx9/dEdk2yjtdZWTk6OzZ89alMoz1a9fX2+99ZY+//xzff7554qOjlavXr20bt06q6O5ldPp1LBhw9S9e3e1adOm1PV8+T3rZ2XdVr78fpWRkaHq1asrODhYDz/8sGbOnKm4uLgS17XqNeV13w4Nz9S1a1eX1t6tWze1atVKb7/9tp577jkLk8GuYmNjFRsba467deum3bt369VXX9UHH3xgYTL3GjJkiDZt2qTvv//e6iger6zbypffr2JjY5Wenq7s7Gx99tlnGjx4sJYsWVJqebECe1wk1a5dW/7+/jpy5IjL/JEjR1SvXr0Sn1OvXr1yre9NrmR7/VpgYKA6dOigXbt2VUZE2yrtdRUWFqYqVapYlMo+Onfu7FOvqaFDh+rrr7/Wt99+q6ioqEuu68vvWVL5ttWv+dL7VVBQkJo1a6ZOnTpp/Pjxio+P1+uvv17iula9piguuvAP1alTJy1cuNCcczqdWrhwYanH9rp27eqyviQtWLCg1PW9yZVsr18rKipSRkaG6tevX1kxbcmXX1cVIT093SdeU4ZhaOjQoZo5c6YWLVqkxo0bX/Y5vvraupJt9Wu+/H7ldDqVn59f4jLLXlOVeuqvjUyfPt0IDg42pkyZYmzZssV46KGHjBo1ahiHDx82DMMw7r33XiMlJcVcf9myZUZAQIAxceJEY+vWrcbo0aONwMBAIyMjw6q/gluVd3uNHTvWmDdvnrF7925j7dq1xl133WWEhIQYmzdvtuqv4Ba5ubnG+vXrjfXr1xuSjFdeecVYv369sXfvXsMwDCMlJcW49957zfX37NljVK1a1XjiiSeMrVu3GqmpqYa/v78xd+5cq/4KblPebfXqq68as2bNMnbu3GlkZGQYjz32mOHn52d88803Vv0V3OaRRx4xwsPDjcWLFxuHDh0y/5w5c8Zch/esC65kW/nq+1VKSoqxZMkSIzMz09i4caORkpJiOBwOY/78+YZheM5riuLyC2+88YbRsGFDIygoyOjcubOxYsUKc1nPnj2NwYMHu6z/ySefGC1atDCCgoKM1q1bG2lpaW5ObK3ybK9hw4aZ60ZGRhpJSUnGunXrLEjtXj9fsvvrPz9vm8GDBxs9e/Ys9pz27dsbQUFBRpMmTYzJkye7PbcVyrutXnzxRaNp06ZGSEiIERERYfTq1ctYtGiRNeHdrKTtJMnltcJ71gVXsq189f3qvvvuMxo1amQEBQUZderUMW688UaztBiG57ymHIZhGJW7TwcAAKBicI4LAACwDYoLAACwDYoLAACwDYoLAACwDYoLAACwDYoLAACwDYoLAACwDYoLAAC4rKVLl2rgwIFq0KCBHA6HZs2aVe6fYRiGJk6cqBYtWig4OFjXXHONxo0bV66fwbdDAwCAyzp9+rTi4+N133336ZZbbrmin/HYY49p/vz5mjhxotq2bauTJ0/q5MmT5foZ3DkXAACUi8Ph0MyZMzVo0CBzLj8/X6NGjdK0adP0008/qU2bNnrxxRfVq1cvSdLWrVvVrl07bdq0SbGxsVf8uzlUBAAArtrQoUO1fPlyTZ8+XRs3btTtt9+u/v37a+fOnZKkr776Sk2aNNHXX3+txo0bKyYmRg888EC597hQXAAAwFXZt2+fJk+erE8//VQ33HCDmjZtqscff1zXX3+9Jk+eLEnas2eP9u7dq08//VRTp07VlClTtHbtWt12223l+l2c4wIAAK5KRkaGioqK1KJFC5f5/Px81apVS5LkdDqVn5+vqVOnmuu999576tSpk7Zv317mw0cUFwAAcFXy8vLk7++vtWvXyt/f32VZ9erVJUn169dXQECAS7lp1aqVpAt7bCguAADALTp06KCioiIdPXpUN9xwQ4nrdO/eXYWFhdq9e7eaNm0qSdqxY4ckqVGjRmX+XVxVBAAALisvL0+7du2SdKGovPLKK+rdu7ciIiLUsGFD/eEPf9CyZcs0adIkdejQQceOHdPChQvVrl07JScny+l0KiEhQdWrV9drr70mp9OpIUOGKCwsTPPnzy9zDooLAAC4rMWLF6t3797F5gcPHqwpU6aooKBAzz//vKZOnaoDBw6odu3auu666zR27Fi1bdtWknTw4EE9+uijmj9/vqpVq6YBAwZo0qRJioiIKHMOigsAALANLocGAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC2QXEBAAC28f8BC7bJTGSd02kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=int)\n",
    "beta = (1./env.action_space.n) * np.ones((env.observation_space.n,env.action_space.n))\n",
    "\n",
    "# Model-based value function computation\n",
    "V_pi0, residuals = policy_eval_iter_mat2(env,pi0,1e-4,10000)\n",
    "Q_pi0 = Q_from_V(env,V_pi0)\n",
    "\n",
    "# TD(0)\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "Qinit = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qtd, error = TD_Qeval(env=env, \\\n",
    "                      pi=pi0, \\\n",
    "                      beta=beta, \\\n",
    "                      max_steps=int(3e6), \\\n",
    "                      alpha=alpha, \\\n",
    "                      gamma=gamma, \\\n",
    "                      Qinit=Qinit, \\\n",
    "                      Qtrue=Q_pi0)\n",
    "\n",
    "# Plot the results\n",
    "print(\"Max error:\", np.max(np.abs(Qtd-Q_pi0)))\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7d260-2fb9-430b-9490-96f8a20764f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Modify the function of the previous exercise to evaluate a stochastic policy `pi` provided as an $|S|\\times |A|$ array of action probabilities. The signature of the function remains the same. The function name should be `TD_Qeval_stoch_pi`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff70ca12-4e29-4fe6-b08e-a5567503560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "\n",
    "def TD_Qeval(env, pi, beta=None, max_steps=int(1e6), alpha=0.001, gamma=0.9, Qinit=None, Qtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (beta is None):\n",
    "        beta = (1./env.action_space.n) * np.ones((env.observation_space.n,env.action_space.n))\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q = np.copy(Qinit)\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        a = np.random.choice(env.action_space.n, p=beta[x,:])\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        aa = np.random.choice(env.action_space.n, p=pi[y,:])\n",
    "        Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][aa]-Q[x][a])\n",
    "        if(Qtrue is not None):\n",
    "            error[t] = np.max(np.abs(Q-Qtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return Q, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8453e2-7d5e-440a-949a-9934245bb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_TD_Qeval_stoch_pi.py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def TD_Qeval_stoch_pi(env, pi, beta=None, max_steps=int(1e6), alpha=0.001, gamma=0.9, Qinit=None, Qtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (beta is None):\n",
    "        beta = (1./env.action_space.n) * np.ones((env.observation_space.n,env.action_space.n))\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q = np.copy(Qinit)\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        a = np.random.choice(env.action_space.n, p=beta[x,:])\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        aa = np.random.choice(env.action_space.n, p=pi[y,:])\n",
    "        Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][aa]-Q[x][a])\n",
    "        if(Qtrue is not None):\n",
    "            error[t] = np.max(np.abs(Q-Qtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return Q, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c50cb-f254-4710-ac5a-7f79555baf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "# Model-based value function computation\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=int)\n",
    "V_pi0, residuals = policy_eval_iter_mat2(env,pi0,1e-4,10000)\n",
    "Q_pi0 = Q_from_V(env,V_pi0)\n",
    "\n",
    "# TD(0)\n",
    "pi0 = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "pi0[:,fl.RIGHT] = 1\n",
    "beta = (1./env.action_space.n) * np.ones((env.observation_space.n,env.action_space.n))\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "Qinit = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qtd, error = TD_Qeval_stoch_pi(env=env, \\\n",
    "                               pi=pi0, \\\n",
    "                               beta=beta, \\\n",
    "                               max_steps=int(3e6), \\\n",
    "                               alpha=alpha, \\\n",
    "                               gamma=gamma, \\\n",
    "                               Qinit=Qinit, \\\n",
    "                               Qtrue=Q_pi0)\n",
    "\n",
    "# Plot the results\n",
    "print(\"Max error:\", np.max(np.abs(Qtd-Q_pi0)))\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68231ab3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Before focusing on TD(0), We wrote the derivation for learning $Q^\\pi(s,a)$ based on samples from $G^\\pi(s,a)$ defined on the discrete $S\\times A$ space. Write the same derivation with a parametric state value function $V_\\theta(s)$, then with a parametric state-action value function $Q_\\theta(s,a)$ (the latter being a repetition of what we have seen in class).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070395c4",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We have $V^\\pi(s) = \\mathbb{E} (G^\\pi(s))$. So we consider samples $g^\\pi(s)$ of $G^\\pi(s)$, drawn according to density $p(\\cdot|s,\\pi)$.\n",
    "\n",
    "The risk function becomes $L(\\theta) = \\int_S \\int_\\mathbb{R} \\left[ V_\\theta(s) - g \\right]^2 \\rho(s) p(g|s,\\pi) dsdg$.\n",
    "    \n",
    "The descent direction estimator becomes:\n",
    "$$d = \\frac{1}{N} \\sum_{i=1}^N \\left[ g^\\pi(s_i) - V_\\theta(s_i)\\right] \\nabla_\\theta V_\\theta(s_i).$$\n",
    "\n",
    "And so the update becomes:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\frac{1}{N} \\sum_{i=1}^N \\left[ g^\\pi(s_i) - V_\\theta(s_i)\\right] \\nabla_\\theta V_\\theta(s_i)$$\n",
    "    \n",
    "For a parametric $Q_\\theta$:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\frac{1}{N} \\sum_{i=1}^N \\left[ g^\\pi(s_i,a_i) - Q_\\theta(s_i,a_i)\\right] \\nabla_\\theta Q_\\theta(s_i).$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed33aab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "We derived the TD(0) algorithm for $Q$ functions. Let us draw inspiration from the previous exercise and the derivation of this section to write TD(0) on $V$ functions.\n",
    "- First, recall $T^\\pi$ in terms of an expectation over random variables $R$ and $S'$.\n",
    "- Then define a bootstrapped sample $g_t$ of this expectation.\n",
    "- Finally write the TD(0) SGD update on parametric state value functions, and the corresponding temporal difference.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d4ec4",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- $T^\\pi$ operator: $(T^\\pi V)(s) = \\mathbb{E}_{R,S'}\\left[ R + \\gamma V(S') \\right]$\n",
    "- Bootstrap sample: $g_t = r_t + \\gamma V_t(s_{t+1})$.\n",
    "- The TD(0) update is $\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ r_i + \\gamma V_\\theta(s_{i+1}) - V_\\theta(s_i) \\right] \\nabla_\\theta V_\\theta(s_i)$.  \n",
    "    The temporal difference is $\\delta = r_t + \\gamma V(s_{t+1}) - V(s_t)$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba338d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "When deriving TD(0) on $Q$ functions, we wrote that we needed to enforce the visitation of every state-action pair when obtaining samples from the MDP. Is it still the case for TD(0) on $V$ functions? Is TD(0) on $V$ functions an off-policy algorithm?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3c801",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We need $g_t$ to be a sample of $G^\\pi(s_t) = R_t + \\gamma V(S_{t+1})$. So,  $r_t$ should be a sample of $R_t$, that is the reward obtained by taking action $\\pi(s_t)$ in $s_t$. Additionally, $S_{t+1}$ should be drawn according to $p(\\cdot | s_t,\\pi(s_t))$. So the behavior policy **needs** to be $\\pi$, otherwise the samples lose all meaning. TD(0) on $V$ functions is an **on-policy** algorithm: its behavior policy is constrained to be the one under evaluation.\n",
    "\n",
    "One could remark that since the policy applied is $\\pi$, we cannot guarantee that all states will be visited. However, the visited states will be those reachable by $\\pi$, from the initial state. Consequently the convergence of TD(0) on state value functions can only be guaranteed on states that are reachable with non-zero probability under $\\pi$, and not on the full state space $S$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b0785",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Based on the model of `TD_Qeval` implement a function `TD_Veval(env, pi, max_steps, alpha, gamma, Vinit=None, Vtrue=None)` that applies TD(0) on state value functions.  \n",
    "Apply this function on the policy that always move right.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede5e5a9-b495-4cec-9c7d-a858f54189f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "\n",
    "def TD_Veval(env, pi, max_steps=int(1e6), alpha=0.001, gamma=0.9, Vinit=None, Vtrue=None):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (Vinit is None):\n",
    "        Vinit = np.zeros(env.observation_space.n)\n",
    "    V = np.copy(Vinit)\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps)):\n",
    "        a = pi[x]\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        V[x] = V[x] + alpha * (r+gamma*V[y]-V[x])\n",
    "        if(Vtrue is not None):\n",
    "            error[t] = np.max(np.abs(V-Vtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return V, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874a833a-fff1-41a3-ba48-2fc9080f99fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_TD_Veval.py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def TD_Veval(env, pi, max_steps, alpha, gamma, Vinit=None, Vtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (Vinit is None):\n",
    "        Vinit = np.zeros((env.observation_space.n))\n",
    "    V = np.copy(Vinit)\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        a = pi[x]\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        V[x] = V[x] + alpha * (r+gamma*V[y]-V[x])\n",
    "        if(Vtrue is not None):\n",
    "            error[t] = np.max(np.abs(V-Vtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return V, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc386693-1bb8-489e-86a9-dae319b5764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=int)\n",
    "\n",
    "# Model-based value function computation\n",
    "V_pi0, residuals = policy_eval_iter_mat2(env,pi0,1e-4,10000)\n",
    "\n",
    "# Run TD(0)\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "max_steps = int(2e6)\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vtd, error = TD_Veval(env=env, pi=pi0, max_steps=int(2e6), alpha=alpha, gamma=gamma, Vinit=Vinit, Vtrue=V_pi0)\n",
    "\n",
    "# Display\n",
    "print(Vtd)\n",
    "print(V_pi0)\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb1ad9-c650-40a9-9411-043d30868a2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Similarly to the previous exercises, implement a function `TD_Veval_stoch_pi` that evaluates a stochastic policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73801fda-f8a6-4a6e-a61e-095fd6ac0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "\n",
    "def TD_Veval(env, pi, max_steps, alpha, gamma, Vinit=None, Vtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (Vinit is None):\n",
    "        Vinit = np.zeros((env.observation_space.n))\n",
    "    V = np.copy(Vinit)\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        a = np.random.choice(env.action_space.n, p=pi[x])\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        V[x] = V[x] + alpha * (r+gamma*V[y]-V[x])\n",
    "        if(Vtrue is not None):\n",
    "            error[t] = np.max(np.abs(V-Vtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return V, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e0f46b5-747d-4518-8252-2871b0527658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_TD_Veval_stoch_pi.py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def TD_Veval_stoch_pi(env, pi, max_steps, alpha, gamma, Vinit=None, Vtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (Vinit is None):\n",
    "        Vinit = np.zeros((env.observation_space.n))\n",
    "    V = np.copy(Vinit)\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        a = np.random.choice(env.action_space.n, p=pi[x,:])\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        aa = np.random.choice(env.action_space.n, p=pi[y,:])\n",
    "        V[x] = V[x] + alpha * (r+gamma*V[y]-V[x])\n",
    "        if(Vtrue is not None):\n",
    "            error[t] = np.max(np.abs(V-Vtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return V, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736fcb1-9823-4427-8f6e-959bae6b7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "# Model-based value function computation\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=int)\n",
    "V_pi0, residuals = policy_eval_iter_mat2(env,pi0,1e-4,10000)\n",
    "\n",
    "# Run TD(0)\n",
    "pi0 = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "pi0[:,fl.RIGHT] = 1\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "max_steps = int(2e6)\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vtd, error = TD_Veval_stoch_pi(env=env, pi=pi0, max_steps=int(2e6), alpha=alpha, gamma=gamma, Vinit=Vinit, Vtrue=V_pi0)\n",
    "\n",
    "# Display\n",
    "print(Vtd)\n",
    "print(V_pi0)\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b014cc",
   "metadata": {},
   "source": [
    "In general, we will call $\\beta$ the *behavior policy*. It is the policy being applied to interact with the environment. Off-policy evaluation algorithms can use a behavior policy that is different than the policy being evaluated. We will call *behavior distribution* the distributions $\\rho^\\beta(s)$ over states and $\\rho^\\beta(s,a)$ over state-action pairs, induced by applying $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b7d9e-97b5-4294-96c1-ed1ba131d72c",
   "metadata": {},
   "source": [
    "## Approximate Policy Iteration based on TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e1cb7-8a00-4302-82bf-c25a49dd7919",
   "metadata": {},
   "source": [
    "TD(0) provided us with a method to evaluate (approximately) a policy from samples. This is a great tool to directly implement an approximate policy iteration algorithm.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Write a `API_with_TD0(env, nb_iter, td0_steps, init_pi, behavior_pi, alpha, gamma, Qinit=None, Qtrue=None, save_frequency=1)` function that uses TD(0) as the approximate policy evaluation method in a policy iteration algorithm.\n",
    "`nb_iter` is the number of policy iterations, `td0_steps` is the number of steps for running TD(0) on each policy, `behavior_pi` is the (fixed) behavior policy, `init_pi` is the starting policy for policy iteration, `alpha` is the learning rate, `gamma` is the discount factor, `Qinit` is the initial value function used in TD(0) for the first policy (for subsequent policies, re-use the last value function computed), `Qtrue` is the true optimal policy. \n",
    "Save the policies every `save_frequency` iteration. Return the sequence of policies, the final value function, and the sequence of stepwise $\\|\\|_\\infty$ errors between $Q$ and $Q^*$ if the latter was provided through `Qtrue`.\n",
    "\n",
    "Warning: you could call `TD_Qeval` from the previous exercises, but that would imply reseting the environment after each policy update. Although this is roughly ok if `td0_steps` is large, it might become problematic when it is small, as only states close to the starting state will be explored. Consequently, you should avoid calling this function and write the TD(0) loop directly within your `API_with_TD0` function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523595e-e1b8-4180-8083-840607d20374",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e844f12-52ef-4b9a-8b9e-0affce8f5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_API_with_TD0.py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from solutions.fl_greedyQpolicy import greedyQpolicy\n",
    "\n",
    "def API_with_TD0(env, nb_iter, td0_steps, init_pi, behavior_pi, alpha, gamma, Qinit=None, Qtrue=None, save_frequency=1, disable_tqdm=False):\n",
    "    nb_steps = nb_iter*td0_steps\n",
    "    error = np.zeros((nb_steps))\n",
    "    save_steps = save_frequency*td0_steps\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q = np.copy(Qinit)\n",
    "    pi = np.copy(init_pi)\n",
    "    pi_sequence = [pi]\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(nb_steps), disable=disable_tqdm):\n",
    "        if ( (t+1) % td0_steps==0):\n",
    "            pi = greedyQpolicy(env, Q)\n",
    "        if ( (t+1) % save_steps==0):\n",
    "            pi_sequence.append(pi)\n",
    "        a = np.random.choice(env.action_space.n, p=beta[x,:])\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][pi[y]]-Q[x][a])\n",
    "        if(Qtrue is not None):\n",
    "            error[t] = np.max(np.abs(Q-Qtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return pi_sequence,Q,error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570886ac-4b40-482b-a006-8fdaf3709e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_value_iteration import value_iteration\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=int)\n",
    "beta = (1./env.action_space.n) * np.ones((env.observation_space.n,env.action_space.n))\n",
    "\n",
    "# Model-based computation of Q* and pi*\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(env,Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(env,Vstar)\n",
    "\n",
    "# Policy iteration based on TD(0)\n",
    "pi_sequence,Q,error = API_with_TD0(env, \\\n",
    "                          max_iter = 6, \\\n",
    "                          td0_steps = 500000, \\\n",
    "                          init_pi = pi0, \\\n",
    "                          behavior_pi = beta, \\\n",
    "                          alpha = .001, \\\n",
    "                          gamma = 0.9, \\\n",
    "                          Qinit = None, \\\n",
    "                          Qtrue = Qstar)\n",
    "\n",
    "# Let's plot the difference between Q and Qstar along training\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);\n",
    "\n",
    "# Let's plot the gap between the successive Qpi and Qstar\n",
    "gap = []\n",
    "for pi in pi_sequence:\n",
    "    V_pi, residuals = policy_eval_iter_mat2(env,pi,1e-4,10000)\n",
    "    Q_pi = Q_from_V(env,V_pi)\n",
    "    gap.append(np.max(np.abs(Q_pi-Qstar)))\n",
    "plt.figure()\n",
    "plt.plot(gap)\n",
    "plt.figure()\n",
    "plt.semilogy(gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f848d76-3245-484b-9fe8-7d03bf7d8e5c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:** \n",
    "Now let's bring the number of TD(0) steps between policy updates down to 1. What is the resulting algorithm?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc5106-87d6-4114-abda-b533934b78f6",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "At any given time, the algorithm has a current value function $Q$ and policy $\\pi$. Taking a single TD(0) step amounts to approximating $Q^{\\pi}$ by taking a single stochastic approximation step from $Q$ towards $T^{\\pi} Q$. So, after collecting sample $(s,a,r,s')$:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma Q(s',\\pi(s'))-  Q(s,a) ).$$\n",
    "\n",
    "Then, $\\pi$ is immediately redefined as $\\mathcal{G}(Q)$.\n",
    "\n",
    "This is precisely Q-learning.\n",
    "\n",
    "Of course, compared to the code provided in the solution to the previous exercice, we should avoid calling `env.reset()` every time `TD_Qeval` is called: since there is only a single time step of TD(0), it should be from the\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2e41a-d6a6-4833-9e85-6df877a18c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_value_iteration import value_iteration\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=int)\n",
    "beta = (1./env.action_space.n) * np.ones((env.observation_space.n,env.action_space.n))\n",
    "\n",
    "# Model-based computation of Q* and pi*\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(env,Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(env,Vstar)\n",
    "\n",
    "# Policy iteration based on TD(0)\n",
    "pi_sequence,Q,error = API_with_TD0(env, \\\n",
    "                                   max_iter = int(2e6), \\\n",
    "                                   td0_steps = 1, \\\n",
    "                                   init_pi = pi0, \\\n",
    "                                   behavior_pi = beta, \\\n",
    "                                   alpha = .001, \\\n",
    "                                   gamma = 0.9, \\\n",
    "                                   Qinit = None, \\\n",
    "                                   Qtrue = Qstar, \\\n",
    "                                   save_frequency=1000)\n",
    "\n",
    "print(len(pi_sequence))\n",
    "\n",
    "# Let's plot the difference between Q and Qstar along training\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);\n",
    "\n",
    "# Let's plot the gap between the successive Qpi and Qstar\n",
    "gap = []\n",
    "for pi in pi_sequence:\n",
    "    V_pi, residuals = policy_eval_iter_mat2(env,pi,1e-4,10000)\n",
    "    Q_pi = Q_from_V(env,V_pi)\n",
    "    gap.append(np.max(np.abs(Q_pi-Qstar)))\n",
    "plt.figure()\n",
    "plt.plot(gap)\n",
    "plt.figure()\n",
    "plt.semilogy(gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015bf81",
   "metadata": {},
   "source": [
    "## Delayed updates and experience replay for TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95542003",
   "metadata": {},
   "source": [
    "We have seen that TD(0), at each time step, takes a gradient step in the direction of $T^\\pi Q$.\n",
    "\n",
    "The result of this gradient step is an approximation of $T^\\pi Q$.\n",
    "\n",
    "We saw in class that despite the (bounded) error $\\epsilon$ made by an approximation operator $\\mathcal{A}$, the sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$ still reached a neighborhood of $Q^*$ (without necessarily converging). The same result holds for the sequence $Q_{n+1} = \\mathcal{A} T^\\pi Q_n$.\n",
    "\n",
    "One single step of stochastic gradient descent makes for a poor approximator. Given a fixed function $Q_n$, if we repeat a certain number $C$ of such gradient steps, we can hope to obtain a better estimate of $T^\\pi Q_n$. So there is an interest in keeping two $Q$ functions in memory. The first is the current estimator, which plays the role of $Q_n$, upon which we apply $T^\\pi$, and which we call the *target* function, sometimes noted $Q^-$ (especially in deep Q-learning). The second is the one we actually optimize and which aims at approximating $T^\\pi Q^-$; we write it $Q$. Every $C$ gradient steps, as $Q$ is hopefully getting close to $T^\\pi Q^-$, we replace $Q^-$ by $Q$ and repeat.\n",
    "\n",
    "Consequently, this procedure of **delayed updates** trades off advancing in the $Q_{n+1} = \\mathcal{A} T^\\pi Q_n$ sequence for better approximation properties for $\\mathcal{A}$.\n",
    "\n",
    "This makes more apparent the remark made before that TD(0) actually solves the $Q_{n+1} = T^\\pi Q_n$ sequence and thus successively minimizes a sequence of losses:\n",
    "$$L_n(Q) = \\mathbb{E}_{(s,a)\\sim\\rho} \\left[ \\left( Q(s,a) - T^\\pi Q_n (s,a) \\right)^2 \\right].$$\n",
    "The loss changes everytime we replace $Q_n$ by $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028c154",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "What's the value of $C$ in vanilla TD(0)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e97cdb",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Vanilla TD(0) replaces $Q^-$ by $Q$ at every step, so $C=1$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17440538",
   "metadata": {},
   "source": [
    "We have written several times the update of TD(0) with a batch of samples. But in order to be able to perform stochastic gradient steps using more than one sample, one needs to keep samples in memory.\n",
    "\n",
    "Recall the loss we defined to introduce the stochastic gradient update:\n",
    "$$L(\\theta) = L(Q_\\theta) = \\mathbb{E}_{\\substack{(s,a)\\sim \\rho\\\\ g\\sim G^\\pi(s,a)}} \\left[ \\left( Q_\\theta(s,a) - g \\right)^2 \\right].$$\n",
    "\n",
    "Recall also that $d = \\frac{1}{N} \\sum_{i=1}^N \\left[ Q_\\theta(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_\\theta Q(s_i,a_i)$ is an unbiased estimate of $\\nabla_Q L(Q)$ only if the $g^\\pi(s_i,a_i)$ are drawn **independently** and **identically** according to the distribution of $G^\\pi(s,a)$.\n",
    "\n",
    "This last condition can only be verified if \n",
    "1. the $s_i,a_i$ are drawn independently of each other and always according to the same distribution $\\rho(s,a)$, and\n",
    "2. given $s_i,a_i$, the realizations $g^\\pi(s_i,a_i)$ are drawn independently of each other and according to the distribution of $G^\\pi(s_i,a_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6eaa2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "What do you think? Is condition 1 verified in vanilla TD(0)? What about condition 2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c538b",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "\n",
    "Let's write $(s_i,a_i,r_i,s'_i)$ the $i$-th sample and $\\beta$ the behavior policy.\n",
    "    \n",
    "Condition 1 is not verified for vanilla TD(0). It is true that if the behavior policy $\\beta$ is constant, then on average the state samples $s_i$ are drawn **identically**, according this policy's state distribution $\\rho^\\beta(s)$ and the action samples are drawn according to $\\beta(\\cdot | s)$. However, successive samples are not drawn **independently** by definition, since $\\mathbb{P}(S_{t+1})$ is actually conditioned by $S_t$ and $A_t\\sim\\beta(\\cdot | S_t)$.\n",
    "\n",
    "    \n",
    "Condition 2, on the other hand is easier to verify for TD(0) updates: since the reward $r_i$ and the next state $s'_i$ are only conditioned by $s_i$ and $a_i\\sim \\beta(\\cdot | s_i)$, the $g^\\pi(s_i,a_i) = r_i + \\gamma Q(s'_i,\\pi(s'_i))$ are all drawn identically and independently of each other.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7e62e",
   "metadata": {},
   "source": [
    "Despite this, TD(0) with tabular function representation still converges as long as states and actions are tried frequently enough. In some cases of function approximation this is also still true. But we might have found a major issue here in the most general case.\n",
    "\n",
    "One way to (approximately) recover the conditional independence of sampled states $s_i$ is to store a large number of samples $(s_i,a_i,r_i,s'_i)$ in memory, and draw samples uniformly at random from this memory for the TD(0) updates. This idea was first introduced by Lin in his 1992 **[Self-improving reactive agents based on reinforcement learning, planning and teaching](https://link.springer.com/article/10.1007/BF00992699)** article, under the name of *experience replay* (although his derivation was not exactly the same and was applied to the Q-learning algorithm, which we reserve for another class).\n",
    "\n",
    "The memory of samples is generally called an **experience replay memory** or **experience replay buffer**, since it allows the learning agent to store past experience in memory and recall it (replay it) as many times as necessary to facilitate learning.\n",
    "\n",
    "Drawing uniformly randomly from a replay buffer preserves the stationary distribution which generated the samples and breaks the conditional dependency between successive samples in a trajectory.\n",
    "\n",
    "Combined with the delayed updates introduced earlier, this yields a general, practical algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25231b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**TD(0) with delayed updates, experience replay and parametric function approximation:**  \n",
    "Given a set of samples $\\left\\{(s_i,a_i,r_i,s'_i)\\right\\}_{i\\in [1,N]}$ all drawn from a fixed behavior distribution, an initial *target* function $Q^-$, and a parametric function approximator $Q_\\theta$, the gradient update is:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ r_i + \\gamma Q^-(s'_{i},\\pi(s'_i)) - Q_\\theta(s_i,a_i) \\right] \\nabla_\\theta Q_\\theta(s_i,a_i)$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, under the Robbins-Monro conditions, and under repeated substitution of $Q^-$ by $Q_\\theta$ every $C$ gradient updates, this procedure reaches values for $\\theta$ such that $Q_{\\theta}$ is within a neighborhood of $Q^\\pi$.\n",
    "</div>\n",
    "\n",
    "Recall that for finite state and action spaces, one can take the parameterization $\\theta$ to be the vector of values taken by $Q$ in eacg state-action pair. This provides an algorithm with no error due to function approximation (but still possible errors due to gradient descent steps and unsuitable behavior policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d8cee-9d57-4755-b102-be70ce6107ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Write a TD(0) algorithm with experience replay in frozen lake.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1a724-f926-4f19-ad6a-43f066cad676",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "552854e7-8c6d-48b6-ae80-403c7918bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/no_solution_yet.py\n",
    "print(\"No solution yet for this exercise. Sorry!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90964a86",
   "metadata": {},
   "source": [
    "## The importance of the behavior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8605618-4524-4682-a370-d788b8764294",
   "metadata": {},
   "source": [
    "Let us rephrase an aspect seen in class and understand what is really minimized when we perform SGD with a replay buffer.\n",
    "\n",
    "We have written above that TD(0) on $Q$ functions is an off-policy algorithm since it will estimate $Q^\\pi$ whatever the behavior policy is, as long as this policy explores all state-action pairs infinitely often.\n",
    "\n",
    "This is actually only true if we are working with tabular representations of $Q$ functions.\n",
    "\n",
    "Let's return to the gradient descent formulation for a minute. We wrote that the risk to minimize was:\n",
    "$$L(\\theta) = \\frac{1}{2} \\mathbb{E}_{\\substack{(s,a)\\sim \\rho\\\\ g\\sim G^\\pi(s,a)}} \\left[ \\left( Q_\\theta(s,a) - g\\right)^2 \\right].$$\n",
    "\n",
    "And we wrote the Monte Carlo estimate of this risk's gradient:\n",
    "$$d = \\frac{1}{N} \\sum_{i=1}^N \\left[ Q_\\theta(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_\\theta Q_\\theta(s_i,a_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae4984-3e0f-42ba-8b15-11958da8b110",
   "metadata": {},
   "source": [
    "This is actually only correct if the samples $(s_i, a_i)$ are drawn according to some fixed $\\rho(s,a)$ distribution in $S \\times A$. But in practice, the samples are drawn in $S\\times A$ according to the behavior distribution $\\rho^\\beta(s,a)$. If the behavior policy changes with time, $\\rho$ is a mix of all successive $\\rho^\\beta$.\n",
    "\n",
    "Let us introduce $\\Gamma^\\pi(s,a,g)$ the density of the joint distribution over state, actions, and returns. \n",
    "In class, we wrote $p(g|s,a,\\pi)$ for the conditional distribution on returns, and $\\rho(s,a)$ the marginal on states and actions.\n",
    "So, given a (possibly mixed) behavior distribution $\\rho(s,a)$, essentially:\n",
    "$$\\Gamma^\\pi(s,a,g) = \\rho(s,a) p(g|s,a,\\pi)$$\n",
    "\n",
    "Then, **if the samples $(s,a,g)$ are drawn according to $\\Gamma(s,a,g)$**, the Monte Carlo estimate\n",
    "$$d = \\frac{1}{N} \\sum_{i=1}^N \\left[ Q_\\theta(s_i,a_i) - g\\right] \\nabla_\\theta Q_\\theta(s_i,a_i)$$\n",
    "is actually a correct estimate of $\\nabla_\\theta L(\\theta)$.\n",
    "\n",
    "What does it mean that $(s,a,g)$ are drawn according to $\\Gamma(s,a,g)$? It means state-action pairs are drawn according to $\\rho(s,a)$ ($\\rho^\\beta(s,a)$ if the behavior policy is fixed) and the $g$ conditionally follow the distribution of $G^\\pi(s,a)$.\n",
    "\n",
    "Otherwise, this Monte Carlo estimate is a biased estimator that minimizes another loss function; for example one defined by another behavior policy $\\beta'$.\n",
    "\n",
    "So when we are summing elements drawn from the replay memory, or when we are using single samples drawn from the interaction with the MDP, we are actually minimizing the loss function defined specifically by (the mix of successive) $\\rho^\\beta(s,a)$. And in the end, for another behavior distribution, the resulting minimizer $Q$ of the loss might be different from the one obtained by using samples collected with $\\rho^\\beta$.\n",
    "\n",
    "In other words, if we change the behavior distribution, TD(0) might not converge to the same $Q$ function.\n",
    "\n",
    "So, does that mean TD(0) on $Q$ functions is not really off-policy?\n",
    "\n",
    "In the case of tabular representations, the fact that the gradients $\\nabla_\\theta Q(s_i,a_i)$ are actually indicator functions of $(s_i, a_i)$ limits the impact of having different distributions on $S\\times A$. In this case, the minimizers of $L(\\theta)$ are actually all the same across behavior distributions, as long as the behavior distribution's support spans fully $S\\times A$ (which is also far from being easy to ensure in large state and action spaces!).\n",
    "\n",
    "However, this nice property is lost in the general case, in particular in the case of function approximation $Q_\\theta$. This has motivated the introduction of the **[Gradient Temporal Difference](https://proceedings.neurips.cc/paper/2008/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html)** family of algorithms. This topic is beyond the scope of this class but the interested reader is encouraged to look at **[H. R. Maei's PhD thesis](https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950acc6",
   "metadata": {},
   "source": [
    "## Monte Carlo evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073059c",
   "metadata": {},
   "source": [
    "When evaluating a given policy, in TD(0), we directly *bootstrapped* the estimator of $V^\\pi(s_t,a_t)$, by writing $g_t^\\pi = r_t + \\gamma V_t(s_{t+1})$.\n",
    "\n",
    "But an immediate way to estimate $V^\\pi(s)$ is to take the empirical average of a series of realizations of the $\\sum\\limits_{t = 0}^\\infty \\gamma^t R_t$ random variable.\n",
    "\n",
    "In plain words: simulate $\\pi$ from $s$ a certain number of times to obtain trajectories, observe and accumulate the rewards along each trajectory, take the empirical average over all trajectories.\n",
    "\n",
    "That is precisely what we did in the homework of the class on MDPs to estimate the value of the starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39567f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_mc_eval.py\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "\n",
    "def mc_eval(env,pi,nb_trials):\n",
    "    horizon = 200\n",
    "    gamma = 0.9\n",
    "    Vepisode = np.zeros(nb_trials)\n",
    "    for i in range(nb_trials):\n",
    "        state,_ = env.reset()\n",
    "        for t in range(horizon):\n",
    "            next_state, r, done, _, _ = env.step(pi[state])\n",
    "            Vepisode[i] += gamma**t * r\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    return Vepisode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054fb6e",
   "metadata": {},
   "source": [
    "This provided an estimate of $V^\\pi(s_0)$. Can we generalize this to learning the value $V^\\pi$ in all states?\n",
    "\n",
    "Let us start with a fully **offline Monte-Carlo** algorithm. The idea is simple: start from $s$, run the policy until termination (or for a long number of steps), repeat for a number of episodes, then update the value of all encountered states. This requires to store in memory full episodes (it also requires that the episodes be finite-length).\n",
    "\n",
    "But we can immediately do better with an **online Monte-Carlo** method. It is almost the same idea: start from $s$, run the policy until termination (or for a long number of steps) then update the value of all encountered states before restarting an episode.  \n",
    "\n",
    "Let $(s_0, r_0, s_1, \\ldots, s_T)$ be the sequence of transitions of such an episode.  \n",
    "For the sake of clarity we will slim down our notations: $g^\\pi(s_t)$ becomes $g_t$.  \n",
    "Then, this sequence provides a sample $g_t$ of $G^\\pi(s_t)$ for all $s_t$ visited during the simulations. \n",
    "<div class=\"alert alert-success\"><b>Monte Carlo return:</b>\n",
    "\n",
    "$$g_t = \\sum_{i>t} \\gamma^{i-t} r_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e00e3e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Write a function `MC_Veval(env, pi, max_steps, max_episodes, alpha, gamma, Vinit=None, Vtrue=None)` that implements an online Monte Carlo estimator. This function runs `max_episodes` rollouts of length at most `max_steps` of `env` controled by `pi`. After each episode, it uses the collected rewards to update the `gamma` discounted value of all encountered states, using a stochastic approximation procedure of constant learning rate `alpha`. It starts from a `Vinit` value function estimate and compares the estimate of $V^\\pi$ after each episode to `Vtrue` if provided. It returns the estimate of $V^\\pi$, an array indicating the cumulated number of time steps after each episode, and the $L_\\infty$ of the difference between the current estimate and `Vtrue`.   \n",
    "Apply this function to the policy that always moves right in FrozenLake.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96730a71-891a-45e8-baef-39b1e122db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "\n",
    "def MC_Veval(env, pi, max_steps, max_episodes, alpha, gamma, Vinit=None, Vtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_episodes))\n",
    "    cumulated_steps = np.zeros((max_episodes))\n",
    "    if (Vinit is None):\n",
    "        Vinit = np.zeros((env.observation_space.n))\n",
    "    V = np.copy(Vinit)\n",
    "    for ep in tqdm(range(max_episodes), disable=disable_tqdm):\n",
    "        x, _ = env.reset()\n",
    "        episode = []\n",
    "        for t in range(max_steps):\n",
    "            y, r, d, _, _ = env.step(pi[x])\n",
    "            episode.append([x, r])\n",
    "            if d==True:\n",
    "                cumulated_steps[ep] = cumulated_steps[ep-1] + t\n",
    "                break\n",
    "            else:\n",
    "                x = y\n",
    "        T = len(episode)\n",
    "        G = np.zeros((T))\n",
    "        G[-1] = episode[-1][1]\n",
    "        x = episode[-1][0]\n",
    "        V[x] = V[x] + alpha * (G[-1] - V[x])\n",
    "        for t in range(-2, -T-1, -1):\n",
    "            G[t] = episode[t][1] + gamma*G[t+1]\n",
    "            x = episode[t][0]\n",
    "            V[x] = V[x] + alpha * (G[t] - V[x])\n",
    "        if (Vtrue is not None):\n",
    "            error[ep] = np.max(np.abs(V-Vtrue))\n",
    "    return V, cumulated_steps, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6120cd9b-856f-4c81-9ed9-efd6f3e5cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_MC_Veval.py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def MC_Veval(env, pi, max_steps, max_episodes, alpha, gamma, Vinit=None, Vtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_episodes)) # used to track the convergence to V_pi0...\n",
    "    cumulated_steps = np.zeros((max_episodes)) # ... against the number of samples\n",
    "    if (Vinit is None):\n",
    "        Vinit = np.zeros((env.observation_space.n))\n",
    "    V = np.copy(Vinit)\n",
    "    for ep in tqdm(range(max_episodes), disable=disable_tqdm):\n",
    "        x,_ = env.reset()\n",
    "        episode = []\n",
    "        # Run episode\n",
    "        for t in range(max_steps):\n",
    "            y,r,d,_,_ = env.step(pi[x])\n",
    "            episode.append([x,r])\n",
    "            if d==True:\n",
    "                cumulated_steps[ep] = cumulated_steps[ep-1] + t\n",
    "                break\n",
    "            else:\n",
    "                x=y\n",
    "        # Update values\n",
    "        T = len(episode)\n",
    "        G = np.zeros((T))\n",
    "        G[-1] = episode[-1][1]\n",
    "        x = episode[-1][0]\n",
    "        V[x] = V[x] + alpha * (G[-1] - V[x])\n",
    "        for t in range(-2,-T-1,-1):\n",
    "            G[t] = episode[t][1] + gamma*G[t+1]\n",
    "            x = episode[t][0]\n",
    "            V[x] = V[x] + alpha * (G[t] - V[x])\n",
    "        if (Vtrue is not None):\n",
    "            error[ep] = np.max(np.abs(V-Vtrue))\n",
    "    return V, cumulated_steps, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32201d-d10c-435d-9049-2b4f97cac073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=int)\n",
    "\n",
    "# Model-based value function computation\n",
    "V_pi0, residuals = policy_eval_iter_mat2(env,pi0,1e-4,10000)\n",
    "\n",
    "# Online MC estimation\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "max_steps = 1000\n",
    "max_episodes = 100000\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vmc,cumulated_steps,error = MC_Veval(env, pi0, max_steps, max_episodes, alpha, gamma, Vinit, V_pi0)\n",
    "\n",
    "# Display results\n",
    "print(Vmc)\n",
    "print(V_pi0)\n",
    "plt.plot(cumulated_steps,error)\n",
    "plt.figure()\n",
    "plt.semilogy(cumulated_steps,error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada2a40",
   "metadata": {},
   "source": [
    "So online Monte-Carlo allows us to update $V^\\pi$ episode after episode. Some values are better estimated than others depending on how often the corresponding state was visited.\n",
    "\n",
    "Monte-Carlo estimation has some flaws nonetheless. It still requires to store one full episode in memory before $V$ is updated. Also, one rare value for $r_t$ affects directly all the value of the states encountered before $s_t$. So we can question the robustness of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a652ef",
   "metadata": {},
   "source": [
    "## TD(lambda)\n",
    "\n",
    "With Monte Carlo (MC) and TD(0), we have two methods with different features:\n",
    "- TD(0): 1-sample update with bootstrapping\n",
    "- MC: $\\infty$-sample update with no bootstrapping\n",
    "\n",
    "What's inbetween?\n",
    "- inbetween: $n$-sample update with bootstrapping\n",
    "\n",
    "In previous classes, we introduced the state-action *bootstrapped return* random variable $G^\\pi_m(s,a,Q)$, for $m\\geq 1$:\n",
    "$$G^\\pi_m(s,a,Q) = \\sum\\limits_{t = 0}^{m-1} \\gamma^t R_t + \\gamma^m Q(S_m, A_m) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "We could define a similar state bootstrapped return random variable $G^\\pi_m(s,V)$, for $m\\geq 1$:\n",
    "$$G^\\pi_m(s,V) = \\sum\\limits_{t = 0}^{m-1} \\gamma^t R_t + \\gamma^m V(S_m) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s\\\\ A_t \\sim \\pi(S_t),\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "Since one generally uses $n$ to count time steps, we will replace $m$ by $n$ in what follows. \n",
    "This $G^\\pi_n(s,V)$ random variable is called the **$n$-step target** or **$n$-step return**. \n",
    "As in the previous section, we will discard the implicit $\\pi$ and $V$ and slim down our notations to write $G^{(n)}_t = G^\\pi_n(s_t,V_t)$.\n",
    "The $n$-step return $G^{(n)}_t$ from state $s_t$ is the random variable:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|l}\n",
    "G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots & \\textrm{MC}\\\\\n",
    "G^{(1)}_t = R_t + \\gamma V_t(S_{t+1}) & 1\\textrm{-step TD = TD(0)}\\\\\n",
    "G^{(2)}_t = R_t + \\gamma R_{t+1} + \\gamma^2 V_t(S_{t+2}) & 2\\textrm{-step TD}\\\\\n",
    "G^{(n)}_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots + \\gamma^n V_t(S_{t+n}) & n\\textrm{-step TD}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "And we define the **$n$-step TD update** as:\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**$n$-step TD update:**\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ G^{(n)}_t - V(s_t) \\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297f94d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Suppose that the immediate reward $R$ has a constant variance $\\sigma^2$ and that for all states $s$ the estimator $V(s)$ of $V^\\pi(s)$ has bias $\\epsilon$.  \n",
    "What is the variance of $G_t^{(n)}$?  \n",
    "What is the bias of $\\mathbb{E}\\left(G_t^{(n)}(s)\\right)$ as an estimator of $V^\\pi(s)$?  \n",
    "Comment on the impact of choosing a certain value for $n$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037c252",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Reminder: $var(X+Y)=var(X)+var(Y)$ and $var(aX)=a^2 var(X)$.  \n",
    "Consequently:\n",
    "\\begin{align*}\n",
    "    var(G_t^{(n)}) &= \\sum_{i=0}^{n-1} \\gamma^{2i} \\sigma^2 + \\gamma^{2n} var(V_t(S_{t+n}))\\\\\n",
    "     &= \\frac{1-\\gamma^{2n}}{1-\\gamma^2}\\sigma^2 + \\gamma^{2n} var(V_t(S_{t+n}))\n",
    "\\end{align*}\n",
    "    \n",
    "The variance grows with $n$, both because $1-\\gamma^n$ grows with $n$ and because $S_{t+n}$ has larger variance as $n$ grows.\n",
    "    \n",
    "On the bias side:\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}\\left(G_t^{(n)}(s)\\right) - V^\\pi(s) &= \\mathbb{E}\\left(G_t^{(n)}(s)\\right) - \\mathbb{E}\\left(\\sum_{i=0}^\\infty \\gamma^t R_t\\right)\\\\\n",
    "    &=\\mathbb{E}\\left(\\sum_{i=0}^{n-1} \\gamma^i R_{t+i} + \\gamma^n V_t(S_{t+n})\\right)  - \\mathbb{E}\\left(\\sum_{i=0}^\\infty \\gamma^t R_t\\right)\\\\\n",
    "    &=\\gamma^n \\left[ \\mathbb{E}\\left(V_t(S_{t+n})\\right) - \\mathbb{E}\\left(\\sum_{i=n}^\\infty \\gamma^t R_t \\right) \\right]\\\\\n",
    "    &=\\gamma^n \\left[ \\mathbb{E}\\left(V_t(S_{t+n})\\right) - V_t(S_{t+n}) \\right]\\\\\n",
    "    &=\\gamma^n \\epsilon\n",
    "\\end{align*}\n",
    "    \n",
    "So the bias decreases with $n$. This makes sense since $V_t$'s importance is weighted by $\\gamma^n$.\n",
    "    \n",
    "Consequently, choosing a value for $n$ is making a bias-variance tradeoff. Small $n$ means small variance and large bias, large $n$ means large variance and small bias. Thus, choosing an intermediate value has in interest in accelerating the convergence of TD algorithms.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503df30b",
   "metadata": {},
   "source": [
    "So MC corresponds to an $\\infty$-step TD update.  \n",
    "    \n",
    "The $n$-step TD update algorithm converges to the true $V^\\pi$ just as TD(0) or MC. It requires to wait for $n$ time steps before performing an update.\n",
    "\n",
    "Remark: for finite-length  episodes of length $T$, all $n$-step returns for $n>T-t$ are equal to the Monte Carlo return $G_t$.\n",
    "\n",
    "So $n$-step TD updates bridge a gap between MC and TD(0). But it's not quite satisfying yet because we never really know what value of $n$ is appropriate to speed up convergence for a given problem. An interesting property is that we can mix $n$ and $m$-step returns together. Consider $G^{mix}_t = \\frac{1}{3} G^{(2)}_t + \\frac{2}{3} G^{(4)}_t$.\n",
    "Then the update $V(s_t) \\leftarrow V(s_t) + \\alpha \\left[G^{mix}_t - V(s_t)\\right]$ still converges to $V^\\pi$. More generally, convex sums of $n$-step returns yield update procedures that still converge to $V^\\pi$.\n",
    "\n",
    "Now, take $\\lambda\\in [0,1]$ and consider the $\\lambda$-return $G^\\lambda_t$:\n",
    "<div class=\"alert alert-success\"><b>\n",
    "\n",
    "$\\lambda$-return:</b>\n",
    "$$G^\\lambda_t = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^\\infty \\lambda^{n-1}G_t^{(n)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943f4e4",
   "metadata": {},
   "source": [
    "The $\\lambda$-return is the mixing of *all* $n$-step returns, with weights $(1-\\lambda) \\lambda^{n-1}$. So, an agent performing a $\\lambda$-return update looks one step in the future and uses that step to update $V(s)$ with weight $(1-\\lambda)$, then looks 2 steps into the future and updates $V(s)$ with a weight $\\lambda (1-\\lambda)$ and so on. The illustrative figure below is an excerpt from **Reinforcement Learning: an introduction** by Sutton and Barto.\n",
    "\n",
    "<center><img src=\"img/TD_lambda_forward.png\"></img></center>\n",
    "\n",
    "To get a better understanding of the $\\lambda$-return and to set ideas, let us consider a finite length episode $(s_t, r_t, s_{t+1}, \\ldots, s_T)$. Since the episode ends after $T$, we have $\\forall k>0, \\ G^{(T-t+k)}_t = G_t$. Thus, we can split the $\\lambda$-return sum in two:\n",
    "\n",
    "\\begin{align*}\n",
    "G^\\lambda_t & = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\sum\\limits_{n=T-t}^{\\infty} \\lambda^{n-1}G_t^{(n)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\lambda^{T-t-1} \\sum\\limits_{n=T-t}^{\\infty} \\lambda^{n-T+t} G_t^{(n)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\lambda^{T-t-1} \\sum\\limits_{k=0}^{\\infty} \\lambda^{k} G_t^{(T-t+k)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t-1} G_t\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So we have $G^\\lambda_t = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t-1} G_t$.\n",
    "- When $\\lambda = 0$, it is a $TD(0)$ update (hence the \"0\" in TD(0)).\n",
    "- When $\\lambda = 1$, it is a MC update.\n",
    "So we can define the **$\\lambda$-return algorithm** that generalizes on TD(0) and MC:\n",
    "<div class=\"alert alert-success\"><b>\n",
    "\n",
    "$\\lambda$-return algorithm:</b>\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\left[G^{\\lambda}_t - V(s_t)\\right] $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4d8ca",
   "metadata": {},
   "source": [
    "That's all very nice and we have replaced the choice of $n$ by the choice of $\\lambda$, which seems less sensitive. But, still, we don't know how to compute those $n$-step returns, and the $\\lambda$-return, without running $n$-step episodes (and thus infinite episodes for the $\\lambda$-return in the general case).\n",
    "\n",
    "This is where we need to flip the little man in the drawing above to make him look backwards in time. When an agent transitions from $s$ to $s'$ and obtains reward $r$, it can compute the $1$-step return for $s$ and perform the corresponding $1$-step TD update. Then, as it transitions from $s'$ to $s''$ and observes $r'$ it can perform the $1$-step TD update in $s'$, but also the $2$-step TD update in $s$! An so on for future transitions. So, incrementally, as time unrolls, the agent will include the $n$-step updates in the $\\lambda$-return of $s$ as they become available. In the limit, when $t\\rightarrow\\infty$, the $\\lambda$-return in every state will be complete and the agent will have completed a $\\lambda$-return algorithm. This figure below (excerpt from **Reinforcement Learning: an introduction** by Sutton and Barto) illustrates this *backward-view* on TD updates.\n",
    "\n",
    "<center><img src=\"img/TD_lambda_backward.png\"></img></center>\n",
    "\n",
    "This seems to imply that we need to remember the states we went through, which is quite the same as remembering full episodes for MC updates. But since we want to update a state seen $n$ steps ago with a weight $\\lambda^n (1-\\lambda)$, we just need to remember, for each state, the last time we visited it (and we can forget about the trajectory linking states together). This way, we store $|S|$ values at all time, instead of an increasingly long sequence of transitions. In order to do this, we introduce the notion of **eligibility trace**:\n",
    "<div class=\"alert alert-success\"><b>\n",
    "\n",
    "Eligibility trace of state $s$:</b>\n",
    "$$e_t(s) = \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e_{t-1}(s) & \\textrm{if }s\\neq s_t\\\\\n",
    "1 & \\textrm{if }s = s_t\n",
    "\\end{array}\\right.$$\n",
    "</div>\n",
    "\n",
    "Initially, all states have an eligibility trace of zero. The eligibility trace of an unvisited state decays exponentially. So $e_t(s)$ measures how old the last visit of $s$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a61b9e",
   "metadata": {},
   "source": [
    "Note that two alternative definitions of eligibility traces prevail:\n",
    "<ul>\n",
    "    <li> \n",
    "    \"<b>replacing traces</b>\": $e_t(s) = 1\\textrm{ if }s = s_t$\n",
    "    <li> \"<b>accumulating traces</b>\": $e_t(s) = e_{t-1}(s) + 1\\textrm{ if }s = s_t$\n",
    "</ul>\n",
    "Often (not always), replacing traces are used in practice.<br>\n",
    "<br>\n",
    "And finally we can define the TD($\\lambda$) algorithm:\n",
    "<div class=\"alert alert-success\"><b>\n",
    "\n",
    "TD($\\lambda$) algorithm:</b><br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> \n",
    "\n",
    "Temporal difference $\\delta = r_t+\\gamma V(s_t') - V(s_t)$.\n",
    "<li> \n",
    "\n",
    "Update eligibility traces for all states<br>\n",
    "$e(s) \\leftarrow \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e(s) & \\textrm{if } s\\neq s_t\\\\\n",
    "1 & \\textrm{if } s=s_t\n",
    "\\end{array}\\right.$\n",
    "<li> \n",
    "\n",
    "Update all state's values $V(s) \\leftarrow V(s) + \\alpha e(s) \\delta$\n",
    "</ol>\n",
    "\n",
    "\n",
    "Initially, $e(s)=0$.\n",
    "</div>\n",
    "\n",
    "Properties and remarks:\n",
    "- Earlier states are given $e(s)$ *credit* for the TD error $\\delta$\n",
    "- If the environment contains terminal states, then $e$ should be reset to zero whenever a new trajectory begins.\n",
    "- If $\\lambda=0$, $e(s)=0$ except in $s_t$ $\\Rightarrow$ standard TD(0)\n",
    "- For $0<\\lambda<1$, $e(s)$ indicates a distance $s \\leftrightarrow s_t$ is in the episode.\n",
    "- If $\\lambda=1$, $e(s)=\\gamma^\\tau$ where $\\tau=$ duration since last visit to $s_t$ $\\Rightarrow$ MC method<br>\n",
    "TD(1) implements Monte Carlo estimation on non-episodic problems!<br>\n",
    "TD(1) learns incrementally for the same result as MC\n",
    "- **TD($\\lambda$) is equivalent to the $\\lambda$-return algorithm.**\n",
    "- The value of $\\lambda$ can even be changed during the algorithm without impacting convergence.\n",
    "- TD($\\lambda$) is an on-policy algorithm: samples must be collected following the policy under evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76386c0c",
   "metadata": {},
   "source": [
    "Note that TD($\\lambda$) is already a batch update (it already updates all state values) but not in the sense of SGD batches.\n",
    "\n",
    "However, in the presentation given above, since the eligibility trace $e(s)$ is defined state by state, the formulation of TD($\\lambda$) is limited to discrete state spaces and tabular function representations. The section on function approximation further down will provide an extension of TD($\\lambda$) to linear function approximation.\n",
    "\n",
    "The extension of TD($\\lambda$) to the off-policy setting has been undertaken in the more general work about **[Gradient Temporal Differences](https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a)** quoted earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af839903",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "\n",
    "Implement a TD($\\lambda$) algorithm to estimate $V^\\pi$ fo the policy that always goes right. As before, take a constant $\\alpha=0.001$, $\\gamma=0.9$ and $\\lambda=0.5$. Run the algorithm for 2000000 steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937ad6e-0583-4d43-9e19-d9c480902993",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "\n",
    "def TDlambda_Veval(env, pi, max_steps, alpha, gamma, lambd, Vinit=None, Vtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (Vinit is None):\n",
    "        Vinit = np.zeros((env.observation_space.n))\n",
    "    V = np.copy(Vinit)\n",
    "    e = np.zeros((env.observation_space.n))\n",
    "    x, _ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        y, r, d, _, _ = env.step(pi[x])\n",
    "        delta = r + gamma*V[y] - V[x]\n",
    "        for s in range(env.observation_space.n):\n",
    "            if s == x:\n",
    "                e[s] = 1\n",
    "            else:\n",
    "                e[s] = e[s]*gamma*lambd\n",
    "            V[s] = V[s] + alpha * e[s] * delta\n",
    "        if (Vtrue is not None):\n",
    "            error[t] = np.max(np.abs(V - Vtrue))\n",
    "        if d == True:\n",
    "            x, _ = env.reset()\n",
    "        else:\n",
    "            x = y\n",
    "    return V, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e527b7-3eb1-4176-92bf-8ff80a4a1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_TDlambda_Veval.py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def TDlambda_Veval(env, pi, max_steps, alpha, gamma, lambd, Vinit=None, Vtrue=None, disable_tqdm=False):\n",
    "    error = np.zeros((max_steps))\n",
    "    if (Vinit is None):\n",
    "        Vinit = np.zeros((env.observation_space.n))\n",
    "    V = np.copy(Vinit)\n",
    "    e = np.zeros((env.observation_space.n))\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        y,r,d,_,_ = env.step(pi[x])\n",
    "        delta = r+gamma*V[y]-V[x]\n",
    "        for s in range(env.observation_space.n):\n",
    "            if s==x:\n",
    "                e[s] = 1\n",
    "            else:\n",
    "                e[s] = e[s]*gamma*lambd\n",
    "            V[s] = V[s] + alpha * e[s] * delta\n",
    "        if(Vtrue is not None):\n",
    "            error[t] = np.max(np.abs(V-Vtrue))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "            e = np.zeros((env.observation_space.n))\n",
    "        else:\n",
    "            x=y\n",
    "    return V, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8e468-729e-4d2d-a036-619d157495ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.fl_policy_eval_iter_mat2 import policy_eval_iter_mat2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=int)\n",
    "\n",
    "# Model-based value function computation\n",
    "V_pi0, residuals = policy_eval_iter_mat2(env,pi0,1e-4,10000)\n",
    "\n",
    "# TD(lambda) estimation\n",
    "gamma = 0.9\n",
    "lambd = 0.5\n",
    "alpha = 0.001\n",
    "max_steps = 2000000\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vtdl, error = TDlambda_Veval(env, pi0, max_steps, alpha, gamma, lambd, Vinit, V_pi0)\n",
    "\n",
    "# Display results\n",
    "print(Vtdl)\n",
    "print(V_pi0)\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f837ba",
   "metadata": {},
   "source": [
    "## Linear value function approximation\n",
    "\n",
    "Often, $S$ is not finite (or is just too large to be enumerated). Consequently, $\\mathbb{R}^S$ has infinite (or just too large) dimension. Thus, tabular representations of $V$ are not possible and one needs to turn to function representations $V_\\theta$ or $Q_\\theta$ with parameters $\\theta$. In this section, we provide a very short discussion on approximation methods for $V$ and $Q$.\n",
    "\n",
    "The FrozenLake example is a toy problem with very few discrete states. It does not lend itself to a convincing demonstration of value function approximation. We shall remain at the theoretical level for the following considerations and reserve practice for later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf397b-8924-482e-9f8f-8b93c3a89b9a",
   "metadata": {},
   "source": [
    "**Linear value function approximation**\n",
    "\n",
    "Suppose we write $V$ as a linear model:\n",
    "$$V(s) = \\theta^T \\varphi(s) = \\sum_{i=1}^K \\theta_i \\varphi_i(s)$$\n",
    "\n",
    "We wish to approximate $V(s)$ as a linear combination of features $\\varphi(s)=\\left(\\varphi_i(s)\\right)_{i\\in[1,K]}$. This way, $V$ lives in the $K$-dimensional function space $span(\\varphi)$. We have plenty of families of functions that we can rely on and the user's expertise plays a big role in choosing a proper **functional basis**. Generally speaking, we would expect the following properties from a good basis:\n",
    "- the target $V^\\pi$ can be closely approximated by its projection on $\\varphi$\n",
    "- given an initial $V_0 \\in span(\\varphi)$ and the recurrence relation $V_{n+1} = \\Pi_\\varphi (T^\\pi V_n)$ (where $\\Pi_\\varphi$ is the projection operator on $span(\\varphi)$), $V_n$ should be a \"close enough\" approximation of $T^\\pi V_n$. This property is illustrated by the figure below with $Q$ instead of $V$ - excerpt from **[Least-Squares Policy Iteration](https://www.jmlr.org/papers/v4/lagoudakis03a.html)** by M. G. Lagoudakis and R. Parr (2003).\n",
    "- $\\varphi$ should form a basis (that is $\\varphi_i \\bot \\varphi_j$)\n",
    "\n",
    "<center><img src=\"img/projection.png\" style=\"width: 600px;\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f43f3f-5d78-48c4-8082-9a6a71553402",
   "metadata": {},
   "source": [
    "If $\\sum_{i=1}^K \\varphi_i(s) = 1, \\forall s$, then $V_\\theta$ is called an *averager*. Averagers are known to be well-behaved for iterative function approximation. Otherwise, other non-averager families of functions are commonly used:\n",
    "\n",
    "- $\\cos$, $\\sin$ over state variables (mimics the Fourier transform, extends to wavelet bases)\n",
    "- polynomials of the state variables (mimics the Taylor expansion)\n",
    "- radial basis functions of the state variables (performs local approximation, extends to kernel smoothing).\n",
    "- among averagers, piecewise constant local functions $\\varphi_i(s) \\in \\{0;1\\}$ group *neighborhoods* in the state space together (note the similarity with tree-based regressors).\n",
    "\n",
    "A very straightforward way of building feature sets is to define features depending on a single state variable and then using the tensor product in order to obtain all possible combinations of single-variable features. More formally and more generally, suppose $S \\subset S_1\\times \\ldots \\times S_k$ and suppose $\\varphi^{(i)}$ defines $d_i$ features over $S_k$; then the tensor product $\\varphi^{(1)} \\otimes \\ldots \\otimes \\varphi^{(k)}$ yields $d=d_1\\ldots d_k$ feature functions on $S$. But there is a catch, the number of these resulting features grows exponentially with $k$ and so does the dimension of the value function's search space $span(\\varphi)$: that is the **curse of dimensionality** that makes searching for a value function exponentially more difficult as the state space dimension grows.\n",
    "\n",
    "Additionnaly, there is **no guarantee** that, for a given $V_n \\in span(\\varphi)$, $T^\\pi V_n$ actually lives in $span(\\varphi)$.\n",
    "\n",
    "But on the bright side, given an initial state $s_0$, the actual reachable space $S'$ given $\\pi$ might be much smaller than $S$. So, in practice, we just need to obtain a good approximation of $V$ on the subspace $S'$.\n",
    "\n",
    "Anyway, to conclude this short paragraph on feature engineering:\n",
    "- good feature engineering in RL is even more crucial than in supervised learning.\n",
    "- it can be very problem-dependent.\n",
    "- good function approximators (generally non-parametric to avoid the fixed $span(\\varphi)$) are of crucial importance.\n",
    "\n",
    "Linear function approximation has played a major role in the RL literature, in particular for temporal differences methods. The **[Policy Evaluation with Temporal Differences](https://www.jmlr.org/papers/v15/dann14a.html)** survey by C. Dann et al. (2014) provides a great overview of this literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139da50",
   "metadata": {},
   "source": [
    "**The tabular case is just a specific case of linear approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbd0d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "If the sentence above is true, what is the set of basis functions corresponding to a tabular representation of $V$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2ac21",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "In the discrete state space case, consider the averager defined as:\n",
    "$$\\varphi_i(s) = \\left\\{\\begin{array}{ll}1 & \\textrm{if }s=s_i\\\\ 0 & \\textrm{otherwise}\\end{array}\\right.$$\n",
    "Feature function $\\varphi_i$ is the indicator function of state $s_i$. Therefore, we have $|S|$ feature functions. So when we write $V(s) = \\sum_{i=1}^{|S|} \\theta_i \\varphi_i(s)$, we actually have $V(s_i) = \\theta_i$. Therefore the tabular representation of $V$ is equivalent to a linear model with the $\\varphi_i$ feature functions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024afa94",
   "metadata": {},
   "source": [
    "Based on the previous exercise, let us rewrite TD($\\lambda$) as a linear model update (we take the accumulating traces version; the replacing traces case is equivalent). We had previously:<br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> \n",
    "\n",
    "Temporal difference $\\delta = r_t+\\gamma V(s_t') - V(s_t)$.\n",
    "<li> \n",
    "\n",
    "Update eligibility traces for all states<br>\n",
    "$e(s) \\leftarrow \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e(s) & \\textrm{if } s\\neq s_t\\\\\n",
    "1 + \\gamma \\lambda e(s)& \\textrm{if } s=s_t\n",
    "\\end{array}\\right.$\n",
    "<li> \n",
    "\n",
    "Update all state's values $V(s) \\leftarrow V(s) + \\alpha e(s) \\delta$\n",
    "</ol>\n",
    "\n",
    "\n",
    "Initially, $e(s)=0$.\n",
    "\n",
    "The temporal difference can be rewritten $\\delta = r_t+\\gamma\\theta^T \\varphi(s_t') - \\theta^T \\varphi(s_t)$.\n",
    "\n",
    "The eligibility trace update can be rewritten $e \\leftarrow \\varphi(s) + \\gamma \\lambda e$.\n",
    "\n",
    "Similarly the value update can be rewritten $\\theta \\leftarrow \\theta + \\alpha e \\delta$.\n",
    "\n",
    "Remark:  \n",
    "Recall the discussion on the importance of the behavior distribution? We concluded that tabular representations were a specific case where TD(0) is truly off-policy because the influence of a sample was limited to the value of $Q$ in the corresponding $(s,a)$ pair. This discussion can be generalized for averagers (although it remains an approximation): such local models are well-behaved to suffer less from the shift in behavior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdac818",
   "metadata": {},
   "source": [
    "**TD($\\lambda$) as a linear approximation update**\n",
    "\n",
    "We generalize the previous result to the general linear model case:\n",
    "<div class=\"alert alert-success\"><b>\n",
    "\n",
    "TD($\\lambda$) with linear function approximation:</b><br>\n",
    "With $V(s) = \\sum_{i=1}^K \\theta_i \\varphi_i(s)$, $e \\in \\mathbb{R}^K$.<br>\n",
    "Initially, $e=0$.<br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> \n",
    "\n",
    "Temporal difference $\\delta = r_t+\\gamma\\theta^T \\varphi(s_t') - \\theta^T \\varphi(s_t)$.\n",
    "<li> \n",
    "\n",
    "Update eligibility traces for all states $e \\leftarrow \\varphi(s) + \\gamma \\lambda e$\n",
    "<li> \n",
    "\n",
    "Update value function $\\theta \\leftarrow \\theta + \\alpha e \\delta$\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "Note that we have provided the results above without proof. We will admit them and refer the reader to RL textbooks for a rigorous justification.\n",
    "\n",
    "Further reading on TD($\\lambda$) with linear function approximation: \n",
    "**[True Online TD($\\lambda$)](http://proceedings.mlr.press/v32/seijen14.html)** by H. Van Seijen and R. Sutton (2014).  \n",
    "An unpublished negative result that somehow follows from this article is also that in the general case it is not possible to have a TD($\\lambda$) algorithm performing on non-linear function approximation and being equivalent to the $\\lambda$-return algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1c57c",
   "metadata": {},
   "source": [
    "**Non-parametric models**\n",
    "\n",
    "Non-parametric models generally refer to function approximators that do not rely on an a-priori fixed finite-dimensional search space and allows the representation space to evolve as needed. Among those non-parametrics models, one can count:\n",
    "- linear approximations that incrementally enrich the functional basis (e.g. **[this article](https://dl.acm.org/doi/abs/10.1145/1390156.1390251)**).\n",
    "- general supervised learning methods: SVMs, k-nearest neighbours (kernel smoothing methods), Gaussian Processes, tree-based methods, neural networks, etc.\n",
    "  In this second category, it is generally useful to distinguish between\n",
    "  - methods that explicitly minimize the L2 loss defined earlier via SGD (e.g. neural networks),\n",
    "  - methods that minimize some other loss (e.g. random forests) and provide alternate (better or worse) guarantees.\n",
    "\n",
    "One quickly realizes that the frontier between parametric and non-parametric models is blur. In the general case of a $s \\mapsto V(s)$ function approximator, the general idea is to feed this approximator with samples of the form $(s, r+\\gamma V(s'))$. But beware: most of the nice results are generally lost when one leaves the realm of linear function approximation. More precisely, when one combines **function approximation**, **off-policy learning** and **bootstrapping** in a temporal difference method, all results are generally lost. This has been studied as the **[Deadly triad of Reinforcement Learning](https://arxiv.org/abs/1812.02648v1)** (H. Van Hasselt et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544575ae",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration and Actor-Critic architectures\n",
    "\n",
    "This short section is mostly an occasion to introduce a bit of vocabulary you might come across when reading RL papers.\n",
    "\n",
    "Remember how we went from Policy Iteration (figure below) to Asynchronous Policy Iteration?\n",
    "<center><img src=\"img/policyiteration.png\"></img></center>\n",
    "\n",
    "Now recall the principle of Asynchronous Policy Iteration. If we consider the two elementary operations:\n",
    "- Bellman backup on $Q$: $Q(s,a) \\leftarrow r(s,a) + \\gamma \\sum\\limits_{s'} p(s'|s,a) Q(s',\\pi(s'))$\n",
    "- Bellman backup on $\\pi$: $\\pi(s) \\leftarrow \\arg\\max_{a} Q(s,a)$\n",
    "\n",
    "Then, as long as every state and every action is visited infinitely often for Bellman backups on $Q$ or $\\pi$, the sequences of $Q_n$ and $\\pi_n$ converge to $Q^*$ and $\\pi^*$.\n",
    "\n",
    "Value Iteration: in each state, one update of $Q$ and one improvement of $\\pi$.<br>\n",
    "Policy Iteration: update $Q$ in all states until convergence, then update $\\pi$ in all states.\n",
    "\n",
    "**Generalized Policy Iteration** is the case where one has two interacting processes: policy evaluation and policy improvement, directly from samples (not from the model anymore). If these processes converge to their respective targets, then Generalized Policy Iteration converges to $Q^*$ and $\\pi^*$. Model-free policy evaluation can take many forms: indirect RL, Monte Carlo evaluations, TD methods...\n",
    "\n",
    "This leads to the definition of the general **actor-critic architectures** (excerpt from [**Algorithms for Reinforcement Learning**](https://sites.ualberta.ca/~szepesva/rlbook.html) by C. Szepesvari):\n",
    "\n",
    "<center><img src=\"img/actor-critic.png\"  style=\"width: 500px;\"></img></center>\n",
    "\n",
    "In such architectures, an *actor* chooses an action based on the current state and the information provided by the *critic*, while the *critic* constantly aims at learning relevant things in order to help the *actor* decide (value functions for example, or an approximate model).\n",
    "\n",
    "**Almost all Reinforcement Learning algorithms fall into an actor-critic architecture.**\n",
    "\n",
    "**Caveat:** there is also an algorithm called actor-critic ([Konda, 1999](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)) that belongs to the family of policy gradient algorithms (which we will discover when we look at direct policy search algorithms in a future chapter). This is unfortunate and sometimes confusing. The actor-critic *algorithm* indeed uses and actor-critic *architecture* (a policy and a value function), but all algorithms we have seen so far, which all can be cast with an actor-critic architecture, are not necessarily actor-critic *algorithms* (actually none of them is). Even worse: in the recent literature, a very successful deep RL algorithm was called *soft actor-critic* (SAC - which will be studied it in a future chapter). But SAC, even though it uses an actor-critic architecture, does not rely on the policy gradient theorem and is not an actor-critic algorithm (it is an approximate value iteration one). Conversely, yet another modern deep RL algorithm called *advantage actor-critic* (A2C), is indeed an actor-critic algorithm, using an actor-critic architecture.   \n",
    "All this is unfortunate and a bit confusing. Nowadays, most people refer to algorithms that store a value function and a policy as actor-critic, in contradiction with the historical origin of the actor-critic algorithm. Generally this does not have any real consequence. But the distinction remains important to have a good understanding of names and notions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff932b-f015-4445-b46f-7ea3e5823cf8",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Implement a `Qlearning(env, max_steps, alpha, gamma, epsilon_update_period, Qinit=None, Qtrue=None)` function which implements Q-learning for discrete state-action environments.\n",
    "To keep things simple, use a constant learning rate $\\alpha$. Add an option for providing the true $Q$ function and monitoring the error along training.\n",
    "Your function should return the learned $Q$ function and the sequence of stepwise $\\|\\|_\\infty$ errors between $Q$ and $Q^\\pi$ if the latter was provided through `Qtrue`.\n",
    "Yes, it's almost the same code as in class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf76e64-eb00-46e9-89e0-ca89a9055f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "def Qlearning(env, max_steps, alpha, gamma, epsilon_update_period, Qinit=None, Qtrue=None, disable_tqdm=False):\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    Qql = np.copy(Qinit)\n",
    "    epsilon = 1\n",
    "    error = np.zeros((max_steps))\n",
    "    x, _ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        if ((t+1)%epsilon_update_period==0):\n",
    "            epsilon = epsilon/2\n",
    "        a = epsilon_greedy(env,Qql,x,epsilon)\n",
    "        y, r, d, _, _ = env.step(a)\n",
    "        Qql[x][a] = Qql[x][a] + alpha * (r+gamma*np.max(Qql[y][:])-Qql[x][a])\n",
    "        error[t] = np.max(np.abs(Qql-Qstar))\n",
    "        if d == True:\n",
    "            x, _ = env.reset()\n",
    "        else:\n",
    "            x = y\n",
    "    return Qql, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0b3e81-5f40-4a46-99cc-3da6e633d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_Qlearning.py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from solutions.fl_epsilon_greedy import epsilon_greedy\n",
    "\n",
    "def Qlearning(env, max_steps, alpha, gamma, epsilon_update_period, Qinit=None, Qtrue=None, disable_tqdm=False):\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n,env.action_space.n)) \n",
    "    Qql = np.copy(Qinit)\n",
    "    epsilon = 1\n",
    "    error = np.zeros((max_steps))\n",
    "    x,_ = env.reset()\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        if((t+1)%epsilon_update_period==0):\n",
    "            epsilon = epsilon/2\n",
    "        a = epsilon_greedy(env,Qql,x,epsilon)\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        Qql[x][a] = Qql[x][a] + alpha * (r+gamma*np.max(Qql[y][:])-Qql[x][a])\n",
    "        error[t] = np.max(np.abs(Qql-Qstar))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "        else:\n",
    "            x=y\n",
    "    return Qql, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b696b-429e-44df-83bb-5308b712cb05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from solutions.fl_value_iteration import value_iteration\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "from solutions.fl_greedyQpolicy import greedyQpolicy\n",
    "from solutions.fl_print_policy import print_policy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "# Model-based computation of Q* and pi*\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(env,Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(env,Vstar)\n",
    "\n",
    "# Q-learning\n",
    "max_steps = int(1e6)\n",
    "epsilon = 1\n",
    "epsilon_update_period = int(1e6)\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "Qql, error = Qlearning(env,max_steps,alpha,gamma,epsilon_update_period,Qinit=None,Qtrue=Qstar)\n",
    "\n",
    "# Let's plot the difference between Qql and Qstar\n",
    "print(\"Max error:\", np.max(np.abs(Qql-Qstar)))\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752c0ff",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00339eba-19bf-4ea0-8c32-2a6bcf4426e1",
   "metadata": {},
   "source": [
    "SARSA is an alternative to Q-learning which received a lot of attention in the past.\n",
    "\n",
    "In a nutshell, it is Q-learning but with a stochastic policy which also serves as a behavior policy.\n",
    "\n",
    "Recall our general definition:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Approximate value iteration as a sequence of risk minimization problems**  \n",
    "$$\\pi_n \\in \\mathcal{G} Q_n,$$\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\left( Q(s,a;\\theta) - G^{\\pi_n}_1(s,a,Q_n) \\right)^2 \\right],$$\n",
    "$$Q_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta).$$\n",
    "</div>\n",
    "\n",
    "In this definition, we wrote earlier that $\\rho = \\rho^\\beta$ (for a fixed behavior policy $\\beta$) or a mix of past $\\rho^\\beta$ (for time-varying behavior policies).\n",
    "\n",
    "Then Q-learning is the above procedure with:\n",
    "- discrete state and action spaces,\n",
    "- tabular representations of Q-functions,\n",
    "- stochastic approximation for learning $Q_n$ (ie. TD(0)),\n",
    "- deterministic policies for $\\pi_n$ (not explictly stored),\n",
    "- $\\epsilon$-greedy policies for $\\beta$ (making the algorithm off-policy).\n",
    "\n",
    "And SARSA is also this procedure but with:\n",
    "- discrete state and action spaces,\n",
    "- tabular representations of Q-functions,\n",
    "- stochastic approximation for learning $Q_n$ (ie. TD(0)),\n",
    "- stochastic policies for $\\pi_n=\\beta$ (not explictly stored) to ensure exploration but with a decreasing level of noise along iterations (making the algorithm on-policy).\n",
    "\n",
    "This might seem a bit abstract. In what follows, we try to give an intuitive introduction to SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ffa3a5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Suppose we modify TD(0) on $Q$-functions so that, instead of picking a random action at each time step, we pick the greedy action with respect to $Q$. What is the risk in doing this?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3c779",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "A deterministic, greedy policy will not try all actions in all states infinitely often. So we can't guarantee convergence, neither of $Q$ to any policy's $Q^\\pi$ (because if some states are not visited no updates will take place for them), nor of the $Q$-greedy policy to $\\pi^*$. The conditions for the convergence of TD(0) or Generalized Policy Iteration are just not met.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbaee2d",
   "metadata": {},
   "source": [
    "To solve this issue we introduce the notion of **Greedy in the limit of infinite exploration** (GLIE) actor:  \n",
    "\n",
    "<div class=\"alert alert-success\"><b>Greedy in the limit of infinite exploration</b> (GLIE) actor:<br>\n",
    "A GLIE actor guarantees that:<br>\n",
    "\n",
    "- All state-action pairs $(s,a)$ are visited infinitely often for updates of $Q$, as $t\\rightarrow\\infty$:\n",
    "$$\\lim_{t\\rightarrow\\infty} count_t(s,a) = \\infty$$\n",
    "- As $t\\rightarrow\\infty$ the actor becomes $Q$-greedy, that is:\n",
    "$$\\lim_{t\\rightarrow\\infty} \\pi_t(a|s) = \\mathbb{1}\\left(a \\in \\arg\\max_{\\hat{a}} Q(s,\\hat{a})\\right)$$\n",
    "</div>\n",
    "\n",
    "An example of GLIE actor is the so-called $\\epsilon$-greedy exploration strategy (that we introduced with Q-learning) that uniformly picks a non-greedy action with probability $\\epsilon$:\n",
    "$$\\pi_t(a|s) = \\left\\{\\begin{array}{ll}1-\\epsilon_t & \\textrm{if }a=\\arg\\max_{\\hat{a}} Q(s,\\hat{a})\\\\\n",
    "\\frac{\\epsilon_t}{|A|-1} & \\textrm{otherwise} \\end{array}\\right.$$\n",
    "With a parameter $\\epsilon_t>0$ that goes to zero as $t$ tends to $\\infty$, one obtains a GLIE actor.\n",
    "\n",
    "GLIE actors (or policies) enforce the limits of the **exploration vs. exploitation tradeoff**. As long as actors respect the GLIE properties, actor-critic architectures fall within the convergence properties of Generalized Policy Iteration.\n",
    "\n",
    "With this last definition, we have all the ingredients to define algorithms that evolve to an optimal behaviour.\n",
    "\n",
    "**A remark**\n",
    "\n",
    "You might have noticed that the totally random policy we applied in the TD(0) update was a very naive choice. Even if all states are reachable from anywhere in the state space (ergodicity property), they might not all be visited with the same frequency and therefore the convergence to $Q^\\pi$ might be delayed because of this uniform exploration strategy.  \n",
    "\n",
    "The same remark applies to $\\epsilon$-greedy exploration strategies that often start with $\\epsilon=1$ and let it slowly decrease towards zero. These strategies don't account for the actual *visitation frequencies* of state-action pairs. In some states it might be good to keep a strong exploration probability because they actually have been seldom visited, while in other states, a faster decrease is desirable. This links also to the question of *value propagation* that was underpinned by the exercises on asynchronous value iteration in the previous chapter.  \n",
    "\n",
    "The point here is to notice that $\\epsilon$-greedy is a simple, very naive exploration strategy that fits within the GLIE requirements but that much better exploration policies are possible by taking the current state into account (contextual exploration) or by using the values of the temporal difference (prioritized sweeping) and the $Q$ estimate (Boltzmann policies, $E^3$, $R_{max}$ or UCBVI strategies) for instance. These topics will be covered in the chapters dedicated to the exploration versus exploitation tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24962345",
   "metadata": {},
   "source": [
    "**SARSA**\n",
    "\n",
    "The key idea behind the SARSA algorithm is to build a TD(0) critic that constantly tries to evaluate the value $Q$ of the actor's policy $\\pi$, and an actor that tends to be greedy with respect to this critic, with some level of noise to insure exploration. The algorithm is written:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**SARSA**  \n",
    "\n",
    "In $s$, choose (*actor*) $a$ using $Q$, then repeat:\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li> \n",
    "\n",
    "Observe $r$, $s'$\n",
    "<li> \n",
    "\n",
    "Choose $a'$ (<i>GLIE actor</i>) using $Q$\n",
    "<li> \n",
    "\n",
    "Temporal difference: $\\delta=r+\\gamma Q(s',a') - Q(s,a)$\n",
    "<li> \n",
    "\n",
    "Update $Q$: $Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta$\n",
    "<li> \n",
    "\n",
    "$s\\leftarrow s'$, $a\\leftarrow a'$\n",
    "</ol>\n",
    "\n",
    "\n",
    "SARSA converges if the actor is GLIE and if $\\alpha$ respects the Robbins-Monro conditions.\n",
    "</div>\n",
    "\n",
    "It is important to note that SARSA is an **on-policy** critic: it constantly evaluates the current $\\pi$... that constantly shifts towards $\\pi^*$ by being $Q$-greedy.\n",
    "\n",
    "The name SARSA comes from the usage of an augmented sample $(s,a,r,s',a')$.<br>\n",
    "<br>\n",
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Let's implement an $\\epsilon$-greedy SARSA on the FrozenLake problem.<br>\n",
    "For the decrease of $\\epsilon$ we can opt for a division by 2 every million steps.<br>\n",
    "Keep track of the state-action visitation count, as in the Q-learning example.<br>\n",
    "You can compare with $Q^*$ and $\\pi^*$ obtained during the model-based class.<br>\n",
    "The utility functions below are here to make the task easier.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Implement a `SARSA(env, max_steps, alpha, gamma, epsilon_update_period, Qinit=None, Qtrue=None)` function which implements SARSA for discrete state-action environments.\n",
    "To keep things simple, use a constant learning rate $\\alpha$. Add an option for providing the true $Q$ function and monitoring the error along training.\n",
    "Your function should return the learned $Q$ function and the sequence of stepwise $\\|\\|_\\infty$ errors between $Q$ and $Q^\\pi$ if the latter was provided through `Qtrue`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340bfd7-2742-49b0-a2b5-7326ae80506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the cell below to load a correction (then you can execute this code).\n",
    "\n",
    "def SARSA(env, max_steps, alpha, gamma, epsilon_update_period, Qinit=None, Qtrue=None, disable_tqdm=False):\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    Q = np.copy(Qinit)\n",
    "    epsilon = 1\n",
    "    error = np.zeros((max_steps))\n",
    "    x, _ = env.reset()\n",
    "    a = epsilon_greedy(env,Q,x,epsilon)\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        if ((t+1)%epsilon_update_period==0):\n",
    "            epsilon = epsilon/2\n",
    "        y, r, d, _, _ = env.step(a)\n",
    "        aa = epsilon_greedy(env,Q,y,epsilon)\n",
    "        Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][aa]-Q[x][a])\n",
    "        error[t] = np.max(np.abs(Q-Qstar))\n",
    "        if d == True:\n",
    "            x, _ = env.reset()\n",
    "            a=epsilon_greedy(env,Q,x,epsilon)\n",
    "        else:\n",
    "            x = y\n",
    "            a = aa\n",
    "    return Q, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efd9afb-0607-4376-8b1c-a14d2849432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fl_SARSA.py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from solutions.fl_epsilon_greedy import epsilon_greedy\n",
    "\n",
    "def SARSA(env, max_steps, alpha, gamma, epsilon_update_period, Qinit=None, Qtrue=None, disable_tqdm=False):\n",
    "    if (Qinit is None):\n",
    "        Qinit = np.zeros((env.observation_space.n,env.action_space.n)) \n",
    "    Q = np.copy(Qinit)\n",
    "    epsilon = 1\n",
    "    error = np.zeros((max_steps))\n",
    "    x,_ = env.reset()\n",
    "    a = epsilon_greedy(env,Q,x,epsilon)\n",
    "    for t in tqdm(range(max_steps), disable=disable_tqdm):\n",
    "        if((t+1)%epsilon_update_period==0):\n",
    "            epsilon = epsilon/2\n",
    "        y,r,d,_,_ = env.step(a)\n",
    "        aa = epsilon_greedy(env,Q,y,epsilon)\n",
    "        Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][aa]-Q[x][a])\n",
    "        error[t] = np.max(np.abs(Q-Qstar))\n",
    "        if d==True:\n",
    "            x,_ = env.reset()\n",
    "            a=epsilon_greedy(env,Q,x,epsilon)\n",
    "        else:\n",
    "            x=y\n",
    "            a=aa\n",
    "    return Q, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9de297-2d0f-4d07-a4e0-54f66ffd16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from solutions.fl_value_iteration import value_iteration\n",
    "from solutions.fl_Q_from_V import Q_from_V\n",
    "from solutions.fl_greedyQpolicy import greedyQpolicy\n",
    "from solutions.fl_print_policy import print_policy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "\n",
    "# Model-based computation of Q* and pi*\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(env,Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(env,Vstar)\n",
    "\n",
    "# Q-learning\n",
    "max_steps = int(5e6)\n",
    "epsilon = 1\n",
    "epsilon_update_period = int(1e6)\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "Qsarsa, error = SARSA(env,max_steps,alpha,gamma,epsilon_update_period,Qinit=None,Qtrue=Qstar)\n",
    "\n",
    "# Let's plot the difference between Qql and Qstar\n",
    "print(\"Max error:\", np.max(np.abs(Qsarsa-Qstar)))\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb6583",
   "metadata": {},
   "source": [
    "The same remark as for TD(0) holds: with a better exploration strategy, the convergence to Qstar could be much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4620501",
   "metadata": {},
   "source": [
    "## Beyond Q-learning and SARSA\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Exercices (open questions, no solutions provided yet):</b><br>\n",
    "<ul>\n",
    "<li><b>Context-dependent exploration</b><br>\n",
    "Can you implement an $(s,a)$-dependent $\\epsilon$-greedy exploration strategy (by using the `count` table introduced earlier for instance)?\n",
    "<li><b>Heuristic initialization on $Q$</b><br>\n",
    "For Q-learning, can you think of an initialization of $Q$ that would be better than plain zeros (for example by exploiting the maximum 1-step reward $r_{max}$)? One that, for instance, would drive the exploration towards unvisited states?\n",
    "<li><b>Reward shaping</b><br>\n",
    "Did you notice that falling into a hole brings no penalty? If we introduced a $-1$ reward for falling into a hole, would it change the optimal policy?\n",
    "<li><b>SARSA($\\lambda$)</b><br>\n",
    "SARSA is an on-policy method and we've seen that so is TD($\\lambda$) so it seems rather straightforward to implement a SARSA($\\lambda$) algorithm that, hopefully, will have better convergence properties than plain SARSA.\n",
    "<li><b>$Q(\\lambda)$</b><br>\n",
    "This time it is not as straightfoward to derive a $Q(\\lambda)$ algorithm from TD($\\lambda$), precisely because TD($\\lambda$) evaluates the policy being applied and not another one. Can you imagine a way to still perform $Q(\\lambda)$ updates? An answer is found in Watkins's thesis that introduces Q-learning in 1989. For a more recent approach and other references, see Sutton et al, <b>A new Q($\\lambda$) with interim forward view and Monte Carlo equivalence</b>, 2014)\n",
    "<li><b>SARSA and $Q$-learning with linear value function approximation</b><br>\n",
    "Can you implement an approximate version of SARSA and $Q$-learning with linear Q-function approximation $Q(s,a)=\\theta^T\\varphi(s,a)$?\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
